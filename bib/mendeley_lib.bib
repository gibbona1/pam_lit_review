@article{Silva2022,
   abstract = {Passive acoustic monitoring, a non-invasive technique, is increasingly used to study animal populations and habitats at much larger spatial and temporal scales than standard methods. However, easy to apply tools for reliable detection and classification of signals of interest among hundreds or even thousands of hours of recording are still lacking.
We introduce the r package soundClass, a tool to train convolutional neural networks, and employ them to classify sound events in recordings. soundClass provides a sound event classification pipeline, from annotating recordings to automating trained networks usage in real-life situations.
We illustrate the package functionality on bat echolocation calls, bird songs and whale echolocation clicks, showing that the package can be used to train networks for several types of sound events, taxonomic groups and environments; and exemplify its application.
This tool facilitates the creation and usage of trained networks and was developed with a strong focus on graphical user interfaces to be used by non-specialist scientists in statistics and programming.},
   author = {Bruno Silva and Frederico Mestre and Sílvia Barreiro and Pedro J Alves and José M Herrera},
   doi = {10.1111/2041-210x.13964},
   issn = {2041-210X},
   journal = {Methods in Ecology and Evolution},
   month = {11},
   pages = {2356-2362},
   publisher = {Wiley},
   title = {soundClass: An automatic sound classification tool for biodiversity monitoring using machine learning},
   url = {http://dx.doi.org/10.1111/2041-210X.13964},
   year = {2022},
}
@book{Hasselman2020,
   abstract = {The greatest potential risk from turbine operation continues to be perceived by regulators and other stakeholders to be that of marine animals colliding with turbine blades. These potential interactions are the most difficult to observe using common oceanographic instruments and must be undertaken in parts of the ocean where fast moving water and high waves make studies challenging. However, our collective understanding of the effects of marine renewable energy (MRE) devices on marine animals and their habitats has improved since the publication of the 2016 State of the Science report. https://tethys.pnnl.gov/publications/state-of-the-science-2020-chapter-10-environmental-monitoring},
   author = {Daniel Hasselman and David Barclay and Robert Cavagnaro and Craig Chandler and Emma Cotter and Douglas Gillespie and Gordon Hastie and John Horne and James Joslin and Caitlin Long and Louise McGarry and Robert Mueller and Carol Sparling and Benjamin Williamson},
   doi = {10.2172/1633202},
   institution = {Office of Scientific and Technical Information (OSTI)},
   month = {11},
   title = {2020 State of the Science Report, Chapter 10: Environmental Monitoring Technologies and Techniques for Detecting Interactions of Marine Animals with Turbines},
   url = {http://dx.doi.org/10.2172/1633202},
   year = {2020},
}
@article{Picciulin2022,
   abstract = {Spatio-temporal variability of marine soundscapes reflects environmental dynamics and local habitat health. This study characterizes the coastal soundscape of the Cres-Lošinj Natura 2000 Site of Community Importance, encompassing the non-tourist (11–15 March 2020) and the tourist (26–30 July 2020) season. A total of 240 h of continuous recordings was manually analyzed and the abundance of animal vocalizations and boat noise was obtained; sound pressure levels were calculated for the low (63–2000 Hz) and high (2000–20,000 Hz) frequency range. Two fish sound types were drivers of both seasonal and diel variability of the low-frequency soundscape. The first is emitted by the cryptic Roche’s snake blenny (Ophidion rochei), while the second, whose emitter remains unknown, was previously only described in canyons and coralligenous habitats of the Western Mediterranean Sea. The high-frequency bands were characterized by bottlenose dolphin (Tursiops truncatus) vocalizations, indicating dolphins’ use of area for various purposes. Boat noise, however, dominated the local soundscape along the whole considered periods and higher sound pressure levels were found during the Tourist season. Human-generated noise pollution, which has been previously found 10 years ago, is still present in the area and this urges management actions.},
   author = {Marta Picciulin and Marta Bolgan and Nikolina Rako-Gospić and Antonio Petrizzo and Marko Radulović and Raffaela Falkner},
   doi = {10.3390/jmse10020300},
   issn = {2077-1312},
   issue = {2},
   journal = {Journal of Marine Science and Engineering},
   month = {11},
   publisher = {MDPI AG},
   title = {A Fish and Dolphin Biophony in the Boat Noise-Dominated Soundscape of the Cres-Lošinj Archipelago (Croatia)},
   url = {http://dx.doi.org/10.3390/jmse10020300},
   year = {2022},
}
@article{Putland2018,
   abstract = {Increasing sound in the ocean from human activity potentially threatens marine animals that use sound to communicate, detect prey, avoid predators and function within their ecosystem. The detection and classification of sound produced by marine animals, such as whales and fish, is an important component in noise mitigation strategies, while also providing valuable insights into their ecology. Traditionally, visual surveys are conducted to assess how these animals utilize a specific area, often underestimating the number of individuals as they don’t spend much time at the surface. Long-term passive acoustic monitoring efforts have become more prevalent to monitor such animals. The large datasets collected can be impractical to manually process, necessitating the development of automated detection methods, which often produce mixed results owing to the broad frequency range and variable duration of many biological sounds. Here we describe a novel approach for automated detection of underwater biophonic sounds employing hidden Markov models (HMM). Acoustic data was collected at a single listening station in Hauraki Gulf, from October 2014 to April 2016. HMM detection models were developed for Bryde’s whales (Balaenoptera edeni) that were used as a model organism because they are notoriously hard to study with traditional visual surveys and produce a characteristic call. Bryde’s whale calls also directly overlap the sounds of anthropogenic activity, in particular the sound of vessels transiting to the busiest port in New Zealand; therefore monitoring whale calls is of utmost importance when confronting increasing sound in the ocean. Vocalizations were detected with a sensitivity of 77% and false positive rate of 23%. Bryde’s whale vocalizations were detected on 11% of all recordings. Overall, there were significantly more detections during summer (n = 1716) than winter (n = 447), and significantly more during the day (n = 1991) compared to night (n = 1264). This study shows the feasibility of using HMMs on long-term acoustic datasets. The method has the potential to be used for a wide range of soniferous animals who, like the Bryde’s whale, also produce unique sounds. The detection method would be particularly useful for mitigation and management strategies of species that are difficult to detect using traditional visual methods.},
   author = {R L Putland and L Ranjard and R Constantine and C A Radford},
   doi = {https://doi.org/10.1016/j.ecolind.2017.09.025},
   issn = {1470-160X},
   journal = {Ecological Indicators},
   keywords = {Acoustic detection,Cetacean,Hidden markov models,Noise mitigation,Passive acoustic monitoring,Temporal variation},
   pages = {479-487},
   title = {A hidden Markov model approach to indicate Bryde's whale acoustics},
   url = {https://www.sciencedirect.com/science/article/pii/S1470160X17305939},
   year = {2018},
}
@article{Jones2021,
   abstract = {Animal sounds are commonly used by humans to infer information about their motivations and their health, yet, acoustic data is an underutilized welfare biomarker especially for aquatic animals. Here, we describe an acoustic monitoring system that is being implemented at the U.S. Navy Marine Mammal Program where dolphins live in groups in ocean enclosures in San Diego Bay. A four-element bottom mounted hydrophone array is used to continuously record, detect and localize acoustic detections from this focal group. Software provides users an automated comparison of the current acoustic behavior to group historical data which can be used to identify periods of normal, healthy thriving dolphins, and allows rare instances of deviations from typical behavior to stand out. Variations in a group or individual’s call rates can be correlated with independent veterinary examinations and behavioral observations in order to better assess dolphin health and welfare. Additionally, the monitoring system identifies time periods in which a sound source from San Diego Bay is of high-enough amplitude that the received level at our array is considered a potential concern for the focal animals. These time stamps can be used to identify and potentially mitigate exposures to acoustic sources that may otherwise not be obvious to human listeners. We hope this application inspires zoos and aquaria to innovate and create ways to incorporate acoustic information into their own animal welfare management programs.},
   author = {Brittany L Jones and Michael Oswald and Samantha Tufano and Mark Baird and Jason Mulsow and Sam H Ridgway},
   doi = {10.3390/jzbg2020015},
   issn = {2673-5636},
   issue = {2},
   journal = {Journal of Zoological and Botanical Gardens},
   pages = {222-233},
   title = {A System for Monitoring Acoustics to Supplement an Animal Welfare Plan for Bottlenose Dolphins},
   volume = {2},
   url = {https://www.mdpi.com/2673-5636/2/2/15},
   year = {2021},
}
@article{Halliday2019,
   abstract = {The Arctic marine environment is changing rapidly through a combination of sea ice loss and increased anthropogenic activity. Given these changes can affect marine animals in a variety of ways, understanding the spatial and temporal distributions of Arctic marine animals is imperative. We use passive acoustic monitoring to examine the presence of marine mammals near Ulukhaktok, Northwest Territories, Canada, from October 2016 to April 2017. We documented bowhead whale (Balaena mysticetus Linnaeus, 1758) and beluga whale (Delphinapterus leucas (Pallas, 1776)) vocalizations later into the autumn than expected, and we recorded bowhead whales in early April. We recorded ringed seal (Pusa hispida (Schreber, 1775)) vocalizations throughout our deployment, with higher vocal activity than in other studies and with peak vocal activity in January. We recorded bearded seals (Erignathus barbatus (Erxleben, 1777)) throughout the deployment, with peak vocal activity in February. We recorded lower bearded seal vocal activity than other studies, and almost no vocal activity near the beginning of the spring breeding season. Both seal species vocalized more when ice concentration was high. These patterns in vocal activity document the presence of each species at this site over autumn and winter and are a useful comparison for future monitoring.},
   author = {W D Halliday and M K Pine and S J Insley and R N Soares and P Kortsalo and X Mouy},
   doi = {10.1139/cjz-2018-0077},
   issn = {1480-3283},
   journal = {Canadian Journal of Zoology},
   month = {11},
   pages = {72-80},
   publisher = {Canadian Science Publishing},
   title = {Acoustic detections of Arctic marine mammals near Ulukhaktok, Northwest Territories, Canada},
   url = {http://dx.doi.org/10.1139/cjz-2018-0077},
   year = {2019},
}
@article{Papin2018,
   abstract = {The grey wolf (Canis lupus) is naturally recolonizing its former habitats in Europe where it was extirpated during the previous two centuries. The management of this protected species is often controversial and its monitoring is a challenge for conservation purposes. However, this elusive carnivore can disperse over long distances in various natural contexts, making its monitoring difficult. Moreover, methods used for collecting signs of presence are usually time-consuming and/or costly. Currently, new acoustic recording tools are contributing to the development of passive acoustic methods as alternative approaches for detecting, monitoring, or identifying species that produce sounds in nature, such as the grey wolf. In the present study, we conducted field experiments to investigate the possibility of using a low-density microphone array to localize wolves at a large scale in two contrasting natural environments in north-eastern France. For scientific and social reasons, the experiments were based on a synthetic sound with similar acoustic properties to howls. This sound was broadcast at several sites. Then, localization estimates and the accuracy were calculated. Finally, linear mixed-effects models were used to identify the factors that influenced the localization accuracy.},
   author = {Morgane Papin and Julian Pichenot and François Guérold and Estelle Germain},
   doi = {10.1186/s12983-018-0260-2},
   issn = {1742-9994},
   journal = {Frontiers in Zoology},
   month = {11},
   publisher = {Springer Science and Business Media LLC},
   title = {Acoustic localization at large scales: a promising method for grey wolf monitoring},
   url = {http://dx.doi.org/10.1186/s12983-018-0260-2},
   year = {2018},
}
@article{Wrege2017,
   abstract = {Summary The accelerating loss of biodiversity worldwide demands effective tools for monitoring animal populations and informing conservation action. In habitats where direct observation is difficult (rain forests, oceans), or for cryptic species (shy, nocturnal), passive acoustic monitoring (PAM) provides cost-effective, unbiased data collection. PAM has broad applicability in terrestrial environments, particularly tropical rain forests. Using examples from studies of forest elephants in Central African rain forest, we show how PAM can be used to investigate cryptic behaviour, mechanisms of communication, estimate population size, quantify threats, and assess the efficacy of conservation strategies. We discuss the methodologies, requirements, and challenges of obtaining these data using acoustics. Where applicable, we compare these methods to more traditional approaches. While PAM methods and associated analysis are maturing rapidly, mechanisms are needed for processing the dense raw data efficiently with standard computer hardware, speeding development of detection algorithms, and harnessing communication networks to move data from the field to research facilities. Passive acoustic monitoring is a viable and cost-effective tool for conservation and should be incorporated in monitoring schemes much more broadly. The capability to quickly assess changes in behaviour, population size, and landscape use, simultaneously over large geographical areas, makes this approach attractive for detecting human-induced impacts and for assessing the success of conservation strategies.},
   author = {Peter H Wrege and Elizabeth D Rowland and Sara Keen and Yu Shiu},
   doi = {https://doi.org/10.1111/2041-210X.12730},
   journal = {Methods in Ecology and Evolution},
   keywords = {Loxodonta cyclotis,anthropogenic impacts,anti-poaching,bioacoustics,cryptic species,density estimate,law enforcement monitoring,passive acoustic monitoring,signal detection,smart},
   pages = {1292-1301},
   title = {Acoustic monitoring for conservation in tropical forests: examples from forest elephants},
   url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12730},
   year = {2017},
}
@misc{,
   abstract = {Marine mammals are protected by three Acts of Congress: the Marine Mammal Protection Act of 1972 (MMPA), the Endangered Species Act of 1973 (ESA), and the National Environmental Protection Act (NEPA) of 1969.  The MMPA prohibits any person or vessel subject to the jurisdiction of the United States to take a marine mammal where "take" is defined as "to harass, hunt, capture, or kill, or attempt to harass, hunt, capture, or kill any marine mammal."  Most of the large whale species were placed on the U.S. Endangered Species List in 1970, which has a similar prohibition on “taking”. The Department of Defense, as a branch of the Federal government, is required under NEPA to evaluate the potential environmental impacts of major actions or activities.

The need for environmental compliance regarding marine mammals and anthropogenic sound creates a need to develop methods to assess the impacts on marine mammal populations. SERDP, the Office of Naval Research, and the environmental readiness branch of the Navy (CNO N45) have made major progress in the past decade on this issue. Major advances have been made in the detection and localization of marine mammal sounds, especially the low frequency calls of baleen whales (SERDP CS-48). We now know what levels of sound may start to cause effects on hearing in dolphins and seals (Ridgway et al. 1998; Kastak and Schusterman 1998; (Finneran et al. 2002; Nachtigall et al. 2003)). A major research effort defined the behavioral responses of four species of baleen whales to different received levels of the SURTASS LFA sonar (SERDP CS-1069). 

As CSSON-01-03 indicates, there is an urgent need for improved techniques to monitor toothed whales and their reactions to sound. Toothed whales are more common than the baleen whales and have hearing and vocalization ranges extending to considerably higher frequencies. Several Navy ranges have broad bandwidth hydrophones that are well-suited to capturing the high frequency calls of toothed whales. Methods for detection and localization of these calls are being developed by NUWC Newport and other research groups. A major goal of this project was to develop and validate methods for passive acoustic monitoring of toothed whales, especially beaked whales.  These techniques offer promise for detecting the otherwise elusive beaked whales, will provide important and cost-effective monitoring capabilities, and will also provide a means for monitoring the long term effects of sounds associated with naval activities in a fixed habitat.

The Statement of Need on marine mammal monitoring lists the requirement for verifying the probability of detecting marine mammals, and the probability of correct classification, using acoustic or non-visual monitoring. The probability of detecting an animal using passive acoustics is the product of two conditional probabilities: (i) the probability of vocalization, which will vary with species, gender, age, and activity, and (ii) the probability of detecting a given call. Clearly, information is needed on:
• The source level and spectral characteristics of the calls of each species.
• The probability of an animal remaining silent, and how this varies with age and sex.
• The identification and location of individual vocalizing animals that are being tracked using range hydrophones.

The technical approach we adopted to obtain data on these topics involved development of an acoustic recording tag technology, the DTAG, which provides high-fidelity, calibrated on-animal recordings of vocalizations. The DTAG is the optimal method, and in many cases the only method, to obtain data on how often an animal vocalizes and to obtain a clean record of the vocalization with 100% certain species identification. The tag provides the precise times and waveforms of vocalizations together with the depth and orientation of the animal and a number of behavioral cues.  

The second area of need listed in CSSON-01-03 was an assessment of the short- and long-term impacts of defense activities on marine mammals. The methods we proposed for long term assessments of impact included census of marine mammals near the Onslow Bay Shallow Water Test Range (SWTR) and collaborating with NUWC on monitoring location and vocal behavior of marine mammals on the AUTEC range. Our work near SWTR defined one resident population of dolphins, and one migratory population. Our work with NUWC at AUTEC has resulted in their developing the capability of tracking beaked whales acoustically in real-time.

There is a particularly high Navy need to determine responses of deep-diving toothed whales to manmade noise: there are anecdotal reports that these animals may be sensitive to military sonar sounds, and there is no evidence available for setting safe exposure levels. There are several associations of atypical mass strandings of beaked whales with naval maneuvers or sonar operations, but no cause and effect relation has been established (Simmonds and Lopez-Jurado 1991; Frantzis 1996). These atypical mass strandings may involve more than ten animals distributed over tens of kilometers of coastline within a few hours of sonar transmissions ((Cox et al. 2006; Evans & England 2001; Martín et al. 2004); Frantzis, 1998; Martín et al., 2004; Simmonds and Lopez-Jurado, 1991). Other known causes of stranding have been ruled out in some cases, and sonar sounds spread rapidly enough over broad enough ranges to be a potential trigger for strandings with the observed timing and distribution (D’Amico, 1998; Evans and England, 2001).Our ignorance of safe exposure levels for deep diving whales hinders assessment of the potential impact of Navy active acoustic operations, a requirement under NEPA, and it hinders estimating the potential number of “takes” under the Marine Mammal Protection Act and Endangered Species Act.

Research on the behavioral effects of noise on deep diving whales has suffered from a lack of methods to observe behavior in sufficient detail. Many deep diving species are visible for <5% of the time, when they are breathing at the surface, so visual observations are seldom adequate. The part of this project related to preparing for effects studies is based upon an acoustic recording tag for monitoring the sounds a whale makes, measuring the received level of stimuli at the whale and tracking behavior including potential disturbance responses of these deep diving species throughout their dive cycle.  Our analysis of the dive behavior of both Ziphius and Mesoplodon densirostris (Tyack et al. accepted) has helped narrow hypotheses relating diving behavior to risk of decompression (Fernandez et al. 2005; Jackson 1986; Jepson et al. 2003; Jepson et al. 2005). We were able to define an unusual behavioral response of a Ziphius to a ship passing over a diving whale (Aguilar et al. 2006), and have written a white paper defining an approach for studying responses of tagged beaked whales to naval sounds. 

The initial phase of the tagging component of this project involved the first attempts to attach these tags on deep-diving beaked whales. We established field sites in the Ligurian Sea and Canary Islands after initial field efforts in North American sites suggested that they were less promising for initiating field work. Our first beaked whale was tagged in October 2002 in the Ligurian Sea. We have continued to be able to tag Ziphius cavirostris there in 2003, 2004 and 2005. We tagged Mesoplodon densirostris in El Hierro, Canary Islands in the spring of 2003, 2004 and 2005.  While it remains time consuming to obtain opportunities to tag beaked whales in either site, we have learned how to do this routinely, and have built up an excellent data base of tagged whales. We now have a total of 7 tagged individuals from each species studied: Ziphius cavirostris and Mesoplodon densirostris. There is a total of 60 hours of tag data from Ziphius and 70 hours from Mesoplodon. 

Our initial goals also included tagging whales within the AUTEC range to estimate the probability of range sensors detecting their vocalizations.  During March of 2002 we conducted field work at the AUTEC range. We were able to tag pilot whales, but our lack of knowledge concerning the vocalizations of beaked whales made it difficult for personnel monitoring the range hydrophone array to help biologists on vessels to find beaked whales. We attempted another field effort at Abaco in May 2004 in collaboration with the Bahamas Marine Mammal Survey, which knew from previous sighting data where to expect animals. However it was not until our field work in the European sites allowed us to characterize the clicks of beaked whales, that the full potential for field work at AUTEC was realized. We sent these data to D. Moretti’s group at NUWC which incorporated a beaked whale detector into their real time monitoring system for the AUTEC range. We have now conducted two ground truthing cruises there, and on all 9 of the occasions when range monitors identified beaked whale clicks, their location data led the vessels to sight beaked whales. They were even able to conduct a joint follow, with the range monitors following a whale when it was foraging at depth, notifying the visual observers when it stopped clicking and headed towards the surface, and with the visual observers sighting the whale and notifying the range monitors when it dove and was likely to start clicking. These successes finally create a high potential for tagging work at AUTEC.},
   author = {Woods Hole Oceanographic Institution},
   issue = {ADA606315},
   institution = {Defense Technical Information Center},
   month = {11},
   title = {Acoustic Response and Detection of Marine Mammals Using an Advanced Digital Acoustic Recording Tag (Rev 3)},
   url = {https://apps.dtic.mil/sti/pdfs/ADA606315.pdf},
   year = {2007},
}
@article{Berman2024,
   abstract = {Most birds are characterized by a seasonal phenology closely adapted to local climatic conditions, even in tropical habitats where climatic seasonality is slight. In order to better understand the phenologies of resident tropical birds, and how phenology may differ among species at the same site, we used ~70 000 hours of audio recordings collected continuously for two years at four recording stations in Singapore and nine custom-made machine learning classifiers to determine the vocal phenology of a panel of nine resident bird species. We detected distinct seasonality in vocal activity in some species but not others. Native forest species sang seasonally. In contrast, species which have had breeding populations in Singapore only for the last few decades exhibited seemingly aseasonal or unpredictable song activity throughout the year. Urbanization and habitat modification over the last 100 years have altered the composition of species in Singapore, which appears to have influenced phenological dynamics in the avian community. It is unclear what is driving the differences in phenology between these two groups of species, but it may be due to either differences in seasonal availability of preferred foods, or because newly established populations may require decades to adjust to local environmental conditions. Our results highlight the ways that anthropogenic habitat modification may disrupt phenological cycles in tropical regions in addition to altering the species community.},
   author = {Laura Berman and Wei Xuan Tan and Ulmar Grafe and Frank Rheindt},
   doi = {10.1111/jav.03298},
   issn = {1600-048X},
   journal = {Journal of Avian Biology},
   month = {11},
   publisher = {Wiley},
   title = {Acoustic phenology of tropical resident birds differs between native forest species and parkland colonizer species},
   url = {http://dx.doi.org/10.1111/jav.03298},
   year = {2024},
}
@article{Sugimatsu2014,
   abstract = {To understand the biosonar click characteristics of Ganges river dolphins (adults, young adults, and calves) in a wild environment along with periodic visual observations, an ongoing program for long-term in situ monitoring has been carried out using a passive acoustic monitoring (PAM) system. During monitoring phase 4 (2012), migrating Ganges river dolphin groups with small calves were visually observed, and click trains having a short interclick interval (ICI: from 6 to 12 ms) were concurrently found from the acoustic data corresponding to the period. Click trains having a short ICI have also been observed in other small-toothed whales during foraging and socializing activities (called buzz) (Thoms, Moss, & Vater, 2004; Simard & Mann, 2008). For analysis of the short ICI click trains produced by the Ganges river dolphins, an advanced technique that automatically detects and discriminates a “short ICI click train” from other click sequences during the selected periods of data recorded by a PAM system was developed. For a robust algorithm, a smaller mean ICI caused by overlapping click trains from multiple dolphins that fulfill the range of ICI values that may get incorrectly labeled as “short ICI click train” was considered to judiciously detect a reliable click train. By applying the selected parameters and ICI values (default or given), the performance of the proposed technique was demonstrated using sample data. The results showed the reliability of the technique for the extraction of a variety of short ICI click trains from other click trains.},
   author = {Harumi Sugimatsu and Junichi Kojima and Tamaki Ura and R Bahl and Sandeep Behera and Vivek Sagar and Hari Singh and Rupak De},
   doi = {10.4031/MTSJ.48.3.15},
   journal = {Marine Technology Society Journal},
   month = {11},
   title = {Advanced Technique for Automatic Detection and Discrimination of a Click Train With Short Interclick Intervals From the Clicks of Ganges River Dolphins (Platanista gangetica gangetica) Recorded by a Passive Acoustic Monitoring System Using Hydrophone Arrays},
   url = {https://doi.org/10.4031/MTSJ.48.3.15},
   year = {2014},
}
@article{Singer2024,
   abstract = {Passive acoustic monitoring (PAM) has gained increasing popularity to study behaviour, habitat preferences, distribution and community assembly of birds and other animals. Automated species classification algorithms like ‘BirdNET’ are capable of detecting and classifying avian vocalizations within extensive audio data, covering entire species assemblages. PAM reveals substantial potential for biodiversity monitoring that informs evidence-based conservation. Nevertheless, fully realizing this potential remains challenging, especially due to the issue of false-positive species detections. Here, we introduce an optimized thresholding framework, which incorporates contextual information extracted from the time-series of automated species detections (i.e. covariates on quality and quantity of species' detections measured at varying time intervals) to improve the differentiation of true and false positives. We verified a sample of BirdNET detections per species and modelled species-specific thresholds using conditional inference trees. These thresholds were designed to minimize false-positive detections while maximizing the preservation of true positives in the dataset. We tested this framework for a large dataset of BirdNET detections (5760 h of audio data, 60 sites) recorded over an entire breeding season. Our results revealed considerable interspecific variability of precision (percentage of true positives) within raw BirdNET data. Our optimized thresholding approach achieved high precision (≥0.9) for 70% of the 61 detected species, while species-specific thresholds solely relying on the BirdNET confidence scores achieved high precision for only 31% of the species. Conservative universal thresholds (not species-specific) reached high precision for 48% of the species. Our thresholding approach outperformed previous thresholding approaches and enhanced interspecific comparability for bird community analyses. By incorporating contextual information from the time-series of species detections, the differentiation of true and false positives was substantially improved. Our approach may enhance a straightforward application of PAM in biodiversity research, landscape planning and evidence-based conservation.},
   author = {David Singer and Jonas Hagge and Johannes Kamp and Hermann Hondong and Andreas Schuldt},
   doi = {10.1002/rse2.385},
   issn = {2056-3485},
   journal = {Remote Sensing in Ecology and Conservation},
   month = {11},
   pages = {517-530},
   publisher = {Wiley},
   title = {Aggregated time-series features boost species-specific differentiation of true and false positives in passive acoustic monitoring of bird assemblages},
   url = {http://dx.doi.org/10.1002/rse2.385},
   year = {2024},
}
@article{Wu2023,
   abstract = {Long-term monitoring is needed to understand the statuses and trends of wildlife communities in montane forests, such as those in Yushan National Park (YSNP), Taiwan. Integrating passive acoustic monitoring (PAM) with an automated sound identifier, a long-term biodiversity monitoring project containing six PAM stations, was launched in YSNP in January 2020 and is currently ongoing. SILIC, an automated wildlife sound identification model, was used to extract sounds and species information from the recordings collected. Animal vocal activity can reflect their breeding status, behaviour, population, movement and distribution, which may be affected by factors, such as habitat loss, climate change and human activity. This massive amount of wildlife vocalisation dataset can provide essential information for the National Park's headquarters on resource management and decision-making. It can also be valuable for those studying the effects of climate change on animal distribution and behaviour at a regional or global scale.To our best knowledge, this is the first open-access dataset with species occurrence data extracted from sounds in soundscape recordings by artificial intelligence. We obtained seven bird species for the first release, with more bird species and other taxa, such as mammals and frogs, to be updated annually. Raw recordings containing over 1.7 million one-minute recordings collected between the years 2020 and 2021 were analysed and SILIC identified 6,243,820 vocalisations of seven bird species in 439,275 recordings. The automatic detection had a precision of 0.95 and the recall ranged from 0.48 to 0.80. In terms of the balance between precision and recall, we prioritised increasing precision over recall in order to minimise false positive detections. In this dataset, we summarised the count of vocalisations detected per sound class per recording which resulted in 802,670 occurrence records. Unlike data from traditional human observation methods, the number of observations in the Darwin Core "organismQuantity" column refers to the number of vocalisations detected for a specific bird species and cannot be directly linked to the number of individuals.We expect our dataset will be able to help fill the data gaps of fine-scale avian temporal activity patterns in montane forests and contribute to studies concerning the impacts of climate change on montane forest ecosystems on regional or global scales.},
   author = {Shih-Hung Wu and Jerome Chie-Jen Ko and Ruey-Shing Lin and Wen-Ling Tsai and Hsueh-Wen Chang},
   doi = {10.3897/BDJ.11.e97811},
   issn = {1314-2836},
   journal = {Biodiversity Data Journal},
   pages = {e97811},
   publisher = {Pensoft Publishers},
   title = {An acoustic detection dataset of birds (Aves) in montane forests using a deep learning approach},
   url = {https://doi.org/10.3897/BDJ.11.e97811},
   year = {2023},
}
@article{Lin2013,
   abstract = {Most studies on tonal sounds extract contour parameters from fundamental frequencies. The presence of harmonics and the frequency distribution of multiple tonal sounds have not been well researched. To investigate the occurrence and frequency modulation of cetacean tonal sounds, the procedure of detecting the instantaneous frequency bandwidth of tonal spectral peaks was integrated within the local-max detector to extract adopted frequencies. The adopted frequencies, considered the representative frequencies of tonal sounds, are used to find the presence of harmonics and overlapping tonal sounds. The utility and detection performance are demonstrated on acoustic recordings of five species from two databases. The recordings of humpback dolphins showed a 75% detection rate with a 5% false detection rate, and recordings from the MobySound archive showed an 85% detection rate with a 5% false detection rate. These detections were achieved in signal-to-noise ratios of −12 to 21 dB. The parameters that measured the distribution of adopted frequency, as well as the prominence of harmonics and overlaps, indicate that the modulation of tonal sounds varied among different species and behaviors. This algorithm can be applied to studies on cetacean communication signals and long-term passive acoustic monitoring.},
   author = {Tzu-Hao Lin and Lien-Siang Chou and Tomonari Akamatsu and Hsiang-Chih Chan and Chi-Fang Chen},
   doi = {10.1121/1.4816572},
   issn = {1520-8524},
   journal = {The Journal of the Acoustical Society of America},
   month = {11},
   pages = {2477-2485},
   publisher = {Acoustical Society of America (ASA)},
   title = {An automatic detection algorithm for extracting the representative frequency of cetacean tonal sounds},
   url = {http://dx.doi.org/10.1121/1.4816572},
   year = {2013},
}
@article{Kershenbaum2013,
   abstract = {Dolphins and whales use tonal whistles for communication, and it is known that frequency modulation encodes contextual information. An automated mathematical algorithm could characterize the frequency modulation of tonal calls for use with clustering and classification. Most automatic cetacean whistle processing techniques are based on peak or edge detection or require analyst assistance in verifying detections. An alternative paradigm is introduced using techniques of image processing. Frequency information is extracted as ridges in whistle spectrograms. Spectral ridges are the fundamental structure of tonal vocalizations, and ridge detection is a well-established image processing technique, easily applied to vocalization spectrograms. This paradigm is implemented as freely available matlab scripts, coined IPRiT (image processing ridge tracker). Its fidelity in the reconstruction of synthesized whistles is compared to another published whistle detection software package, silbido. Both algorithms are also applied to real-world recordings of bottlenose dolphin (Tursiops trunactus) signature whistles and tested for the ability to identify whistles belonging to different individuals. IPRiT gave higher fidelity and lower false detection than silbido with synthesized whistles, and reconstructed dolphin identity groups from signature whistles, whereas silbido could not. IPRiT appears to be superior to silbido for the extraction of the precise frequency variation of the whistle.},
   author = {Arik Kershenbaum and Marie A Roch},
   doi = {10.1121/1.4828821},
   issn = {1520-8524},
   journal = {The Journal of the Acoustical Society of America},
   month = {11},
   pages = {4435-4445},
   publisher = {Acoustical Society of America (ASA)},
   title = {An image processing based paradigm for the extraction of tonal sounds in cetacean communications},
   url = {http://dx.doi.org/10.1121/1.4828821},
   year = {2013},
}
@article{Diepstraten2021,
   abstract = {Four field seasons of snow leopard (Panthera uncia) camera trapping inside Naryn State Nature Reserve, Kyrgyzstan, performed thanks to citizen science expeditions, allowed detecting a minimal population of five adults, caught every year with an equilibrated sex ratio (1.5:1) and reproduction: five cubs or subadults have been identified from three litters of two different females. Crossings were observed one to three times a year, in front of most camera traps, and several times a month in front of one of them. Overlap of adults’ minimal territories was observed in front of several camera traps, regardless of their sex. Significant snow leopard presence was detected in the buffer area and at Ulan area which is situated at the reserve border. To avoid poaching on this apex predator and its preys, extending the more stringent protection measures of the core zone to both the Southern buffer area and land adjacent to Ulan is recommended.},
   author = {Johan Diepstraten and Jacob Willie},
   doi = {10.1016/j.gecco.2021.e01850},
   journal = {Global Ecology and Conservation},
   pages = {e01850},
   title = {Assessing the structure and drivers of biological sounds along a disturbance gradient},
   url = {https://doi.org/10.1016/j.gecco.2021.e01850},
   year = {2021},
}
@misc{Scheidat2009,
   abstract = {The aim of this study was to investigate if the Offshore Wind farm Egmond aan Zee (OWEZ) influenced the occurrence of harbour porpoises. In order to evaluate the environmental impacts of OWEZ, porpoise occurrence in the area was monitored: during a baseline (T0) study 2003/2004 (Brasseur at al. 2004) after the construction of the wind park (T1) from 2007 to 2009. The comparison between the T0 and the T1 was conducted to determine if and how harbour porpoises react to the presence of the wind park. This report describes the results and analyses of this comparison.},
   author = {M Scheidat and G Aarts and A Bakker and S Brasseur and J Carstensen and P van Leeuwen and M Leopold and T van Polanen Petel and P Reijnders and J Teilmann and J Tougaard and H Verdaat},
   institution = {IMARES - Wageningen UR},
   keywords = {Fixed Offshore Wind,Habitat Change,Marine Mammals,Wind Energy},
   month = {11},
   title = {Assessment of the Effects of the Offshore Wind Farm Egmond aan Zee (OWEZ) for Harbour Porpoise (comparison T0 and T1)},
   url = {https://tethys.pnnl.gov/sites/default/files/publications/Assessment-of-OSW-farm.pdf},
   year = {2009},
}
@article{Mcloughlin2019,
   abstract = {Vocalizations carry emotional, physiological and individual information. This suggests that they may serve as potentially useful indicators for inferring animal welfare. At the same time, automated methods for analysing and classifying sound have developed rapidly, particularly in the fields of ecology, conservation and sound scene classification. These methods are already used to automatically classify animal vocalizations, for example, in identifying animal species and estimating numbers of individuals. Despite this potential, they have not yet found widespread application in animal welfare monitoring. In this review, we first discuss current trends in sound analysis for ecology, conservation and sound classification. Following this, we detail the vocalizations produced by three of the most important farm livestock species: chickens (Gallus gallus domesticus), pigs (Sus scrofa domesticus) and cattle (Bos taurus). Finally, we describe how these methods can be applied to monitor animal welfare with new potential for developing automated methods for large-scale farming.},
   author = {Michael P Mcloughlin and Rebecca Stewart and Alan G McElligott},
   doi = {10.1098/rsif.2019.0225},
   issn = {1742-5662},
   journal = {Journal of The Royal Society Interface},
   month = {11},
   pages = {20190225},
   publisher = {The Royal Society},
   title = {Automated bioacoustics: methods in ecology and conservation and their potential for animal welfare monitoring},
   url = {http://dx.doi.org/10.1098/rsif.2019.0225},
   year = {2019},
}
@inproceedings{,
   abstract = {Ecological characteristics favor high biodiversity on
the Caribbean slope of Costa Rica, but this piedmont zone
is poorly studied. In birds, the use of automated song and
call recognition has progressed to support bird studies about
ecology and behavior. We used a Pattern Matching method to
label the presence of the Cinnamon Woodpecker, Great-Green
Macaw, Red-capped Manakin and Chestnut-backed Antbird to
create a random forest model and detect the species’ vocal-
izations to characterize their vocal activity in the reserve. We
used Audiomoth recorders. For all acoustic detection models,
accuracy, and precision values above 91% were obtained despite
the imbalance of positive and negative classes, the value of
Unweighted Average Recall was high for each model. Three sites
showed the highest number of detections and the number of
detections per site varied among species. A preference of use for
some sites within the reserve was identified for A. ambiguus and
C. mentalis and a more generalist use for C. loricatus and P.
exsul.For each species, greater acoustic activity was found in the
morning hours with less activity in the afternoon hours for some
of the species with a peak of activity between February to May.
The acoustic detection patterns found agreed with the literature
for each species when analyzing the ecological behaviors inside
and outside the breeding season of birds in Costa Rica. This
acoustic information will improve conservation decision making
for the species involved and other species that develop in these
ecosystems.},
   author = {Roberto Vargas-Masís and David Segura-Sequeira and Danny Alfaro-Rojas and Daniel Rosen Díaz},
   doi = {10.1109/BIP56202.2022.10032472},
   booktitle = {2022 IEEE 4th International Conference on BioInspired Processing (BIP)},
   keywords = {Biological system modeling;Ecosystems;Decision making;Birds;Acoustics;Ecology;Behavioral sciences;Acoustic detection;Passive acoustic monitoring;Random forest model;Pattern matching;conservation;Costa Rica},
   pages = {1-8},
   title = {Automated bird acoustic detection at Las Arrieras Nature Reserve in Sarapiquí, Costa Rica},
   url = {https://ieeexplore.ieee.org/document/10032472},
   year = {2022},
}
@article{,
   abstract = {Passive acoustic monitoring (PAM) – an approach that uses autonomous acoustic recording units (ARUs) – can provide insights into the behavior of cryptic or endangered species that produce loud calls. However, extracting useful information from PAM data often requires substantial human effort, along with effective estimates of the detection range of the acoustic units, which can be challenging to obtain. We studied the duetting behavior of pair-living red titi monkeys (Plecturocebus discolor) using PAM coupled with an open-source automated detection tool. Using data on spontaneous duetting by one titi pair, combined with recordings from two Song Meter SM2 ARUs placed within their home range, we estimated that the average source level of titi duets was ~105 dB re 20 μPa at 1 m with an attenuation rate of 8 dB per doubling of distance, and we determined that the detection radius for manual annotation of duets in audio recordings was at least 125 to 200 m, depending on the approach used. We also used a supervised template-based detection algorithm (binary point matching) to evaluate the efficacy of automated detection for titi duets in audio recordings using linear arrays of ARUs within a ~2 km2 area. We used seven titi duet templates and a set of “off-target” howler monkey (Alouatta seniculus) templates to reduce false positive results. For duets with a signal-to-noise (SNR) ratio > 10 dB (corresponding to a detection radius of ~125 m) our detection approach had a recall (the number of all duets that are correctly detected) of 1.0. Performance decreased when including duets with a lower SNR (recall = 0.71, precision = 0.75). The fact that multiple lines of evidence suggest an effective detection radius of 125 to 200 m for titi duets across upland terra firme and seasonally flooded forest lends support to our findings. We suggest that PAM studies of other cryptic but vocally active species would benefit from following similar experimental and analytic procedures to determine an ARU’s effective detection radius and to improve the performance of automated detection algorithms.},
   author = {Silvy M van Kuijk and Sun O'Brien and Dena J Clink and John G Blake and Anthony Di Fiore},
   doi = {10.3389/fevo.2023.1173722},
   issn = {2296-701X},
   journal = {Frontiers in Ecology and Evolution},
   month = {11},
   publisher = {Frontiers Media SA},
   title = {Automated detection and detection range of primate duets: a case study of the red titi monkey (Plecturocebus discolor) using passive acoustic monitoring},
   url = {http://dx.doi.org/10.3389/fevo.2023.1173722},
   year = {2023},
}
@inproceedings{Huang2016,
   abstract = {a novel approach has been developed for detecting and classifying foraging calls of two mysticete species in passive acoustic recordings. This automated detector/classifier applies a computer-vision based technique, a pattern recognition method, to detect the foraging calls and remove ambient noise effects. The detected calls were then classified as blue whale D-calls [1] or fin whale 40-Hz calls [2] using a logistic regression classifier, a machine learning technique. The detector/classifier has been trained using the 2015 Detection, Classification, Localization and Density Estimation (DCLDE 2015, Scripps Institution of Oceanography UCSD [3]) low-frequency annotated set of passive acoustic data, collected in the Southern California Bight, and its out-of-sample performance was estimated by using a cross-validation technique. The DCLDE 2015 scoring tool was used to estimate the detector/classifier performance in a standardized way. The pattern recognition algorithm's out-of-sample performance was scored as 96.68% recall with 92.03 % precision. The machine learning algorithm's out-of-sample prediction accuracy was 95.20%. The result indicated the potential of this detector/classifier on real-time passive acoustic marine mammal monitoring and bioacoustics signal processing.},
   author = {Ho Chun Huang and John Joseph and Ming Jer Huang and Tetyana Margolina},
   doi = {10.1109/oceans.2016.7761269},
   booktitle = {OCEANS 2016 MTS/IEEE Monterey},
   month = {11},
   pages = {1-7},
   publisher = {IEEE},
   title = {Automated detection and identification of blue and fin whale foraging calls by combining pattern recognition and machine learning techniques},
   url = {https://doi.org/10.1109/OCEANS.2016.7761269},
   year = {2016},
}
@article{Barker2014,
   abstract = {Background
Ultrasonic vocalizations (USVs) have been utilized to infer animals’ affective states in multiple research paradigms including animal models of drug abuse, depression, fear or anxiety disorders, Parkinson's disease, and in studying neural substrates of reward processing. Currently, the analysis of USV data is performed manually, and thus is time consuming.
New method
The goal of the present study was to develop a method for automated USV recognition using a ‘template detection’ procedure for vocalizations in the 50-kHz range (35–80 kHz). The detector is designed to run within XBAT, a MATLAB graphical user interface and extensible bioacoustics tool developed at Cornell University.
Results
Results show that this method is capable of detecting >90% of emitted USVs and that time spent analyzing data by experimenters is greatly reduced.
Comparison with existing methods
Currently, no viable and publicly available methods exist for the automated detection of USVs. The present method, in combination with the XBAT environment is ideal for the USV community as it allows others to (1) detect USVs within a user-friendly environment, (2) make improvements to the detector and disseminate and (3) develop new tools for analysis within the MATLAB environment.
Conclusions
The present detector provides an open-source, accurate method for the detection of 50-kHz USVs. Ongoing research will extend the current method for use in the 22-kHz frequency range of ultrasonic vocalizations. Moreover, collaborative efforts among USV researchers may enhance the capabilities of the current detector via changes to the templates and the development of new programs for analysis.},
   author = {David J Barker and Christopher Herrera and Mark O West},
   doi = {10.1016/j.jneumeth.2014.08.007},
   issn = {0165-0270},
   journal = {Journal of Neuroscience Methods},
   month = {11},
   pages = {68-75},
   publisher = {Elsevier BV},
   title = {Automated detection of 50-kHz ultrasonic vocalizations using template matching in XBAT},
   volume = {236},
   url = {http://dx.doi.org/10.1016/j.jneumeth.2014.08.007},
   year = {2014},
}
@article{Owens2024,
   abstract = {Passive acoustic monitoring is a promising tool for monitoring at-risk populations of vocal species, yet extracting relevant information from large acoustic datasets can be time-consuming, creating a bottleneck at the point of analysis. To address this, we adapted an open-source framework for deep learning in bioacoustics to automatically detect Bornean white-bearded gibbon (Hylobates albibarbis) great call vocalisations in a long-term acoustic dataset from a rainforest location in Borneo. We describe the steps involved in developing this solution, including collecting audio recordings, developing training and testing datasets, training neural network models, and evaluating model performance. Our best model performed at a satisfactory level (F score = 0.87), identifying 98% of the highest-quality calls from 90 hours of manually-annotated audio recordings and greatly reduced analysis times when compared to a human observer. We found no significant difference in the temporal distribution of great call detections between the manual annotations and the models output. Future work should seek to apply our model to long-term acoustic datasets to understand spatiotemporal variations in H. albibarbis calling activity. Overall, we present a roadmap for applying deep learning to identify the vocalisations of species of interest which can be adapted for monitoring other endangered vocalising species.Competing Interest StatementThe authors have declared no competing interest.},
   author = {A F Owens and Kimberley J Hockings and Muhammed Ali Imron and Shyam Madhusudhana and Mariaty and Tatang Mitra Setia and Manmohan Sharma and Siti Maimunah and F J F Van Veen and Wendy M Erb},
   doi = {10.1101/2024.04.15.589517},
   journal = {bioRxiv},
   publisher = {Cold Spring Harbor Laboratory},
   title = {Automated detection of Bornean white-bearded gibbon (Hylobates albibarbis) vocalisations using an open-source framework for deep learning},
   url = {https://www.biorxiv.org/content/early/2024/07/21/2024.04.15.589517},
   year = {2024},
}
@article{Stowell2018,
   abstract = {Abstract Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus, passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here, we report outcomes from a collaborative data challenge. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects. Multiple methods were able to attain performance of around 88% area under the receiver operating characteristic (ROC) curve (AUC), much higher performance than previous general-purpose methods. With modern machine learning, including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data, with no manual recalibration, and no pretraining of the detector for the target species or the acoustic conditions in the target environment.},
   author = {Dan Stowell and Michael D Wood and Hanna Pamuła and Yannis Stylianou and Hervé Glotin},
   doi = {10.1111/2041-210x.13103},
   editor = {David Orme},
   issn = {2041-210X},
   journal = {Methods in Ecology and Evolution},
   keywords = {bird,deep learning,machine learning,passive acoustic monitoring,sound},
   month = {11},
   pages = {368-380},
   publisher = {Wiley},
   title = {Automatic acoustic detection of birds through deep learning: The first Bird Audio Detection challenge},
   url = {http://dx.doi.org/10.1111/2041-210X.13103},
   year = {2018},
}
@article{Fundel2023,
   abstract = {Automatically identifying bat species from their echolocation calls is a difficult but important task for monitoring bats and the ecosystem they live in. Major challenges in automatic bat call identification are high call variability, similarities between species, interfering calls and lack of annotated data. Many currently available models suffer from relatively poor performance on real-life data due to being trained on single call datasets and, moreover, are often too slow for real-time classification. Here, we propose a Transformer architecture for multi-label classification with potential applications in real-time classification scenarios. We train our model on synthetically generated multi-species recordings by merging multiple bats calls into a single recording with multiple simultaneous calls. Our approach achieves a single species accuracy of 88.92% (F1-score of 84.23%) and a multi species macro F1-score of 74.40% on our test set. In comparison to three other tools on the independent and publicly available dataset ChiroVox, our model achieves at least 25.82% better accuracy for single species classification and at least 6.9% better macro F1-score for multi species classification.},
   author = {Frank Fundel and Daniel A Braun and Sebastian Gottwald},
   doi = {https://doi.org/10.1016/j.ecoinf.2023.102288},
   issn = {1574-9541},
   journal = {Ecological Informatics},
   keywords = {Acoustic monitoring,Attention,Bat calls,Computational bioacoustics,Echolocation,Species identification,Transformer},
   title = {Automatic bat call classification using transformer networks},
   url = {https://www.sciencedirect.com/science/article/pii/S1574954123003175},
   year = {2023},
}
@article{,
   abstract = {The bioacoustic analyses of animal sounds result in an enormous amount of digitized acoustic data, and we need effective automatic processing to extract the information content of the recordings. Our research focuses on the song of Collared Flycatcher (Ficedula albicollis) and we are interested in the evolution of acoustic signals. During the last 20 years, we obtained hundreds of hours of recordings of bird songs collected in natural environment, and there is a permanent need for the automatic process of recordings. In this study, we chose an open-source, deep-learning image detection system to (1) find the species-specific songs of the Collared Flycatcher on the recordings and (2) to detect the small, discrete elements so-called syllables within the song. For these tasks, we first transformed the acoustic data into spectrogram images, then we trained two deep-learning models separately on our manually segmented database. The resulted models detect the songs with an intersection of union higher than 0.8 and the syllables higher than 0.7. This technique anticipates an order of magnitude less human effort in the acoustic processing than the manual method used before. Thanks to the new technique, we are able to address new biological questions that need large amount of acoustic data.},
   author = {Sándor Zsebők and Máté Ferenc Nagy-Egri and Gergely Gábor Barnaföldi and Miklós Laczi and Gergely Nagy and Éva Vaskuti and László Zsolt Garamszegi},
   doi = {10.2478/orhu-2019-0015},
   issn = {2061-9588},
   issue = {2},
   journal = {Ornis Hungarica},
   month = {11},
   pages = {59-66},
   publisher = {Walter de Gruyter GmbH},
   title = {Automatic bird song and syllable segmentation with an open-source deep-learning object detection method - a case study in the Collared Flycatcher (Ficedula albicollis)},
   volume = {27},
   url = {http://dx.doi.org/10.2478/orhu-2019-0015},
   year = {2019},
}
@inproceedings{Jancovic2017,
   abstract = {This paper presents an automatic system for detection of bird species in field recordings. A sinusoidal detection algorithm is employed to segment the acoustic scene into isolated spectro-temporal segments. Each segment is represented as a temporal sequence of frequencies of the detected sinusoid, referred to as frequency track. Each bird species is represented by a set of hidden Markov models (HMMs), each HMM modelling an individual type of bird vocalisation element. These HMMs are obtained in an unsupervised manner. The detection is based on a likelihood ratio of the test utterance against the target bird species and non-target background model. We explore on selection of cohort for modelling the background model, z-norm and t-norm score normalisation techniques and score compensation to deal with outlier data. Experiments are performed using over 40 hours of audio field recordings from 48 bird species plus an additional 16 hours of field recordings as impostor trials. Evaluations are performed using detection error trade-off plots. The equal error rate of 5% is achieved when impostor trials are non-target bird species vocalisations and 1.2% when using field recordings which do not contain bird vocalisations.},
   author = {Peter Jancovic and Munevver Kokuer},
   doi = {10.23919/eusipco.2017.8081515},
   booktitle = {2017 25th European Signal Processing Conference (EUSIPCO)},
   keywords = {Hidden Markov models;Birds;Feature extraction;Training;Acoustics;Data models;bird species detection;field recording;hidden Markov model;HMM;score normalisation;cohort;outlier;vocalisation;element;unsupervised training;sinusoid detection;sinusoidal modelling;frequency track},
   month = {11},
   pages = {1779-1783},
   publisher = {IEEE},
   title = {Automatic detection of bird species from audio field recordings using HMM-based modelling of frequency tracks},
   url = {http://dx.doi.org/10.23919/EUSIPCO.2017.8081515},
   year = {2017},
}
@inproceedings{Ntalampiras2020,
   abstract = {Precision livestock farming dictates the use of advanced technologies to understand, analyze, assess and finally optimize a farm’s production collectively as well as the contribution of each single animal. This Work is part of a research project wishing to steer the dairy farms’ producers to more ethical rearing systems. To study cow’s welfare, we focus on reciprocal vocalizations including mother-offspring contact calls. We show the set-up of a suitable audio capturing system composed of automated recording units and propose an algorithm to automatically detect cow vocalizations in an indoor farm setting. More specifically, the algorithm has a two-level structure: a) first, the Hilbert follower is applied to segment the raw audio signals, and b) second the detected blocks of acoustic activity are refined via a classification scheme based on hidden Markov models. After thorough evaluation, we demonstrate excellent detection results in terms of false positives, false negatives and confusion matrix.},
   author = {Stavros Ntalampiras and Andrea Pezzuolo and Silvana Mattiello and Monica Battini and Marta Brscic},
   doi = {10.1109/tsp49548.2020.9163522},
   booktitle = {2020 43rd International Conference on Telecommunications and Signal Processing (TSP)},
   keywords = {Ethics;Animals;Signal processing algorithms;Hidden Markov models;Production;Agriculture;Acoustics;Precision livestock farming;cow/calf vocalization;acoustic signal processing;vocalization detection;audio pattern recognition},
   month = {11},
   pages = {41-45},
   publisher = {IEEE},
   title = {Automatic detection of cow/calf vocalizations in free-stall barn},
   url = {http://dx.doi.org/10.1109/TSP49548.2020.9163522},
   year = {2020},
}
@article{Mouy2009,
   abstract = {Monitoring blue and fin whales summering in the St. Lawrence Estuary with passive acoustics requires call recognition algorithms that can cope with the heavy shipping noise of the St. Lawrence Seaway and with multipath propagation characteristics that generate overlapping copies of the calls. In this paper, the performance of three time-frequency methods aiming at such automatic detection and classification is tested on more than 2000 calls and compared at several levels of signal-to-noise ratio using typical recordings collected in this area. For all methods, image processing techniques are used to reduce the noise in the spectrogram. The first approach consists in matching the spectrogram with binary time-frequency templates of the calls (coincidence of spectrograms). The second approach is based on the extraction of the frequency contours of the calls and their classification using dynamic time warping (DTW) and the vector quantization (VQ) algorithms. The coincidence of spectrograms was the fastest method and performed better for blue whale A and B calls. VQ detected more 20 Hz fin whale calls but with a higher false alarm rate. DTW and VQ outperformed for the more variable blue whale D calls.},
   author = {Xavier Mouy and Mohammed Bahoura and Yvan Simard},
   doi = {10.1121/1.3257588},
   issn = {1520-8524},
   journal = {The Journal of the Acoustical Society of America},
   month = {11},
   pages = {2918-2928},
   publisher = {Acoustical Society of America (ASA)},
   title = {Automatic recognition of fin and blue whale calls for real-time monitoring in the St. Lawrence},
   url = {http://dx.doi.org/10.1121/1.3257588},
   year = {2009},
}
@article{Tani2013,
   abstract = {Monitoring the eating habits and rumination of cattle is effective for evaluating forage values and making animal management decisions for stalls and grazing pastures. Acoustic monitoring is a feasible method to monitor these activities, but it requires automatic distinction and quantification of activities to gain broader application. This automatic distinction and quantification may be possible using a pattern matching method, which distinguishes jaw movements by extracting characteristic patterns from eating and ruminating, and then matches similar characteristic patterns in unanalyzed activities. The objectives of this study were to define an acoustic monitoring system with a single-axis acceleration sensor, to determine an effective sensor attachment site (i.e., the horn, forehead, or nasal bridge), and to assess the automatic classification of ingestive and ruminative chewing behaviors using a pattern matching method. Four beef cows fed Italian ryegrass silage were each recorded eating and ruminating for more than 60 and 20 min, respectively. The oscillatory waveforms generated while eating and ruminating were clearly recorded by the single-axis acceleration sensor, regardless of sensor attachment site. The pattern matching method correctly distinguished ingestive chewing with an accuracy of over 0.9, regardless of the sensor site (estimated number of chews by automatic detection/true number of chews by manual detection). Placement of the sensor on the horn of the animal resulted in the most matched measurement (0.99). In contrast, the method overestimated ruminating chewing time (approximately 1.50) because noises that occurred during idling were misclassified as ruminating chewing. The provision of an additional chewing criterion for automatic detection, i.e., chewing is performed consecutively (context-based cues), improved the classification accuracy of ruminating to a range of 1.02–1.08. In conclusion, automatic detection using a pattern matching method successfully classified ingestive chewing and ruminating chewing in cattle, and a single-axis acceleration sensor was useful for acoustic monitoring at all sensor attachment sites.},
   author = {Yukinori Tani and Yasunari Yokota and Masato Yayota and Shigeru Ohtani},
   doi = {10.1016/j.compag.2013.01.001},
   issn = {0168-1699},
   journal = {Computers and Electronics in Agriculture},
   month = {11},
   pages = {54-65},
   publisher = {Elsevier BV},
   title = {Automatic recognition and classification of cattle chewing activity by an acoustic monitoring method with a single-axis acceleration sensor},
   url = {http://dx.doi.org/10.1016/j.compag.2013.01.001},
   year = {2013},
}
@inproceedings{S2024,
   abstract = {Studies on bird sound classification have become increasingly popular in recent years. Birds serve as valuable ecological indicators, and the threat to bird species populations is a serious concern. The emergence of artificial intelligence has been a milestone in species classification, making the classification easier and more accurate. For the acoustic classification of bird species, a number of works have been reported in the literature but there are still challenges. In this, we propose a deep learning approach for bird sound classification. A dataset consisting of recordings of 10 bird species is used for classification. The dataset is sourced from recordings on the Xenocanto website. Many bird audio recordings are available for studying their existence, but classifying these recordings is challenging due to the overlapping calls from multiple birds. A different approach is used in the proposed work for preprocessing the downloaded audio. BirdNet, which can recognize over 6500 birds, is employed for separating the audio of the corresponding species from the overlappings of the multiple species present in the downloaded audio files. The BirdNet separated recordings of the 10 species are then converted into spectrograms. The obtained spectrograms are then fed into a CNN model for classification. The model achieved an average accuracy of 90% for the ten bird species.},
   author = {Salini S and Suresh K},
   doi = {10.1109/APCI61480.2024.10617409},
   booktitle = {2024 International Conference on Advancements in Power, Communication and Intelligent Systems (APCI)},
   keywords = {Training;Deep learning;Accuracy;Biological system modeling;Sociology;Birds;Audio recording;BAT-CNN;BirdNet;Bird Sound Classification;MelSpectrogram;Xeno-Canto},
   pages = {1-5},
   title = {BAT-CNN: BirdNet Assisted Training for CNN},
   url = {https://ieeexplore.ieee.org/document/10617409},
   year = {2024},
}
@article{,
   abstract = {assive acoustic sensing has emerged as a powerful tool for quantifying anthropogenic impacts on biodiversity, especially for echolocating bat species. To better assess bat population trends there is a critical need for accurate, reliable, and open source tools that allow the detection and classification of bat calls in large collections of audio recordings. The majority of existing tools are commercial or have focused on the species classification task, neglecting the important problem of first localizing echolocation calls in audio which is particularly problematic in noisy recordings. We developed a convolutional neural network based open-source pipeline for detecting ultrasonic, full-spectrum, search-phase calls produced by echolocating bats. Our deep learning algorithms were trained on full-spectrum ultrasonic audio collected along road-transects across Europe and labelled by citizen scientists from www.batdetective.org. When compared to other existing algorithms and commercial systems, we show significantly higher detection performance of search-phase echolocation calls with our test sets. As an example application, we ran our detection pipeline on bat monitoring data collected over five years from Jersey (UK), and compared results to a widely-used commercial system. Our detection pipeline can be used for the automatic detection and monitoring of bat populations, and further facilitates their use as indicator species on a large scale. Our proposed pipeline makes only a small number of bat specific design decisions, and with appropriate training data it could be applied to detecting other species in audio. A crucial novelty of our work is showing that with careful, non-trivial, design and implementation considerations, state-of-the-art deep learning methods can be used for accurate and efficient monitoring in audio.},
   author = {Oisin Mac Aodha and Rory Gibb and Kate E Barlow and Ella Browning and Michael Firman and Robin Freeman and Briana Harder and Libby Kinsey and Gary R Mead and Stuart E Newson and Ivan Pandourski and Stuart Parsons and Jon Russ and Abigel Szodoray-Paradi and Farkas Szodoray-Paradi and Elena Tilova and Mark Girolami and Gabriel Brostow and Kate E Jones},
   doi = {10.1371/journal.pcbi.1005995},
   editor = {Brock Fenton},
   issn = {1553-7358},
   journal = {PLOS Computational Biology},
   month = {11},
   pages = {e1005995},
   publisher = {Public Library of Science (PLoS)},
   title = {Bat detective—Deep learning tools for bat acoustic signal detection},
   url = {http://dx.doi.org/10.1371/journal.pcbi.1005995},
   year = {2018},
}
@article{Enari2023,
   abstract = {Some nonhuman primate species, whose original habitats have been reclaimed by artificial activities, have acquired boldness toward humans which is evident based on the diminished frequency of escape behaviors. Eventually, such species have become regular users of human settlements, and are referred to as “urban primates.” Considering this, we developed a noninvasive technique based on bioacoustics to provide a transparent assessment of troop addiction levels in anthropogenic environments, which are determined by the dependence on agricultural crops and human living sphere for their diets and daily ranging, respectively. We attempted to quantify the addiction levels based on the boldness of troops when raiding settlements, characterized by a “landscape of fear” because of the presence of humans as predators. We hypothesized that the boldness of troops could be measured using two indices: the frequency of raiding events on settlements and the amount of time spent there. For hypothesis testing, we devised an efficient method to measure these two indices using sound cues (i.e., spontaneous calls) for tracing troop movements that are obtainable throughout the day from most primate species (e.g., contact calls). We conducted a feasibility study of this assessment procedure, targeting troops of Japanese macaques (Macaca fuscata). For this study, we collected 346 recording weeks of data using autonomous recorders from 24 troops with different addiction levels during the nonsnowy seasons. The results demonstrated that troops that reached the threshold level, at which radical interventions including mass culling of troop members is officially permitted, could be readily identified based on the following behavioral characteristics: troop members raiding settlements two or three times per week and mean time spent in settlements per raiding event exceeding 0.4 h. Thus, bioacoustic monitoring could become a valid option to ensure the objectivity of policy judgment in urban primate management.},
   author = {Hiroto Enari and Haruka S Enari},
   doi = {10.1002/ajp.23558},
   issn = {1098-2345},
   journal = {American Journal of Primatology},
   month = {11},
   publisher = {Wiley},
   title = {Bioacoustic monitoring to determine addiction levels of primates to the human sphere: A feasibility study on Japanese macaques},
   url = {http://dx.doi.org/10.1002/ajp.23558},
   year = {2023},
}
@article{Clink2024,
   abstract = {Recent advances in deep and transfer learning have revolutionized our ability for the automated detection and classification of acoustic signals from long-term recordings. Here, we provide a benchmark for the automated detection of southern yellow-cheeked crested gibbon (Nomascus gabriellae) calls collected using autonomous recording units (ARUs) in Andoung Kraleung Village, Cambodia. We compared the performance of support vector machines (SVMs), a quasi-DenseNet architecture (Koogu), transfer learning with pretrained convolutional neural network (ResNet50) models trained on the ImageNet dataset, and transfer learning with embeddings from a global birdsong model (BirdNET) based on an EfficientNet architecture. We also investigated the impact of varying the number of training samples on the performance of these models. We found that BirdNET had superior performance with a smaller number of training samples, whereas Koogu and ResNet50 models only had acceptable performance with a larger number of training samples (&gt;200 gibbon samples). Effective automated detection approaches are critical for monitoring endangered species, like gibbons. It is unclear how generalizable these results are for other signals, and future work on other vocal species will be informative. Code and data are publicly available for future benchmarking.Competing Interest StatementThe authors have declared no competing interest.},
   author = {Dena J Clink and Hope Cross-Jaya and Jinsung Kim and Abdul Hamid Ahmad and Moeurk Hong and Roeun Sala and Hélène Birot and Cain Agger and Thinh Tien Vu and Hoa Nguyen Thi and Thanh Nguyen Chi and Holger Klinck},
   doi = {10.1101/2024.08.17.608420},
   journal = {bioRxiv},
   publisher = {Cold Spring Harbor Laboratory},
   title = {Benchmarking automated detection and classification approaches for monitoring of endangered species: a case study on gibbons from Cambodia},
   url = {https://www.biorxiv.org/content/early/2024/08/22/2024.08.17.608420},
   year = {2024},
}
@article{Akamatsu2005,
   abstract = {Detecting objects in their paths is a fundamental perceptional function of moving organisms. Potential risks and rewards, such as prey, predators, conspecifics or non-biological obstacles, must be detected so that an animal can modify its behaviour accordingly. However, to date few studies have considered how animals in the wild focus their attention. Dolphins and porpoises are known to actively use sonar or echolocation. A newly developed miniature data logger attached to a porpoise allows for individual recording of acoustical search efforts and inspection distance based on echolocation. In this study, we analysed the biosonar behaviour of eight free-ranging finless porpoises (Neophocaena phocaenoides) and demonstrated that these animals inspect the area ahead of them before swimming silently into it. The porpoises inspected distances up to 77 m, whereas their swimming distance without using sonar was less than 20 m. The inspection distance was long enough to ensure a wide safety margin before facing real risks or rewards. Once a potential prey item was detected, porpoises adjusted their inspection distance from the remote target throughout their approach.},
   author = {Tomonari Akamatsu and Ding Wang and Kexiong Wang and Yasuhiko Naito},
   doi = {10.1098/rspb.2004.3024},
   issn = {1471-2954},
   journal = {Proceedings of the Royal Society B: Biological Sciences},
   month = {11},
   pages = {797-801},
   publisher = {The Royal Society},
   title = {Biosonar behaviour of free-ranging porpoises},
   volume = {272},
   url = {http://dx.doi.org/10.1098/rspb.2004.3024},
   year = {2005},
}
@article{Vieira2015,
   abstract = {The study of acoustic communication in animals often requires not only the recognition of species specific acoustic signals but also the identification of individual subjects, all in a complex acoustic background. Moreover, when very long recordings are to be analyzed, automatic recognition and identification processes are invaluable tools to extract the relevant biological information. A pattern recognition methodology based on hidden Markov models is presented inspired by successful results obtained in the most widely known and complex acoustical communication signal: human speech. This methodology was applied here for the first time to the detection and recognition of fish acoustic signals, specifically in a stream of round-the-clock recordings of Lusitanian toadfish (Halobatrachus didactylus) in their natural estuarine habitat. The results show that this methodology is able not only to detect the mating sounds (boatwhistles) but also to identify individual male toadfish, reaching an identification rate of ca. 95%. Moreover this method also proved to be a powerful tool to assess signal durations in large data sets. However, the system failed in recognizing other sound types.},
   author = {Manuel Vieira and Paulo J Fonseca and M Clara P Amorim and Carlos J C Teixeira},
   doi = {10.1121/1.4936858},
   issn = {1520-8524},
   journal = {The Journal of the Acoustical Society of America},
   month = {11},
   pages = {3941-3950},
   publisher = {Acoustical Society of America (ASA)},
   title = {Call recognition and individual identification of fish vocalizations based on automatic speech recognition: An example with the Lusitanian toadfish},
   url = {http://dx.doi.org/10.1121/1.4936858},
   year = {2015},
}
@article{Guo2024,
   abstract = {Bird species monitoring is important for the preservation of biological diversity because it provides fundamental information for biodiversity assessment and protection. Automatic acoustic recognition is considered to be an essential technology for realizing automatic monitoring of bird species. Current deep learning-based bird sound recognition methods do not fully conduct long-term correlation modeling along both the time and frequency axes of the spectrogram. Additionally, these methods have not completely studied the impact of different scales of features on the final recognition. To solve the abovementioned problems, we propose a Conformer-based dual path joint modeling network (CDPNet) for bird sound recognition. To the best of our knowledge, this is the first attempt to adopt Conformer in the bird sound recognition task. Specifically, the proposed CDPNet mainly consists of a dual-path time-frequency joint modeling module (DPTFM) and a multi-scale feature fusion module (MSFFM). The former aims to simultaneously capture time-frequency local features, long-term time dependence, and long-term frequency dependence to better model bird sound characteristics effectively. The latter is designed to improve recognition accuracy by fusing different scales of features. The proposed algorithm is implemented on an edge computing platform, NVIDIA Jetson Nano, to build a real-time bird sound recognition monitoring system. The ablation experimental results verify the benefit of using the DPTFM and the MSFFM. Through training and testing on the Semibirdaudio dataset containing 27,155 sound clips and the public Birdsdata dataset, the proposed CDPNet outperforms the other state-of-the-art models in terms of F1-score, precision, recall, and accuracy.},
   author = {Huimin Guo and Haifang Jian and Yiyu Wang and Hongchang Wang and Shuaikang Zheng and Qinghua Cheng and Yuehao Li},
   doi = {10.1007/s10489-024-05362-9},
   issn = {1573-7497},
   journal = {Applied Intelligence},
   month = {11},
   pages = {3152-3168},
   publisher = {Springer Science and Business Media LLC},
   title = {CDPNet: conformer-based dual path joint modeling network for bird sound recognition},
   url = {http://dx.doi.org/10.1007/s10489-024-05362-9},
   year = {2024},
}
@article{Sun2022,
   abstract = {To protect tropical forest biodiversity, we need to be able to detect it reliably, cheaply, and at scale. Automated detection of sound producing animals from passively recorded soundscapes via machine-learning approaches is a promising technique towards this goal, but it is constrained by the necessity of large training data sets. Using soundscapes from a tropical forest in Borneo and a Convolutional Neural Network model (CNN), we investigate i) the minimum viable training data set size for accurate prediction of call types (‘sonotypes’), and ii) the extent to which data augmentation and transfer learning can overcome the issue of small and imbalanced training data sets. We found that even relatively high sample sizes (>80 per sonotype) lead to mediocre accuracy, which however improved significantly with data augmentation and transfer learning, including at extremely small sample sizes (3 per sonotype), regardless of taxonomic group or call characteristics. Neither transfer learning nor data augmentation alone achieved high accuracy. Our results suggest that transfer learning and data augmentation could make the use of CNNs to classify species’ vocalizations feasible even for small soundscape-based projects with many rare species. Retraining our open-source model requires only basic programming skills which makes it possible for individual conservation initiatives to match their local context, in order to enable more evidence-informed management of biodiversity.},
   author = {Yuren Sun and Tatiana Midori Maeda and Claudia Solís-Lemus and Daniel Pimentel-Alarcón and Zuzana Buřivalová},
   doi = {10.1016/j.ecolind.2022.109621},
   issn = {1470-160X},
   journal = {Ecological Indicators},
   month = {11},
   publisher = {Elsevier BV},
   title = {Classification of animal sounds in a hyperdiverse rainforest using convolutional neural networks with data augmentation},
   url = {http://dx.doi.org/10.1016/j.ecolind.2022.109621},
   year = {2022},
}
@inproceedings{Lin2017,
   abstract = {A monitoring network for biodiversity change is essential for wildlife conservation. In recent years, many soundscape monitoring projects have been carried out to investigate the diversity of vocalizing animals. However, the acoustic-based biodiversity assessment remains challenging due to the lack of sufficient recognition database and the inability to disentangle mixed sound sources. Since 2014, an Asian Soundscape monitoring project has been initiated in Taiwan. So far, there are 15 recording sites in Taiwan and three sites in Southeast Asia, with more than 20,000 hours of recordings archived in the Asian Soundscape. In this study, we employed the visualization of long-duration recordings, blind source separation, and clustering techniques, to investigate the spatio-temporal variations of forest biodiversity in the Triangle Mountain, Lienhuachih, and T aipingshan. On the basis of blind source separation, biological sounds, with prominent diurnal occurrence pattern, can be separated from the environmental sounds without any recognition database. Thus, clusters of biological sounds can be effectively identified and employed to measure the daily change in bioacoustic diversity. Our results show that the bioacoustic diversity was higher in the evergreen broad-leaved forest. However, the seasonal variation in bioacoustic diversity was most evident in the high elevation coniferous forest. This study demonstrates that a suitable integration of machine learning and ecoacoustics can facilitate the evaluation of biodiversity changes. In addition to biological activities, we can also measure the environmental variability from soundscape information. In the future, the Asian Soundscape will not only serve as an open database for soundscape recordings, but also will provide tools for analyzing the interactions between biodiversity, environment, and human activities.},
   author = {Tzu-Hao Lin and Yu Tsao and Yu-Huang Wang and Han-Wei Yen and Sheng-Shan Lu},
   doi = {10.23919/PNC.2017.8203533},
   booktitle = {2017 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC)},
   keywords = {Biodiversity;Monitoring;Animals;Transient analysis;Large Hadron Collider;Acoustics;Biodiversity assessment;soundscape;biological sounds;machine learning;open data},
   pages = {128-133},
   title = {Computing biodiversity change via a soundscape monitoring network},
   url = {https://ieeexplore.ieee.org/document/8203533},
   year = {2017},
}
@article{Trapanotto2022,
   abstract = {The classification of vocal individuality for passive acoustic monitoring (PAM) and census of animals is becoming an increasingly popular area of research. Nearly all studies in this field of inquiry have relied on classic audio representations and classifiers, such as Support Vector Machines (SVMs) trained on spectrograms or Mel-Frequency Cepstral Coefficients (MFCCs). In contrast, most current bioacoustic species classification exploits the power of deep learners and more cutting-edge audio representations. A significant reason for avoiding deep learning in vocal identity classification is the tiny sample size in the collections of labeled individual vocalizations. As is well known, deep learners require large datasets to avoid overfitting. One way to handle small datasets with deep learning methods is to use transfer learning. In this work, we evaluate the performance of three pretrained CNNs (VGG16, ResNet50, and AlexNet) on a small, publicly available lion roar dataset containing approximately 150 samples taken from five male lions. Each of these networks is retrained on eight representations of the samples: MFCCs, spectrogram, and Mel spectrogram, along with several new ones, such as VGGish and stockwell, and those based on the recently proposed LM spectrogram. The performance of these networks, both individually and in ensembles, is analyzed and corroborated using the Equal Error Rate and shown to surpass previous classification attempts on this dataset; the best single network achieved over 95% accuracy and the best ensembles over 98% accuracy. The contributions this study makes to the field of individual vocal classification include demonstrating that it is valuable and possible, with caution, to use transfer learning with single pretrained CNNs on the small datasets available for this problem domain. We also make a contribution to bioacoustics generally by offering a comparison of the performance of many state-of-the-art audio representations, including for the first time the LM spectrogram and stockwell representations. All source code for this study is available on GitHub.},
   author = {Martino Trapanotto and Loris Nanni and Sheryl Brahnam and Xiang Guo},
   doi = {10.3390/jimaging8040096},
   issn = {2313-433X},
   journal = {Journal of Imaging},
   title = {Convolutional Neural Networks for the Identification of African Lions from Individual Vocalizations},
   url = {https://www.mdpi.com/2313-433X/8/4/96},
   year = {2022},
}
@article{Schall2024,
   abstract = {Passive acoustic monitoring (PAM) is commonly used to obtain year-round continuous data on marine soundscapes harboring valuable information on species distributions or ecosystem dynamics. This continuously increasing amount of data requires highly efficient automated analysis techniques in order to exploit the full potential of the available data. Here, we propose a benchmark, which consists of a public dataset, a well-defined task and evaluation procedure to develop and test automated analysis techniques. This benchmark focuses on the special case of detecting animal vocalizations in a real-world dataset from the marine realm. We believe that such a benchmark is necessary to monitor the progress in the development of new detection algorithms in the field of marine bioacoustics. We ultimately use the proposed benchmark to test three detection approaches, namely ANIMAL-SPOT, Koogu and a simple custom sequential convolutional neural network (CNN), and report performances. We report the performance of the three detection approaches in a blocked cross-validation fashion with 11 site-year blocks for a multi-species detection scenario in a large marine passive acoustic dataset. Performance was measured with three simple metrics (i.e., true classification rate, noise misclassification rate and call misclassification rate) and one combined fitness metric, which allocates more weight to the minimization of false positives created by noise. Overall, ANIMAL-SPOT performed the best with an average F metric of 0.83, followed by the custom CNN with an average fitness metric of 0.79 and finally Koogu with an average fitness metric of 0.59. The presented benchmark is an important step to advance in the automatic processing of the continuously growing amount of PAM data that are collected throughout the world's oceans. To ultimately achieve usability of developed algorithms, the focus of future work should be laid on the reduction of the false positives created by noise.},
   author = {Elena Schall and Idil Ilgaz Kaya and Elisabeth Debusschere and Paul Devos and Clea Parcerisas},
   doi = {10.1002/rse2.392},
   issn = {2056-3485},
   journal = {Remote Sensing in Ecology and Conservation},
   month = {11},
   pages = {642-654},
   publisher = {Wiley},
   title = {Deep learning in marine bioacoustics: a benchmark for baleen whale detection},
   url = {http://dx.doi.org/10.1002/rse2.392},
   year = {2024},
}
@article{Shiu2020,
   abstract = {Deep neural networks have advanced the field of detection and classification and allowed for effective identification of signals in challenging data sets. Numerous time-critical conservation needs may benefit from these methods. We developed and empirically studied a variety of deep neural networks to detect the vocalizations of endangered North Atlantic right whales (Eubalaena glacialis). We compared the performance of these deep architectures to that of traditional detection algorithms for the primary vocalization produced by this species, the upcall. We show that deep-learning architectures are capable of producing false-positive rates that are orders of magnitude lower than alternative algorithms while substantially increasing the ability to detect calls. We demonstrate that a deep neural network trained with recordings from a single geographic region recorded over a span of days is capable of generalizing well to data from multiple years and across the species’ range, and that the low false positives make the output of the algorithm amenable to quality control for verification. The deep neural networks we developed are relatively easy to implement with existing software, and may provide new insights applicable to the conservation of endangered species.},
   author = {Yu Shiu and K J Palmer and Marie A Roch and Erica Fleishman and Xiaobai Liu and Eva-Marie Nosal and Tyler Helble and Danielle Cholewiak and Douglas Gillespie and Holger Klinck},
   doi = {10.1038/s41598-020-57549-y},
   issn = {2045-2322},
   journal = {Scientific Reports},
   month = {11},
   publisher = {Springer Science and Business Media LLC},
   title = {Deep neural networks for automated detection of marine mammal species},
   url = {http://dx.doi.org/10.1038/s41598-020-57549-y},
   year = {2020},
}
@misc{Roy2022,
   abstract = {Overview of the project Approximately £400 million is invested annually in agri environment schemes in England, designed to compensate farmers for loss of production (income foregone) and additional costs, to meet environmental objectives. A further £1.8 billion in subsidies is paid to comply with environmental conditions of cross compliance and greening. In December 2019, it was estimated there were ca. 50,000 agri-environment agreements covering 1.8 million hectares. Current and previous monitoring of agri-environment schemes is diverse and varied, and conducted at scales of specific management options, whole AES agreements and 1 km squares covering multiple agreements. The Monitoring and Evaluation Programme of these schemes aims to deliver evidence to achieve the following: • Evaluate the delivery of agri-environment schemes and their effectiveness in achieving their intended policy objectives; • Inform current and future agri-environment policy, scheme delivery and development; • Fulfil domestic and European reporting requirements. The existing Monitoring and Evaluation Programme (2015-2020) for Agri-environment schemes (AES) in England includes four elements: integrated monitoring, landscape scale, thematic, and evaluation and synthesis. To date, monitoring has been primarily based on tried-and-tested approaches, which provide widely recognised metrics of quality, condition and species-resolution biodiversity data, and that have counterfactuals often derived from national monitoring schemes to enable evaluation of AES. Developing technologies will need to integrate with existing tried-and-tested approaches but offer huge potential for more extensive and efficient evaluation of AES. In this project we focus on technologies with greatest potential to deliver enhanced and more cost-effective monitoring of environmental ‘outcomes’ of direct land management and interventions, in support of the Environmental Land Management Scheme (ELMS). The technology areas covered by this project are: bioacoustics, DNA-methods, Earth Observation, computer vision and machine learning. For each technology area, the objectives were to: • Carry out horizon scanning of developing technologies which may be relevant for agrienvironment monitoring; • Review how these developing technologies could be used for Agri-environment monitoring; • Review what outputs/outcomes are relevant for the developing technology; • Propose how developing technologies could be used in existing and future agrienvironment monitoring; • Pilot or proof of concept studies to demonstrate how technology could be used for agrienvironment monitoring.},
   author = {D B Roy and C Abrahams and T August and J Christelow and F Gerard and K Howell and M Logie and M McCracken and D Pallett and M Pocock and D S Read and D Sadykova and J T Staley},
   city = {Wallingford, UK},
   month = {11},
   note = {Freely available via Official URL link.},
   publisher = {UK Centre for Ecology & Hydrology},
   title = {Developing technologies for Agri-environment monitoring},
   url = {http://nora.nerc.ac.uk/id/eprint/533486/},
   year = {2022},
}
@article{Sanchez2017,
   abstract = {Natalus primus constitutes one of the most vulnerable mammalian species of Cuba. Until now, only one extant population is known to live in one single cave in the westernmost part of Cuba, within the Guanahacabibes National Park. Over multiple trips, we recorded ultrasonic vocalizations from several individuals of this species. We found short, high frequency-modulated multiharmonic calls for N. primus; these could be used to identify this species in acoustic inventories conducted in Cuba. Identifying N. primus through their echolocation calls will allow conducting passive acoustic monitoring, constituting a noninvasive approach to study this vulnerable species without causing disturbances on its roosts and foraging areas.},
   author = {Lida Sanchez and Christian R Moreno and Emanuel C Mora},
   doi = {10.1080/23312025.2017.1355027},
   editor = {Hynek Burda},
   issn = {2331-2025},
   journal = {Cogent Biology},
   month = {11},
   publisher = {Informa UK Limited},
   title = {Echolocation calls ofNatalus primus(Chiroptera: Natalidae): Implications for conservation monitoring of this species},
   url = {http://dx.doi.org/10.1080/23312025.2017.1355027},
   year = {2017},
}
@inproceedings{Solomes2020,
   abstract = {Monitoring wildlife is an important aspect of conservation initiatives. Deep learning detectors can help with this, although it is not yet clear whether they can run efficiently on an embedded system in the wild. This paper proposes an automatic detection algorithm for the Bela embedded Linux device for wildlife monitoring. The algorithm achieves good quality recognition, efficiently running on continuously streamed data on a commercially available platform. The program is capable of computing on-board detection using convolutional neural networks (CNNs) with an AUC score of 82.5% on the testing set of an international data challenge. This paper details how the model is exported to work on the Bela Mini in C++, with the spectrogram generation and the implementation of the feed-forward network, and evaluates its performance on the Bird Audio Detection challenge 2018 DCASE data.},
   author = {Alexandru-Marius Solomes and Dan Stowell},
   doi = {10.1109/icassp40776.2020.9053533},
   booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   month = {11},
   pages = {746-750},
   publisher = {IEEE},
   title = {Efficient Bird Sound Detection on the Bela Embedded System},
   url = {http://dx.doi.org/10.1109/ICASSP40776.2020.9053533},
   year = {2020},
}
@article{Sheard2024,
   abstract = {Emerging technologies are increasingly employed in environmental citizen science projects. This integration offers benefits and opportunities for scientists and participants alike. Citizen science can support large-scale, long-term monitoring of species occurrences, behaviour and interactions. At the same time, technologies can foster participant engagement, regardless of pre-existing taxonomic expertise or experience, and permit new types of data to be collected. Yet, technologies may also create challenges by potentially increasing financial costs, necessitating technological expertise or demanding training of participants. Technology could also reduce people's direct involvement and engagement with nature. In this perspective, we discuss how current technologies have spurred an increase in citizen science projects and how the implementation of emerging technologies in citizen science may enhance scientific impact and public engagement. We show how technology can act as (i) a facilitator of current citizen science and monitoring efforts, (ii) an enabler of new research opportunities, and (iii) a transformer of science, policy and public participation, but could also become (iv) an inhibitor of participation, equity and scientific rigour. Technology is developing fast and promises to provide many exciting opportunities for citizen science and insect monitoring, but while we seize these opportunities, we must remain vigilant against potential risks.

This article is part of the theme issue ‘Towards a toolkit for global insect biodiversity monitoring’.},
   author = {Julie Koch Sheard and Tim Adriaens and Diana E Bowler and Andrea Büermann and Corey T Callaghan and Elodie C M Camprasse and Shawan Chowdhury and Thore Engel and Elizabeth A Finch and Julia von Gönner and Pen-Yuan Hsing and Peter Mikula and Rui Ying Rachel Oh and Birte Peters and Shyam S Phartyal and Michael J O Pocock and Jana Wäldchen and Aletta Bonn},
   doi = {10.1098/rstb.2023.0106},
   issn = {1471-2970},
   journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
   month = {11},
   publisher = {The Royal Society},
   title = {Emerging technologies in citizen science and potential for insect monitoring},
   url = {http://dx.doi.org/10.1098/rstb.2023.0106},
   year = {2024},
}
@article{Marques2012,
   abstract = {Reliable estimation of the size or density of wild animal populations is very important for effective wildlife management, conservation and ecology. Currently, the most widely used methods for obtaining such estimates involve either sighting animals from transect lines or some form of capture-recapture on marked or uniquely identifiable individuals. However, many species are difficult to sight, and cannot be easily marked or recaptured. Some of these species produce readily identifiable sounds, providing an opportunity to use passive acoustic data to estimate animal density. In addition, even for species for which other visually based methods are feasible, passive acoustic methods offer the potential for greater detection ranges in some environments (e.g. underwater or in dense forest), and hence potentially better precision. Automated data collection means that surveys can take place at times and in places where it would be too expensive or dangerous to send human observers.

Here, we present an overview of animal density estimation using passive acoustic data, a relatively new and fast-developing field. We review the types of data and methodological approaches currently available to researchers and we provide a framework for acoustics-based density estimation, illustrated with examples from real-world case studies. We mention moving sensor platforms (e.g. towed acoustics), but then focus on methods involving sensors at fixed locations, particularly hydrophones to survey marine mammals, as acoustic-based density estimation research to date has been concentrated in this area. Primary among these are methods based on distance sampling and spatially explicit capture-recapture. The methods are also applicable to other aquatic and terrestrial sound-producing taxa.

We conclude that, despite being in its infancy, density estimation based on passive acoustic data likely will become an important method for surveying a number of diverse taxa, such as sea mammals, fish, birds, amphibians, and insects, especially in situations where inferences are required over long periods of time. There is considerable work ahead, with several potentially fruitful research areas, including the development of (i) hardware and software for data acquisition, (ii) efficient, calibrated, automated detection and classification systems, and (iii) statistical approaches optimized for this application. Further, survey design will need to be developed, and research is needed on the acoustic behaviour of target species. Fundamental research on vocalization rates and group sizes, and the relation between these and other factors such as season or behaviour state, is critical. Evaluation of the methods under known density scenarios will be important for empirically validating the approaches presented here.},
   author = {Tiago A Marques and Len Thomas and Stephen W Martin and David K Mellinger and Jessica A Ward and David J Moretti and Danielle Harris and Peter L Tyack},
   doi = {10.1111/brv.12001},
   issn = {1469-185X},
   issue = {2},
   journal = {Biological Reviews},
   month = {11},
   pages = {287-309},
   publisher = {Wiley},
   title = {Estimating animal population density using passive acoustics},
   url = {http://dx.doi.org/10.1111/brv.12001},
   year = {2012},
}
@article{Jordan2024,
   abstract = {In highly threatened habitats such as the dry deciduous forests of western Madagascar, it is essential to develop new approaches to detect population changes and evaluate conservation measures. Passive acoustic monitoring (PAM) is such a promising approach. This method has many advantages over conventional methods, such as time efficiency, money savings, and reduced wildlife disturbance. It is especially suitable for studying occupancy and activity patterns of vocalizing species such as birds. Our study analyzed data recorded with autonomous sound recorders in 2018 in Kirindy Forest for the territorial calls of Coua gigas and Coua coquereli. We modeled occupancy and detection probability for both species in the study area. We also examined activity patterns and found that the peak of vocal activity for Coua coquereli is at 700h and for Coua gigas at 1100h. To also test the value of PAM in relation to ecological factors we modeled occupancy and included logging status as a site covariate. We detected a positive influence of logging in occupancy of Coua gigas. Our study provides guidelines for future occupancy studies using PAM in the two coua species. We conclude that PAM will improve the ecological monitoring of soniferous animals in Madagascar.},
   author = {Celine Jordan and Matthias Markolf},
   doi = {10.4314/mcd.v18i1.6},
   issn = {1662-2510},
   journal = {Madagascar Conservation &amp; Development},
   month = {11},
   pages = {39-47},
   publisher = {African Journals Online (AJOL)},
   title = {Exploring the potential of occupancy modelling using passive acoustics in &lt;i&gt;Coua gigas&lt;/i&gt; and &lt;i&gt;Coua coquereli&lt;/i&gt;},
   url = {http://dx.doi.org/10.4314/mcd.v18i1.6},
   year = {2024},
}
@article{Marck2022,
   abstract = {Animal vocal communication is a broad and multi-disciplinary field of research. Studying various aspects of communication can provide key elements for understanding animal behavior, evolution, and cognition. Given the large amount of acoustic data accumulated from automated recorders, for which manual annotation and analysis is impractical, there is a growing need to develop algorithms and automatic methods for analyzing and identifying animal sounds. In this study we developed an automatic detection and analysis system based on audio signal processing algorithms and deep learning that is capable of processing and analyzing large volumes of data without human bias. We selected the White Spectacled Bulbul (Pycnonotus xanthopygos) as our bird model because it has a complex vocal communication system with a large repertoire which is used by both sexes, year-round. It is a common, widespread passerine in Israel, which is relatively easy to locate and record in a broad range of habitats. Like many passerines, the Bulbul’s vocal communication consists of two primary hierarchies of utterances, syllables and words. To extract each of these units’ characteristics, the fundamental frequency contour was modeled using a low degree Legendre polynomial, enabling it to capture the different patterns of variation from different vocalizations, so that each pattern could be effectively expressed using very few coefficients. In addition, a mel-spectrogram was computed for each unit, and several features were extracted both in the time-domain (e.g., zero-crossing rate and energy) and frequency-domain (e.g., spectral centroid and spectral flatness). We applied both linear and non-linear dimensionality reduction algorithms on feature vectors and validated the findings that were obtained manually, namely by listening and examining the spectrograms visually. Using these algorithms, we show that the Bulbul has a complex vocabulary of more than 30 words, that there are multiple syllables that are combined in different words, and that a particular syllable can appear in several words. Using our system, researchers will be able to analyze hundreds of hours of audio recordings, to obtain objective evaluation of repertoires, and to identify different vocal units and distinguish between them, thus gaining a broad perspective on bird vocal communication.},
   author = {Aya Marck and Yoni Vortman and Oren Kolodny and Yizhar Lavner},
   doi = {10.3389/fnbeh.2021.812939},
   issn = {1662-5153},
   journal = {Frontiers in Behavioral Neuroscience},
   month = {11},
   publisher = {Frontiers Media SA},
   title = {Identification, Analysis and Characterization of Base Units of Bird Vocal Communication: The White Spectacled Bulbul (Pycnonotus xanthopygos) as a Case Study},
   url = {http://dx.doi.org/10.3389/fnbeh.2021.812939},
   year = {2022},
}
@article{Madhusudhana2021,
   abstract = {Many animals rely on long-form communication, in the form of songs, for vital functions such as mate attraction and territorial defence. We explored the prospect of improving automatic recognition performance by using the temporal context inherent in song. The ability to accurately detect sequences of calls has implications for conservation and biological studies. We show that the performance of a convolutional neural network (CNN), designed to detect song notes (calls) in short-duration audio segments, can be improved by combining it with a recurrent network designed to process sequences of learned representations from the CNN on a longer time scale. The combined system of independently trained CNN and long short-term memory (LSTM) network models exploits the temporal patterns between song notes. We demonstrate the technique using recordings of fin whale (Balaenoptera physalus) songs, which comprise patterned sequences of characteristic notes. We evaluated several variants of the CNN + LSTM network. Relative to the baseline CNN model, the CNN + LSTM models reduced performance variance, offering a 9–17% increase in area under the precision–recall curve and a 9–18% increase in peak F1-scores. These results show that the inclusion of temporal information may offer a valuable pathway for improving the automatic recognition and transcription of wildlife recordings.},
   author = {Shyam Madhusudhana and Yu Shiu and Holger Klinck and Erica Fleishman and Xiaobai Liu and Eva-Marie Nosal and Tyler Helble and Danielle Cholewiak and Douglas Gillespie and Ana Širović and Marie A Roch},
   doi = {10.1098/rsif.2021.0297},
   issn = {1742-5662},
   journal = {Journal of The Royal Society Interface},
   month = {11},
   pages = {20210297},
   publisher = {The Royal Society},
   title = {Improve automatic detection of animal call sequences with temporal context},
   url = {http://dx.doi.org/10.1098/rsif.2021.0297},
   year = {2021},
}
@article{Jeantet2023,
   abstract = {Bioacoustics, the exploration of animal vocalizations and natural soundscapes, has emerged as a valuable tool for studying species within their habitats, particularly those that are challenging to observe. This approach has broadened the horizons of biodiversity assessment and ecological research. However, monitoring wildlife with acoustic recorders produces large volumes of data that can be labor-intensive to analyze. Deep learning has recently transformed many computational disciplines by enabling the automated processing of large and complex datasets and has gained attention within the bioacoustics community. Despite the revolutionary impact of deep learning on acoustic detection and classification, attaining both high detection accuracy and low false positive rates in bioacoustics remains a significant challenge. An intriguing yet unexplored avenue for enhancing deep learning in bioacoustics involves the utilization of contextual information, such as time and location, to discern animal vocalizations within acoustic recordings. As a first case study, a multi-branch Convolutional Neural Network (CNN) was developed to classify 22 different bird songs using spectrograms as a first input, and spatial metadata as a secondary input. A comparison was made to a baseline model with only spectrogram input. A geographical prior neural network was trained, separately, to estimate the probability of a species occurring at a given location. The output of this network was combined with the baseline CNN. As a second case study, temporal data and spectrograms were used as input to a multi-branch CNN for the detection of Hainan gibbon (Nomascus hainanus) calls, the world’s rarest primate. Our findings demonstrate that adding metadata to the bird song classifier significantly improves classification performance, with the highest improvement achieved using the geographical prior model (F1-score of 87.78% compared to 61.02% for the baseline model). The multi-branch CNNs also proved efficient (F1-scores of 76.87% and 78.77%) and simpler to use than the geographical prior. In the second case study, our findings revealed a decrease in false positives by 63% (94% of the calls were detected) when the metadata was used by the multi-branch CNN, and an increase of 19% in gibbon detection. This study has uncovered an exciting new avenue for improving classifier performance in bioacoustics. The methodology described in this study can assist ecologists, wildlife management teams, and researchers in reducing the amount of time spent analyzing large acoustic datasets obtained from passive acoustic monitoring studies. Our approach can be adapted and applied to other calling species, and thus tailored to other use cases.},
   author = {Lorène Jeantet and Emmanuel Dufourq},
   doi = {https://doi.org/10.1016/j.ecoinf.2023.102256},
   issn = {1574-9541},
   journal = {Ecological Informatics},
   keywords = {Bioacoustics,Birds,Convolutional neural networks,Deep learning,Hainan gibbons,Passive acoustic monitoring,Species identification},
   title = {Improving deep learning acoustic classifiers with contextual information for wildlife monitoring},
   url = {https://www.sciencedirect.com/science/article/pii/S1574954123002856},
   year = {2023},
}
@article{Arnaud2022,
   abstract = {Despite the accumulation of data and studies, deciphering animal vocal communication remains highly challenging. While progress has been made with some species for which we now understand the information exchanged through vocal signals, researchers are still left struggling with sparse recordings composing Small, Unbalanced, Noisy, but Genuine (SUNG) datasets. SUNG datasets offer a valuable but distorted vision of communication systems. Adopting the best practices in their analysis is therefore essential to effectively extract the available information and draw reliable conclusions. Here we show that the most recent advances in machine learning applied to a SUNG dataset succeed in unraveling the complex vocal repertoire of the bonobo, and we propose a workflow that can be effective with other animal species. We implement acoustic parameterization in three feature spaces along with three classification algorithms (Support Vector Machine, xgboost, neural networks) and their combination to explore the structure and variability of bonobo calls, as well as the robustness of the individual signature they encode. We underscore how classification performance is affected by the feature set and identify the most informative features. We highlight the need to address data leakage in the evaluation of classification performance to avoid misleading interpretations. Finally, using a Uniform Manifold Approximation and Projection (UMAP), we show that classifiers generate parsimonious data descriptions which help to understand the clustering of the bonobo acoustic space. Our results lead to identifying several practical approaches that are generalizable to any other animal communication system. To improve the reliability and replicability of vocal communication studies with SUNG datasets, we thus recommend: i) comparing several acoustic parameterizations; ii) adopting Support Vector Machines as the baseline classification approach; iii) explicitly evaluating data leakage and possibly implementing a mitigation strategy; iv) visualizing the dataset with UMAPs applied to classifier predictions rather than to raw acoustic features.Competing Interest StatementThe authors have declared no competing interest.},
   author = {Vincent Arnaud and François Pellegrino and Sumir Keenan and Xavier St-Gelais and Nicolas Mathevon and Florence Levréro and Christophe Coupé},
   doi = {10.1101/2022.06.26.497684},
   journal = {bioRxiv},
   publisher = {Cold Spring Harbor Laboratory},
   title = {Improving the workflow to crack Small, Unbalanced, Noisy, but Genuine (SUNG) datasets in bioacoustics: the case of bonobo calls},
   url = {https://www.biorxiv.org/content/early/2022/06/29/2022.06.26.497684},
   year = {2022},
}
@article{Garcia2024,
   abstract = {This study explores the integration of directional microphones with convolutional neural networks (CNNs) for long-range bird species identification. By employing directional microphones, we aimed to capture high-resolution audio from specific directions, potentially improving the clarity of bird calls over extended distances. Our approach involved processing these recordings with CNNs trained on a diverse dataset of bird calls. The results demonstrated that the system is capable of systematically identifying bird species up to 150 m, reaching 280 m for species vocalizing at frequencies greater than 1000 Hz and clearly distinct from background noise. The furthest successful detection was obtained at 510 m. While the method showed promise in enhancing the identification process compared to traditional techniques, there were notable limitations in the clarity of the audio recordings. These findings suggest that while the integration of directional microphones and CNNs for long-range bird species identification is promising, further refinement is needed to fully realize the benefits of this approach. Future efforts should focus on improving the audio-capture technology to reduce ambient noise and enhance the system’s overall performance in long-range bird species identification.},
   author = {Tiago Garcia and Luís Pina and Magnus Robb and Jorge Maria and Roel May and Ricardo Oliveira},
   doi = {10.3390/make6040115},
   issn = {2504-4990},
   journal = {Machine Learning and Knowledge Extraction},
   pages = {2336-2354},
   title = {Long-Range Bird Species Identification Using Directional Microphones and CNNs},
   url = {https://www.mdpi.com/2504-4990/6/4/115},
   year = {2024},
}
@article{Caruso2017,
   abstract = {Dolphins emit short ultrasonic pulses (clicks) to acquire information about the surrounding environment, prey and habitat features. We investigated Delphinidae activity over multiple temporal scales through the detection of their echolocation clicks, using long-term Passive Acoustic Monitoring (PAM). The Istituto Nazionale di Fisica Nucleare operates multidisciplinary seafloor observatories in a deep area of the Central Mediterranean Sea. The Ocean noise Detection Experiment collected data offshore the Gulf of Catania from January 2005 to November 2006, allowing the study of temporal patterns of dolphin activity in this deep pelagic zone for the first time. Nearly 5,500 five-minute recordings acquired over two years were examined using spectrogram analysis and through development and testing of an automatic detection algorithm. Echolocation activity of dolphins was mostly confined to nighttime and crepuscular hours, in contrast with communicative signals (whistles). Seasonal variation, with a peak number of clicks in August, was also evident, but no effect of lunar cycle was observed. Temporal trends in echolocation corresponded to environmental and trophic variability known in the deep pelagic waters of the Ionian Sea. Long-term PAM and the continued development of automatic analysis techniques are essential to advancing the study of pelagic marine mammal distribution and behaviour patterns.},
   author = {Francesco Caruso and Giuseppe Alonge and Giorgio Bellia and Emilio De Domenico and Rosario Grammauta and Giuseppina Larosa and Salvatore Mazzola and Giorgio Riccobene and Gianni Pavan and Elena Papale and Carmelo Pellegrino and Sara Pulvirenti and Virginia Sciacca and Francesco Simeone and Fabrizio Speziale and Salvatore Viola and Giuseppa Buscaino},
   doi = {10.1038/s41598-017-04608-6},
   issn = {2045-2322},
   journal = {Scientific Reports},
   month = {11},
   publisher = {Springer Science and Business Media LLC},
   title = {Long-Term Monitoring of Dolphin Biosonar Activity in Deep Pelagic Waters of the Mediterranean Sea},
   url = {http://dx.doi.org/10.1038/s41598-017-04608-6},
   year = {2017},
}
@article{Munger2022,
   abstract = {Sound production rates of fishes can be used as an indicator for coral reef health, providing an opportunity to utilize long-term acoustic recordings to assess environmental change. As acoustic datasets become more common, computational techniques need to be developed to facilitate analysis of the massive data files produced by long-term monitoring. Machine learning techniques demonstrate an advantage in the identification of fish sounds over manual sampling approaches. Here we evaluated the ability of convolutional neural networks to identify and monitor call patterns for pomacentrids (damselfishes) in a tropical reef region of the western Pacific. A stationary hydrophone was deployed for 39 mo (2014-2018) in the National Park of American Samoa to continuously record the local marine acoustic environment. A neural network was trained—achieving 94% identification accuracy of pomacentrids—to demonstrate the applicability of machine learning in fish acoustics and ecology. The distribution of sound production was found to vary on diel and interannual timescales. Additionally, the distribution of sound production was correlated with wind speed, water temperature, tidal amplitude, and sound pressure level. This research has broad implications for state-of-the-art acoustic analysis and promises to be an efficient, scalable asset for ecological research, environmental monitoring, and conservation planning.},
   author = {J E Munger and D P Herrera and S M Haver and L Waterhouse and M F McKenna and R P Dziak and J Gedamke and S A Heppell and J H Haxel},
   doi = {10.3354/meps13912},
   issn = {1616-1599},
   journal = {Marine Ecology Progress Series},
   month = {11},
   pages = {197-210},
   publisher = {Inter-Research Science Center},
   title = {Machine learning analysis reveals relationship between pomacentrid calls and environmental cues},
   url = {http://dx.doi.org/10.3354/meps13912},
   year = {2022},
}
@article{Risch2013,
   abstract = {Passive acoustic monitoring (PAM) is a rapidly growing field, providing valuable insights in marine ecology. The approach allows for long-term, species-specific monitoring over a range of spatial scales. For many baleen whales fundamental information on seasonal occurrence and distribution is still missing. In this study, pulse trains produced by the North Atlantic minke whale, a highly mobile and cryptic species, are used to examine its seasonality, diel vocalization patterns and spatial distribution throughout the Stellwagen Bank National Marine Sanctuary (SBNMS), USA. Three and a half years (2006, 2007 to 2010) of near continuous passive acoustic data were analyzed using automated detection methods. Random forests and cluster analyses grouped pulse trains into 3 main categories (slow-down, constant and speed-up), with several sub-types. Slow-down pulse trains were the most commonly recorded call category. Minke whale pulse train occurrence was highly seasonal across all years. Detections were made from August to November, with 88% occurring in September and October. No detections were recorded in January and February, and only few from March to June. Minke whale pulse trains showed a distinct diel pattern, with a nighttime peak from approximately 20:00 to 01:00 h Eastern Standard Time (EST). The highest numbers of pulse trains were detected to the east of Stellwagen Bank, suggesting that minke whales travel preferably in deeper waters along the outer edge of the sanctuary. These data show that minke whales consistently use Stellwagen Bank as part of their migration route to and from the feeding grounds. Unlike other baleen whales in this area they do not appear to have a persistent year-round acoustic presence.},
   author = {D Risch and C W Clark and P J Dugan and M Popescu and U Siebert and S M Van Parijs},
   doi = {10.3354/meps10426},
   issn = {1616-1599},
   journal = {Marine Ecology Progress Series},
   month = {11},
   pages = {279-295},
   publisher = {Inter-Research Science Center},
   title = {Minke whale acoustic behavior and multi-year seasonal and diel vocalization patterns in Massachusetts Bay, USA},
   url = {http://dx.doi.org/10.3354/meps10426},
   year = {2013},
}
@article{Zhou2023,
   abstract = {The extraction of species-specific calls from passive acoustic recordings is a common preliminary step in ecological analysis. But for many species, especially those occupying noisy, acoustically variable habitats, the call extraction process remains largely manual, a time-consuming, and increasingly unsustainable process. Deep neural networks have been shown to provide excellent performance in a range of acoustic classification applications. We take as an example the recognition of four songs of one of the rarest mammals in the world, the western black-crested gibbon (Nomascus concolors).The process of recognition in this paper which includes distributed BIC ambient sound segmentation based on the OpenPAI platform; DNN-based western black-crested gibbon song enhancement processing; data pre-processing, labeling samples; proposed DNN + ResNet34 + CBAM + GRU + Attention recognition model; comparing other classical neural network models.Our best model converts acoustic recordings into spectrogram images on the mel frequency scale and uses these images to train convolutional neural networks.Our proposed model proved to be very accurate in predicting the segmented western black-crested gibbon songs with an accuracy of 99.8%, and almost a few western black-crested gibbon songs were incorrectly identified when all segmented data were recognized. In the four consecutive years of the acoustic monitoring system deployed in the Ailao Mountain National Nature Reserve, the western black-crested gibbon was most active near the monitoring site from March to August each year, and least active in January and February. Based on call sound intensity analysis, we monitored a total of two different western black-crested gibbon groups (G1 and G2) during the monitoring cycle. We demonstrate that passive acoustic monitoring combined with CNN classifiers is an effective tool for the remote detection of one of the rarest and most threatened species in the world.},
   author = {Xiaotao Zhou and Kunrong Hu and Zhenhua Guan and Chunjiang Yu and Shuai Wang and Meng Fan and Yongke Sun and Yong Cao and Yijie Wang and Guangting Miao},
   doi = {https://doi.org/10.1016/j.ecolind.2023.110908},
   issn = {1470-160X},
   journal = {Ecological Indicators},
   keywords = {Attentional mechanisms,Bioacoustics,Deep learning,Passive acoustic monitoring,Western black-crested gibbons song recognition},
   title = {Methods for processing and analyzing passive acoustic monitoring data: An example of song recognition in western black-crested gibbons},
   url = {https://www.sciencedirect.com/science/article/pii/S1470160X23010506},
   year = {2023},
}
@article{Caruso2020,
   abstract = {Passive acoustic monitoring (PAM) is increasingly being adopted as a non-invasive method for the assessment of ocean ecological dynamics. PAM is an important sampling approach for acquiring critical information about marine mammals, especially in areas where data are lacking and where evaluations of threats for vulnerable populations are required. The Indo-Pacific humpback dolphin (IPHD, Sousa chinensis) is a coastal species which inhabits tropical and warm-temperate waters from the eastern Indian Ocean throughout Southeast Asia to central China. A new population of this species was recently discovered in waters southwest of Hainan Island, China. An array of passive acoustic platforms was deployed at depths of 10–20 m (the preferred habitat of humpback dolphins), across sites covering more than 100 km of coastline. In this study, we explored whether the acoustic data recorded by the array could be used to classify IPHD echolocation clicks, with the aim of investigating the spatiotemporal patterns of distribution and acoustic behavior of this species. A number of supervised machine learning algorithms were trained to automatically classify echolocation clicks from the different types of short-broadband pulses recorded. The best performance was reported by a cubic support vector machine (Cubic SVM), which was applied to 19,215 5-min recordings (∼4.2 TB), collected over a period of 75 days at six locations. Subsequently, using spectrogram visualization and audio listening, human operators confirmed the presence of clicks within the selected files. Additionally, other dolphin vocalizations (including whistles, buzzes, and burst pulses) and different sound sources (soniferous fishes, snapping shrimps, human activities) were also reported. The detection range of IPHD clicks was estimated using a transmission loss (TL) model and the performance of the trained classifier was compared with data synchronously collected by an acoustic data logger (A-tag). This study demonstrates that the distribution and habitat use of a coastal and resident dolphin species can be monitored over a large spatiotemporal scale, using an array of passive acoustic platforms and a data analysis protocol that includes both machine learning techniques and spectrogram inspection.},
   author = {Francesco Caruso and Lijun Dong and Mingli Lin and Mingming Liu and Zining Gong and Wanxue Xu and Giuseppe Alonge and Songhai Li},
   doi = {10.3389/fmars.2020.00267},
   issn = {2296-7745},
   journal = {Frontiers in Marine Science},
   month = {11},
   publisher = {Frontiers Media SA},
   title = {Monitoring of a Nearshore Small Dolphin Species Using Passive Acoustic Platforms and Supervised Machine Learning Techniques},
   url = {http://dx.doi.org/10.3389/fmars.2020.00267},
   year = {2020},
}
@article{,
   abstract = {Human–animal conflict (HAC) is one of the main issues that the government of India is now addressing. In this work, we proposed a stacked long short-term memory (LSTM) as well as hybrid features for automatic wild animal detection and state of mind classification based on intelligent perception of the environment. The elephant was the wildlife animal under consideration in this work. This study initially collects the information of wild animals from their environment. We then extracted and combined the mel frequency cepstral coefficient (MFCC), delta MFCC, double delta MFCC, and Linear Predictive Coding (LPC) features in various combinations. This combination of MFCC and its derivatives with LPC provides improved performance. After that, the elephants are identified, and their state of mind (SOM) is classified by utilising the proposed stacked LSTM framework. The results obtained demonstrated that the stacked LSTM framework performed better than both the single LSTM and the bidirectional LSTM learning network. For elephant detection, the classification accuracy obtained was 98%, and for state-of-mind detection, the classification accuracy obtained was 97%. Further, if the presence of elephants is confirmed, it is repelled with the help of an animated predator to scare the animal.},
   author = {R Varun Prakash and V Karthikeyan and S Vishali and M Karthika},
   doi = {10.1007/s00371-024-03588-9},
   issn = {1432-2315},
   journal = {The Visual Computer},
   month = {11},
   publisher = {Springer Science and Business Media LLC},
   title = {Multi-level LSTM framework with hybrid sonic features for human-animal conflict evasion},
   url = {http://dx.doi.org/10.1007/s00371-024-03588-9},
   year = {2024},
}
@article{Symes2024,
   abstract = {Night-time light can have profound ecological effects, even when the source is natural moonlight. The impacts of light can, however, vary substantially by taxon, habitat and geographical region. We used a custom machine learning model built with the Python package Koogu to investigate the in situ effects of moonlight on the calling activity of neotropical forest katydids over multiple years. We prioritised species with calls that were commonly detected in human annotated data, enabling us to evaluate model performance. We focused on eight species of katydids that the model identified with high precision (generally greater than 0.90) and moderate-to-high recall (minimum 0.35), ensuring that detections were generally correct and that many calls were detected. These results suggest that moonlight has modest effects on the amount of calling, with the magnitude and direction of effect varying by species: half of the species showed positive effects of light and half showed negative. These findings emphasize the importance of understanding natural history for anticipating how biological communities respond to moonlight. The methods applied in this project highlight the emerging opportunities for evaluating large quantities of data with machine learning models to address ecological questions over space and time.

This article is part of the theme issue ‘Towards a toolkit for global insect biodiversity monitoring’.},
   author = {Laurel B Symes and Shyam Madhusudhana and Sharon J Martinson and Inga Geipel and Hannah M ter Hofstede},
   doi = {10.1098/rstb.2023.0110},
   issn = {1471-2970},
   journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
   month = {11},
   publisher = {The Royal Society},
   title = {Multi-year soundscape recordings and automated call detection reveals varied impact of moonlight on calling activity of neotropical forest katydids},
   url = {http://dx.doi.org/10.1098/rstb.2023.0110},
   year = {2024},
}
@article{Potamitis2009,
   abstract = {The present work reports research efforts toward development and evaluation of a unified framework for automatic bioacoustic recognition of specific insect pests. Our approach is based on capturing and automatically recognizing the acoustic emission resulting from typical behaviors, e.g., locomotion and feeding, of the target pests. After acquisition the signals are amplified, filtered, parameterized, and classified by advanced machine learning methods on a portable computer. Specifically, we investigate an advanced signal parameterization scheme that relies on variable size signal segmentation. The feature vector computed for each segment of the signal is composed of the dominant harmonic, which carry information about the periodicity of the signal, and the cepstral coefficients, which carry information about the relative distribution of energy among the different spectral sub-bands. This parameterization offers a reliable representation of both the acoustic emissions of the pests of interest and the interferences from the environment. We illustrate the practical significance of our methodology on two specific cases: 1) a devastating pest for palm plantations, namely, Rhynchophorus ferrugineus Olivier and 2) a pest that attacks warehouse stored rice (Oryza sativa L.), the rice weevil, Sitophilus oryzae (L.) (both Coleoptera: Curculionidae, Dryophorinae). These pests are known in many countries around the world and contribute for significant economical loss. The proposed approach led to detection results in real field trials, reaching 99.1% on real-field recordings of R. ferrugineus and 100% for S. oryzae.},
   author = {Ilyas Potamitis and Todor Ganchev and Dimitris Kontodimas},
   doi = {10.1603/029.102.0436},
   issn = {0022-0493},
   journal = {Journal of Economic Entomology},
   month = {11},
   pages = {1681-1690},
   publisher = {Oxford University Press (OUP)},
   title = {On Automatic Bioacoustic Detection of Pests: The Cases of &lt;I&gt;Rhynchophorus ferrugineus&lt;/I&gt; and &lt;I&gt;Sitophilus oryzae&lt;/I&gt;},
   url = {http://dx.doi.org/10.1603/029.102.0436},
   year = {2009},
}
@article{,
   abstract = {The affordability, storage and power capacity of compact modern recording hardware have evolved passive acoustic monitoring (PAM) of animals and soundscapes into a non-invasive, cost-effective tool for research and ecological management particularly useful for bats and toothed whales that orient and forage using ultrasonic echolocation. The use of PAM at large scales hinges on effective automated detectors and species classifiers which, combined with distance sampling approaches, have enabled species abundance estimation of toothed whales. But standardized, user-friendly and open access automated detection and classification workflows are in demand for this key conservation metric to be realized for bats.
We used the PAMGuard toolbox including its new deep learning classification module to test the performance of four open-source workflows for automated analyses of acoustic datasets from bats. Each workflow used a different initial detection algorithm followed by the same deep learning classification algorithm and was evaluated against the performance of an expert manual analyst.
Workflow performance depended strongly on the signal-to-noise ratio and detection algorithm used: the full deep learning workflow had the best classification accuracy (≤67%) but was computationally too slow for practical large-scale bat PAM. Workflows using PAMGuard's detection module or triggers onboard an SM4BAT or AudioMoth accurately classified up to 47%, 59% and 34%, respectively, of calls to species. Not all workflows included noise sampling critical to estimating changes in detection probability over time, a vital parameter for abundance estimation. The workflow using PAMGuard's detection module was 40 times faster than the full deep learning workflow and missed as few calls (recall for both ~0.6), thus balancing computational speed and performance.
We show that complete acoustic detection and classification workflows for bat PAM data can be efficiently automated using open-source software such as PAMGuard and exemplify how detection choices, whether pre- or post-deployment, hardware or software-driven, affect the performance of deep learning classification and the downstream ecological information that can be extracted from acoustic recordings. In particular, understanding and quantifying detection/classification accuracy and the probability of detection are key to avoid introducing biases that may ultimately affect the quality of data for ecological management.},
   author = {Signe M M Brinkløv and Jamie Macaulay and Christian Bergler and Jakob Tougaard and Kristian Beedholm and Morten Elmeros and Peter Teglberg Madsen},
   doi = {10.1111/2041-210x.14131},
   issn = {2041-210X},
   journal = {Methods in Ecology and Evolution},
   month = {11},
   pages = {1747-1763},
   publisher = {Wiley},
   title = {Open-source workflow approaches to passive acoustic monitoring of bats},
   url = {http://dx.doi.org/10.1111/2041-210X.14131},
   year = {2023},
}
@article{Rankin2024,
   abstract = {Passive acoustic monitoring is increasingly used for assessing populations of marine mammals; however, analysis of large datasets is limited by our ability to easily classify sounds detected. Classification of beaked whale acoustic events, in particular, requires evaluation of multiple lines of evidence by expert analysts. Here we present a highly automated approach to acoustic detection and classification using supervised machine learning and open source software methods. Data from four large scale surveys of beaked whales (northwestern North Atlantic, southwestern North Atlantic, Hawaii, and eastern North Pacific) were analyzed using PAMGuard (acoustic detection), PAMpal (acoustic analysis) and BANTER (hierarchical random forest classifier). Overall classification accuracy ranged from 88% for the southwestern North Atlantic data to 97% for the northwestern North Atlantic. Results for many species could likely be improved with increased sample sizes, consideration of alternative automated detectors, and addition of relevant environmental features. These methods provide a highly automated approach to acoustic detection and classification using open source methods that can be readily adopted for other species and geographic regions.},
   author = {Shannon Rankin and Taiki Sakai and Frederick I Archer and Jay Barlow and Danielle Cholewiak and Annamaria I DeAngelis and Jennifer L K McCullough and Erin M Oleson and Anne E Simonis and Melissa S Soldevilla and Jennifer S Trickey},
   doi = {https://doi.org/10.1016/j.ecoinf.2024.102511},
   issn = {1574-9541},
   journal = {Ecological Informatics},
   keywords = {Beaked whale,Bioacoustics,Machine learning,Passive acoustic monitoring,Random forest,Species classification},
   title = {Open-source machine learning BANTER acoustic classification of beaked whale echolocation pulses},
   url = {https://www.sciencedirect.com/science/article/pii/S1574954124000530},
   year = {2024},
}
@article{Metcalf2021,
   abstract = {Estimation of avian biodiversity is a cornerstone measure of ecosystem condition. Surveys conducted using autonomous recorders are often more efficient at estimating diversity than traditional point-count surveys. However, there is limited research into the optimal temporal resolution for sampling—the trade-off between the number of samples and sample duration when sampling a survey window with a fixed survey effort—despite autonomous recorders allowing easy repeat sampling compared to traditional survey methods. We assess whether the additional temporal coverage from high temporal resolution (HTR) sampling, consisting of 240 15-s samples spread randomly across a survey window detects higher alpha and gamma diversity than low temporal resolution (LTR) sampling of four 15-min samples at the same locations. We do so using an acoustic dataset collected from 29 locations in a region of very high avian biodiversity—the eastern Brazilian Amazon. We find HTR sampling outperforms LTR sampling in every metric considered, with HTR sampling predicted to detect approximately 50% higher alpha diversity, and 10% higher gamma diversity. This effect is primarily driven by increased coverage of variation in detectability across the morning, with the earliest period containing a distinct community that is often under sampled using LTR sampling. LTR sampling produced almost four times as many false absences for species presence. Additionally, LTR sampling incorrectly found 70 species (34%) at only a single forest type when they were in fact present in multiple forest types, while the use of HTR sampling reduced this to just two species (0.9%). When considering multiple independent detections of species, HTR sampling detected three times more uncommon species than LTR sampling. We conclude that high temporal resolution sampling of passive-acoustic monitoring-based surveys should be considered the primary method for estimating the species richness of bird communities in tropical forests.},
   author = {Oliver C Metcalf and Jos Barlow and Stuart Marsden and Nárgila de Moura and Erika Berenguer and Joice Ferreira and Alexander C Lees},
   doi = {10.1002/rse2.227},
   editor = {Nathalie Pettorelli and Christos Astaras},
   issn = {2056-3485},
   journal = {Remote Sensing in Ecology and Conservation},
   month = {11},
   pages = {45-56},
   publisher = {Wiley},
   title = {Optimizing tropical forest bird surveys using passive acoustic monitoring and high temporal resolution sampling},
   url = {http://dx.doi.org/10.1002/rse2.227},
   year = {2021},
}
@article{,
   abstract = {A wide range of anthropogenic structures exist in the marine environment with the extent of these set to increase as the global offshore renewable energy industry grows. Many of these pose acute risks to marine wildlife; for example, tidal energy generators have the potential to injure or kill seals and small cetaceans through collisions with moving turbine parts. Information on fine scale behaviour of animals close to operational turbines is required to understand the likely impact of these new technologies. There are inherent challenges associated with measuring the underwater movements of marine animals which have, so far, limited data collection. Here, we describe the development and application of a system for monitoring the three-dimensional movements of cetaceans in the immediate vicinity of a subsea structure. The system comprises twelve hydrophones and software for the detection and localisation of vocal marine mammals. We present data demonstrating the systems practical performance during a deployment on an operational tidal turbine between October 2017 and October 2019. Three-dimensional locations of cetaceans were derived from the passive acoustic data using time of arrival differences on each hydrophone. Localisation accuracy was assessed with an artificial sound source at known locations and a refined method of error estimation is presented. Calibration trials show that the system can accurately localise sounds to 2m accuracy within 20m of the turbine but that localisations become highly inaccurate at distances greater than 35m. The system is currently being used to provide data on rates of encounters between cetaceans and the turbine and to provide high resolution tracking data for animals close to the turbine. These data can be used to inform stakeholders and regulators on the likely impact of tidal turbines on cetaceans.},
   author = {Laura A N D Macaulay Jamie A N D Sparling Carol A N D Hastie Gordon Gillespie Douglas AND Palmer},
   doi = {10.1371/journal.pone.0229058},
   journal = {PLOS ONE},
   month = {11},
   pages = {1-16},
   publisher = {Public Library of Science},
   title = {Passive acoustic methods for tracking the 3D movements of small cetaceans around marine structures},
   url = {https://doi.org/10.1371/journal.pone.0229058},
   year = {2020},
}
@article{Dufourq2022,
   abstract = {Progress in deep learning, more specifically in using convolutional neural networks (CNNs) for the creation of classification models, has been tremendous in recent years. Within bioacoustics research, there has been a large number of recent studies that use CNNs. Designing CNN architectures from scratch is non-trivial and requires knowledge of machine learning. Furthermore, hyper-parameter tuning associated with CNNs is extremely time consuming and requires expensive hardware. In this paper we assess whether it is possible to build good bioacoustic classifiers by adapting and re-using existing CNNs pre-trained on the ImageNet dataset - instead of designing them from scratch, a strategy known as transfer learning that has proved highly successful in other domains. This study is a first attempt to conduct a large-scale investigation on how transfer learning can be used for passive acoustic monitoring (PAM), to simplify the implementation of CNNs and the design decisions when creating them, and to remove time consuming hyper-parameter tuning phases. We compare 12 modern CNN architectures across 4 passive acoustic datasets that target calls of the Hainan gibbon Nomascus hainanus, the critically endangered black-and-white ruffed lemur Varecia variegata, the vulnerable Thyolo alethe Chamaetylas choloensis, and the Pin-tailed whydah Vidua macroura. We focus our work on data scarcity issues by training PAM binary classification models very small datasets, with as few as 25 verified examples. Our findings reveal that transfer learning can result in up to 82% F1 score while keeping CNN implementation details to a minimum, thus rendering this approach accessible, easier to design, and speeding up further vocalisation annotations to create PAM robust models.},
   author = {Emmanuel Dufourq and Carly Batist and Ruben Foquet and Ian Durbach},
   doi = {https://doi.org/10.1016/j.ecoinf.2022.101688},
   issn = {1574-9541},
   journal = {Ecological Informatics},
   keywords = {Bioacoustics,Convolutional neural networks,Deep learning,Transfer learning,Vocalisation classification},
   title = {Passive acoustic monitoring of animal populations with transfer learning},
   url = {https://www.sciencedirect.com/science/article/pii/S1574954122001388},
   year = {2022},
}
@article{Ruff2023,
   abstract = {We present PNW-Cnet v4, a deep neural net with an associated Shiny-based application designed to facilitate efficient data processing to detect terrestrial wildlife species through passive acoustic monitoring. PNW-Cnet v4 is a deep convolutional neural network that detects audio signatures of 37 focal species of birds and mammals that inhabit forests of the Pacific Northwest, USA, along with other commonly occurring forest sounds. The primary objective of developing PNW-Cnet v4 was to support a long-term northern spotted owl (Strix occidentalis caurina) monitoring program. By incorporating additional species classes, PNW-Cnet v4 expands applicability of the program to broadscale biodiversity research and monitoring. Using the Shiny app with PNW-Cnet v4, users can process audio data using a graphical user interface, summarize apparent detections visually, and export results in tabular format.},
   author = {Zachary J Ruff and Damon B Lesmeister and Julianna M A Jenkins and Christopher M Sullivan},
   doi = {10.1016/j.softx.2023.101473},
   issn = {2352-7110},
   journal = {SoftwareX},
   month = {11},
   publisher = {Elsevier BV},
   title = {PNW-Cnet v4: Automated species identification for passive acoustic monitoring},
   volume = {23},
   url = {http://dx.doi.org/10.1016/j.softx.2023.101473},
   year = {2023},
}
@misc{,
   abstract = {The Aurora LNG Project (the Project) is a joint venture between Nexen Energy ULC, INPEX Corporation, 
and JGC Corporation to develop a liquefied natural gas (LNG) plant, export facility, and associated 
marine terminal near Prince Rupert, British Columbia. An Environmental Impact Assessment is being 
developed as part of this process. 
This report analyzes acoustic data collected for this Project from two underwater acoustic recorders 
placed between Digby and Kaien Islands and across Chatham Sound near Triple Island, BC.  
Stantec Consulting Ltd., under contract to Nexen Energy ULC, engaged JASCO Applied Sciences to 
document baseline noise conditions near the proposed Project site. JASCO made these measurements 
with two Autonomous Multichannel Acoustic Recorders (AMARs) that recorded acoustic data between 
July and October/November 2014. The first AMAR, Station 1, was located approximately 3 km southwest 
of Prince Rupert, near the proposed Project site. The second AMAR, Station 2, was located 
approximately 30 km from the Project site, near the ferry and container vessel traffic lanes. 
Measurements were also made at a third location, Grassy Point, because both Grassy Point and Digby 
Island were being considered as potential Project sites; however, Nexen has formally withdrawn the 
Grassy Point site so data for this location were not analyzed. 
Automated analysis techniques were applied to the acoustic data to determine the presence of marine 
mammals in the Project area. Clicks from undistinguished species of porpoises were detected almost 
daily at both stations throughout the recording period, whereas calls from undistinguished ecotypes of 
killer whales were detected sporadically, with Station 2 having more detection days than Station 1. Pacific 
white-sided dolphins were detected only on one day (in July) at Station 2, but not at all at Station 1. 
Humpback whale calls were detected only at Station 2, with the number of detection days peaking from 
mid-July to late October. Fin whale calls were detected only twice (in October) and only at Station 2. 
Harbour seal calls were detected only at Station 1 and occurred mainly from mid-July to mid-August. 
Vessels contributed to the soundscape at both stations; Station 1 had the most vessels detected per day 
and showed a strong diel trend with more vessels during the day than the night, while this pattern was not 
observed at Station 2. Wind also contributed to ambient noise at Station 2. 
Sound levels at Station 1 exceeded 120 dB re 1 µPa root-mean-square sound pressure level (rms SPL), 
the regulatory threshold set by the United States-based organization National Oceanic and Atmospheric 
Administration Fisheries (NOAA) for marine mammal disturbance with non-impulsive noise sources, 8.2% 
of the time. At Station 2, this threshold was exceeded 2.5% of the time.},
   author = {H Frouin-Mouy and H Yurk and X Mouy and B Martin},
   issue = {Document 01129, Version 3.0},
   institution = {JASCO Applied Sciences},
   title = {Prince Rupert - Aurora LNG Acoustic Monitoring Study: Chatham Sound Region},
   url = {https://projects.eao.gov.bc.ca/api/document/58923173b637cc02bea163f0/fetch/Appendix_O_Acoustic_Monitoring_Final_screening.pdf},
   year = {2016},
}
@article{Rivas2024,
   abstract = {Crickets were the first study subjects at the dawn of bioacoustics, yet, almost 100 years later, freely available and robust analysis tools and protocols are still needed.We introduce Rthoptera, a new open-source R package for the analysis of insect acoustic signals, offering accurate graphics and standardised temporal and spectral measurements through a streamlined workflow. Our package delivers results in a fraction of the time typically required by multi-software methods. Most of the functions have interactive versions (Shiny applications), facilitating their adoption by researchers with any level of technical expertise.New acoustic metrics are introduced: Spectral Excursion, Pattern Complexity Index, Temporal Excursion, Dynamic Excursion, and Broadband Activity Index.Accompanied by an appropriate recording protocol, our tool can become the backbone of a standard analysis protocol for comparative insect bioacoustics.Competing Interest StatementThe authors have declared no competing interest.},
   author = {Francisco Rivas and Cesare Brizio and Filippo M Buzzetti and Bryan Pijanowski},
   doi = {10.1101/2024.11.04.621915},
   journal = {bioRxiv},
   publisher = {Cold Spring Harbor Laboratory},
   title = {Rthoptera: Standardised Insect Bioacoustics in R},
   url = {https://www.biorxiv.org/content/early/2024/11/08/2024.11.04.621915},
   year = {2024},
}
@inproceedings{Chen2021,
   abstract = {With the continuous development of offshore engineering, major offshore projects develop rapidly. During the development of offshore engineering, different degrees of noise will be generated, and the man-made noise can have harmful effects on marine mammals. Currently, The researchers usually use Passive Acoustic Monitoring(PAM) method to monitor the marine mammals. However, it is impossible to acquire, monitor and analyze the sound of marine mammals in real time and lack of comprehensive information of marine mammals monitoring, and the data analysis and study of vocalization rules can only be carried out after data collection is completed and equipment is recovered. Therefore, this paper proposes a real-time automatic detection and classification technology to monitor targeted marine mammals efficiently and continuously timely in offshore engineering areas.},
   author = {Yankun Chen and Weiping Wang and Yinian Liang and Defu Zhou and Chao Dong and Jie Li},
   doi = {10.1109/coa50123.2021.9519906},
   booktitle = {2021 OES China Ocean Acoustics (COA)},
   month = {11},
   pages = {1027-1031},
   publisher = {IEEE},
   title = {Real-time Detection and Classification for Targeted Marine Mammals},
   url = {http://dx.doi.org/10.1109/COA50123.2021.9519906},
   year = {2021},
}
@article{,
   abstract = {Remote acquisition of information on ecosystem dynamics is essential for conservation management, especially for the deep ocean. Soundscape offers unique opportunities to study the behavior of soniferous marine animals and their interactions with various noise-generating activities at a fine temporal resolution. However, the retrieval of soundscape information remains challenging owing to limitations in audio analysis techniques that are effective in the face of highly variable interfering sources. This study investigated the application of a seafloor acoustic observatory as a long-term platform for observing marine ecosystem dynamics through audio source separation. A source separation model based on the assumption of source-specific periodicity was used to factorize time-frequency representations of long-duration underwater recordings. With minimal supervision, the model learned to discriminate source-specific spectral features and prove to be effective in the separation of sounds made by cetaceans, soniferous fish, and abiotic sources from the deep-water soundscapes off northeastern Taiwan. Results revealed phenological differences among the sound sources and identified diurnal and seasonal interactions between cetaceans and soniferous fish. The application of clustering to source separation results generated a database featuring the diversity of soundscapes and revealed a compositional shift in clusters of cetacean vocalizations and fish choruses during diurnal and seasonal cycles. The source separation model enables the transformation of single-channel audio into multiple channels encoding the dynamics of biophony, geophony, and anthropophony, which are essential for characterizing the community of soniferous animals, quality of acoustic habitat, and their interactions. Our results demonstrated the application of source separation could facilitate acoustic diversity assessment, which is a crucial task in soundscape-based ecosystem monitoring. Future implementation of soundscape information retrieval in long-term marine observation networks will lead to the use of soundscapes as a new tool for conservation management in an increasingly noisy ocean.},
   author = {Tomonari A N D Tsao Yu Lin Tzu-Hao AND Akamatsu},
   doi = {10.1371/journal.pcbi.1008698},
   issue = {2},
   journal = {PLOS Computational Biology},
   month = {11},
   pages = {1-23},
   publisher = {Public Library of Science},
   title = {Sensing ecosystem dynamics via audio source separation: A case study of marine soundscapes off northeastern Taiwan},
   url = {https://doi.org/10.1371/journal.pcbi.1008698},
   year = {2021},
}
@article{Monczak2019,
   abstract = {Soundscape ecology is a relatively new scientific field that uses sound to characterize ecosystems, which can be helpful in tracking species, estimating relative population sizes, and monitoring behavior and overall habitat quality. Estuarine soundscapes are acoustically rich, and sound patterns in these systems are understudied. Therefore, the goal of this study was to understand the soundscape of a deep tidal river estuary, the May River, South Carolina, USA. Acoustic recorders (DSG-Oceans) were deployed to collect sound samples for 2 min every 20 min at 6 stations from February to November 2014. Acoustic data revealed that sound pressure levels (i.e. broadband, low, and high frequency) varied spatially and temporally, exhibiting distinct rhythmic patterns. Acoustic detection rates and diversity of biophonic (e.g. snapping shrimp, fish, and bottlenose dolphins Tursiops truncatus) and anthrophonic sounds (e.g. boat noise) were higher near the river mouth and decreased towards the headwaters. The soundscape exhibited strong temporal patterns of snapping shrimp (genus Alpheus and Synalpheus) snaps, fish calls and choruses (e.g. silver perch Bairdiella chrysoura, black drum Pogonias cromis, oyster toadfish Opsanus tau, spotted seatrout Cynoscion nebulosus, and red drum Sciaenops ocellatus), bottlenose dolphin vocalizations, and vessel noise. Depending upon the species, certain variables (i.e. location, month, day length, lunar phase, day/night, tide, and temperature anomaly) influenced sound production. These data provide new tools and baseline measurements to better understand how soundscapes can be used to gauge habitat quality and impacts of stormwater runoff, climate change, and noise pollution.},
   author = {A Monczak and C Mueller and M E Miller and Y Ji and S A Borgianini and E W Montie},
   doi = {10.3354/meps12813},
   issn = {1616-1599},
   journal = {Marine Ecology Progress Series},
   month = {11},
   pages = {49-68},
   publisher = {Inter-Research Science Center},
   title = {Sound patterns of snapping shrimp, fish, and dolphins in an estuarine soundscape of the southeastern USA},
   url = {http://dx.doi.org/10.3354/meps12813},
   year = {2019},
}
@misc{Bressler2023,
   abstract = {This paper presents Soundbay, an open-source Python framework that allows bio-acoustics and machine learning researchers to implement and utilize deep learning-based algorithms for acoustic audio analysis. Soundbay provides an easy and intuitive platform for applying existing models on one's data or creating new models effortlessly. One of the main advantages of the framework is the capability to compare baselines on different benchmarks, a crucial part of emerging research and development related to the usage of deep-learning algorithms for animal call analysis. We demonstrate this by providing a benchmark for cetacean call detection on multiple datasets. The framework is publicly accessible via this https URL (https://github.com/deep-voice/soundbay)},
   author = {Noam Bressler and Michael Faran and Amit Galor and Michael Moshe Michelashvili and Tomer Nachshon and Noa Weiss},
   title = {Soundbay: Deep Learning Framework for Marine Mammals and Bioacoustic Research},
   url = {https://arxiv.org/abs/2311.04343},
   year = {2023},
}
@article{Bas2017,
   abstract = {Passive Acoustic Monitoring (PAM) recently extended to a very wide range of animals, but no available open software has been sufficiently generic to automatically treat several taxonomic groups. Here we present Tadarida, a software toolbox allowing for the detection and labelling of recorded sound events, and to classify any new acoustic data into known classes. It is made up of three modules handling Detection, Labelling and Classification and running on either Linux or Windows. This development resulted in the first open software (1) allowing generic sound event detection (multi-taxa), (2) providing graphical sound labelling at a single-instance level and (3) covering the whole process from sound detection to classification. This generic and modular design opens numerous reuse opportunities among (bio)acoustics researchers, especially for those managing and/or developing PAM schemes.The whole toolbox is openly developed in C++ (Detection and Labelling) and R (Classification) and stored at https://github.com/YvesBas.},
   author = {Yves Bas and Didier Bas and Jean-François Julien},
   doi = {10.5334/jors.154},
   journal = {Journal of Open Research Software},
   month = {11},
   title = {Tadarida: A Toolbox for Animal Detection on Acoustic Recordings},
   year = {2017},
}
@article{Milanelli2024,
   abstract = {Soundscape ecology, facilitated by the collection of environmental sounds, enables the determination of the area used by specific species, estimation of population abundance, verification of habitat health, and assistance in management and monitoring processes. Estuaries are key environments in the life cycle of many species. As coastal ecosystems, they face potential impacts by human activities. This study aims to characterize the soundscape of the Patos Lagoon estuary (PLE), Brazil, examining its temporal and seasonal variations. Additionally, it seeks to investigate the influence of temperature and salinity on sound pressure levels (SPL) across three different frequency bands throughout the year. For data collection, two autonomous devices - a SoundTrap digital recorder and an F-POD sampler - were used. The data collection spanned from September 2020 to August 2021 and covered one consecutive week per season. The analysis of audio files was performed manually using Raven software. Sound emissions by Lahille's bottlenose dolphin (Tursiops truncatus gephyreus) and fish were observed throughout the sampling period, with peak detections occurring during the summer and spring, respectively. Boat noise was detected throughout the year but more intensely in the summer. Generalized linear models (GLMs) revealed that, for the three SPL frequency bands, the season significantly influenced the composition of the PLE soundscape, with positive fluctuations in the warmer seasons (summer > spring > autumn > winter). Salinity, in turn, showed a slightly negative relationship with the SPL values in the three models, suggesting a decrease in frequencies with increasing salinity. Our results also indicate that the low-frequency SPL, originating mostly from various sources of anthropogenic noise, constitutes the main component of the PLE soundscape. In conclusion, our study provides novel insights into the temporal patterns of the soundscape in an urban estuary, including biotic and anthropogenic sounds, and their relationships with selected environmental variables.},
   author = {A M Milanelli and M R Rossi-Santos and P F Fruet and R Assumpção and A M Cavalcanti and L Dalla Rosa},
   doi = {10.1016/j.ecss.2023.108596},
   issn = {0272-7714},
   journal = {Estuarine, Coastal and Shelf Science},
   month = {11},
   publisher = {Elsevier BV},
   title = {Temporal patterns in the soundscape of the port area in an urban estuary},
   volume = {297},
   url = {http://dx.doi.org/10.1016/j.ecss.2023.108596},
   year = {2024},
}
@article{,
   abstract = {Land transformation into agricultural areas and the intensification of management practices represent two of the most devastating threats to biodiversity worldwide. Within this study, we investigated the effect of intensively managed agroecosystems on bat activity and species composition within two focal areas differing in landscape structure. We sampled bats via acoustic monitoring and insects with flight interception traps in banana and pineapple monoculture plantations and two nearby protected forested areas within the area of Sarapiquí, Costa Rica. Our results revealed that general occurrence and feeding activity of bats was higher above plantations compared to forested areas. We also recorded higher species richness at recording sites in plantations. This trend was especially strong within a fragmented landscape, with only four species recorded in forests, but 12 above pineapple plantations. Several bat species, however, occurred only once or twice above plantations, and forest specialist species such as Centronycteris centralis, Myotis riparius and Pteronotus mesoamericanus were only recorded at forest sites. Our results indicated, that mostly mobile open space and edge foraging bat species can use plantations as potential foraging habitat and might even take advantage of temporal insect outbreaks. However, forests are vital refugia for several species, including slower flying forest specialists, and thus a prerequisite to safeguard bat diversity within agricultural dominated landscapes.},
   author = {Bernal A N D Jung Kirsten Alpízar Priscilla AND Rodríguez-Herrera},
   doi = {10.1371/journal.pone.0210364},
   journal = {PLOS ONE},
   month = {11},
   pages = {1-15},
   publisher = {Public Library of Science},
   title = {The effect of local land use on aerial insectivorous bats (Chiroptera) within the two dominating crop types in the Northern-Caribbean lowlands of Costa Rica},
   url = {https://doi.org/10.1371/journal.pone.0210364},
   year = {2019},
}
@article{,
   abstract = {Insects are the most diverse animal taxon on Earth and play a key role in ecosystem functioning. However, they are often neglected by ecological surveys owing to the difficulties involved in monitoring this small and hyper-diverse taxon. With technological advances in biomonitoring and analytical methods, these shortcomings may finally be addressed. Here, we performed passive acoustic monitoring at 141 sites (eight habitats) to investigate insect acoustic activity in the Viruá National Park, Brazil. We first describe the frequency range occupied by three soniferous insect groups (cicadas, crickets and katydids) to calculate the acoustic evenness index (AEI). Then, we assess how AEI varies spatially and temporally among habitat types, and finally we investigate the relationship between vegetation structure variables and AEI for each insect category. Overall, crickets occupied lower and narrower frequency bands than cicadas and katydids. AEI values varied among insect categories and across space and time. The highest acoustic activity occurred before sunrise and the lowest acoustic activity was recorded in pastures. Canopy cover was positively associated with cricket acoustic activity but not with katydids. Our findings contribute to a better understanding of the role of time, habitat and vegetation structure in shaping insect activity within diverse Amazonian ecosystems.},
   author = {Leandro A Do Nascimento and Cristian Pérez-Granados and Janderson B Rodrigues Alencar and Karen H Beard},
   doi = {10.1098/rstb.2023.0112},
   issn = {1471-2970},
   journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
   month = {11},
   publisher = {The Royal Society},
   title = {Time and habitat structure shape insect acoustic activity in the Amazon},
   url = {http://dx.doi.org/10.1098/rstb.2023.0112},
   year = {2024},
}
@article{Gomes2020,
   abstract = {BACKGROUND: Previous research has shown diverse vertical space use by various taxa, highlighting the importance of forest vertical structure. Yet, we know little about vertical space use of tropical forests, and we often fail to explore how this three-dimensional space use changes over time. METHODS: Here we use canopy tower systems in French Guiana and passive acoustic monitoring to measure Neotropical bat activity above and below the forest canopy throughout nine nights. We use a Bayesian generalized linear mixed effect model and kernel density estimates to demonstrate patterns in space-use over time. RESULTS: We found that different bats use both canopy and understory space differently and that these patterns change throughout the night. Overall, bats were more active above the canopy (including Cormura brevirostris, Molossus molossus, Peropteryx kappleri and Peropteryx macrotis), but multiple species or acoustic complexes (when species identification was impossible) were more active in the understory (such as Centronycteris maximiliani, Myotis riparius, Pteronotus alitonus and Pteronotus rubiginosus). We also found that most bats showed temporally-changing preferences in hourly activity. Some species were less active (e.g., P. kappleri and P. macrotis), whereas others were more active (Pteronotus gymnonotus, C. brevirostris, and M. molossus) on nights with higher moon illuminance. DISCUSSION: Here we show that Neotropical bats use habitat above the forest canopy and within the forest understory differently throughout the night. While bats generally were more active above the forest canopy, we show that individual groups of bats use space differently over the course of a night, and some prefer the understory. This work highlights the need to consider diel cycles in studies of space use, as animals use different habitats during different periods of the day.},
   author = {Dylan G E Gomes and Giulliana Appel and Jesse R Barber},
   doi = {10.7717/peerj.10591},
   issue = {e10591},
   journal = {PeerJ},
   keywords = {Bat activity; Chiroptera; Daily cycle; Diel; Moon; Neotropics; Passive acoustic monitoring; Rainforest; Temporal patterns},
   month = {11},
   pages = {e10591},
   pmid = {33384906},
   publisher = {PeerJ},
   title = {Time of night and moonlight structure vertical space use by insectivorous bats in a Neotropical rainforest: an acoustic monitoring study},
   year = {2020},
}
@article{Clink2021,
   abstract = {Abstract Passive acoustic monitoring (PAM) has the potential to greatly improve our ability to monitor cryptic yet vocal animals. Advances in automated signal detection have increased the scope of PAM, but distinguishing between individuals—which is necessary for density estimation—remains a major challenge. When individual identity is known, supervised classification techniques can be used to distinguish between individuals. Supervised methods require labelled training data, whereas unsupervised techniques do not. If the acoustic signals of individuals are sufficiently different, the number of clusters might represent the number of individuals sampled. The majority of applications of unsupervised techniques in animal vocalizations have focused on quantifying species-specific call repertoires. However, with increased interest in PAM applications, unsupervised methods that can distinguish between individuals are needed. Here we use an existing dataset of Bornean gibbon female calls with known identity from five sites on Malaysian Borneo to test the ability of three different unsupervised clustering algorithms (affinity propagation, K-medoids and Gaussian mixture model-based clustering) to distinguish between individuals. Calls from different gibbon females are readily distinguishable using supervised techniques. For internal validation of unsupervised cluster solutions, we calculated silhouette coefficients. For external validation, we compared clustering results with female identity labels using a standard metric: normalized mutual information. We also calculated classification accuracy by assigning unsupervised cluster solutions to females based on which cluster had the highest number of calls from a particular female. We found that affinity propagation clustering consistently outperformed the other algorithms for all metrics used. In particular, classification accuracy of affinity propagation clustering was more consistent as the number of females increased, and when we randomly sampled females across sites. We conclude that unsupervised techniques may be useful for providing additional information regarding individual identity for PAM applications. We stress that although we use gibbons as a case study, these methods will be applicable for any individually distinct vocal animal.},
   author = {Dena J Clink and Holger Klinck},
   doi = {https://doi.org/10.1111/2041-210X.13520},
   issue = {2},
   journal = {Methods in Ecology and Evolution},
   keywords = {Gaussian mixture models,Hylobates,K-medoids,affinity propagation,normalized mutual information,passive acoustic monitoring,unsupervised clustering},
   pages = {328-341},
   title = {Unsupervised acoustic classification of individual gibbon females and the implications for passive acoustic monitoring},
   url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13520},
   year = {2021},
}
@article{Rigakis2021,
   abstract = {Is there a wood-feeding insect inside a tree or wooden structure? We investigate several ways of how deep learning approaches can massively scan recordings of vibrations stemming from probed trees to infer their infestation state with wood-boring insects that feed and move inside wood. The recordings come from remotely controlled devices that sample the internal soundscape of trees on a 24/7 basis and wirelessly transmit brief recordings of the registered vibrations to a cloud server. We discuss the different sources of vibrations that can be picked up from trees in urban environments and how deep learning methods can focus on those originating from borers. Our goal is to match the problem of the accelerated—due to global trade and climate change— establishment of invasive xylophagus insects by increasing the capacity of inspection agencies. We aim at introducing permanent, cost-effective, automatic monitoring of trees based on deep learning techniques, in commodity entry points as well as in wild, urban and cultivated areas in order to effect large-scale, sustainable pest-risk analysis and management of wood boring insects such as those from the Cerambycidae family (longhorn beetles).},
   author = {Iraklis Rigakis and Ilyas Potamitis and Nicolaos-Alexandros Tatlas and Stelios M Potirakis and Stavros Ntalampiras},
   doi = {10.3390/smartcities4010017},
   issn = {2624-6511},
   journal = {Smart Cities},
   month = {11},
   pages = {271-285},
   publisher = {MDPI AG},
   title = {TreeVibes: Modern Tools for Global Monitoring of Trees for Borers},
   url = {http://dx.doi.org/10.3390/smartcities4010017},
   year = {2021},
}
