@Article{Silva2022,
  AUTHOR = {Silva, Bruno and Mestre, Frederico and Barreiro, Sílvia and Alves, Pedro J. and Herrera, José M.},
  JOURNAL = {Methods in Ecology and Evolution},
  MONTH = {08},
  NUMBER = {},
  PAGES = {2356--2362},
  PUBLISHER = {Wiley},
  TITLE = {soundClass: An automatic sound classification tool for biodiversity monitoring using machine learning},
  VOLUME = {},
  YEAR = {2022},
  URL = {http://dx.doi.org/10.1111/2041-210X.13964},
  DOI = {10.1111/2041-210x.13964},
  ISSN = {2041-210X}
}


@Book{Hasselman2020,
  AUTHOR = {Hasselman, Daniel and Barclay, David and Cavagnaro, Robert and Chandler, Craig and Cotter, Emma and Gillespie, Douglas and Hastie, Gordon and Horne, John and Joslin, James and Long, Caitlin and McGarry, Louise and Mueller, Robert and Sparling, Carol and Williamson, Benjamin},
  INSTITUTION = {Office of Scientific and Technical Information (OSTI)},
  MONTH = {09},
  TITLE = {2020 State of the Science Report, Chapter 10: Environmental Monitoring Technologies and Techniques for Detecting Interactions of Marine Animals with Turbines},
  YEAR = {2020},
  URL = {http://dx.doi.org/10.2172/1633202},
  DOI = {10.2172/1633202}
}


@Article{Picciulin2022,
  AUTHOR = {Picciulin, Marta and Bolgan, Marta and Rako-Gospić, Nikolina and Petrizzo, Antonio and Radulović, Marko and Falkner, Raffaela},
  JOURNAL = {Journal of Marine Science and Engineering},
  MONTH = {02},
  NUMBER = {2},
  PAGES = {},
  PUBLISHER = {MDPI AG},
  TITLE = {A Fish and Dolphin Biophony in the Boat Noise-Dominated Soundscape of the Cres-Lošinj Archipelago (Croatia)},
  VOLUME = {},
  YEAR = {2022},
  URL = {http://dx.doi.org/10.3390/jmse10020300},
  DOI = {10.3390/jmse10020300},
  ISSN = {2077-1312}
}


@Article{PUTLAND2018479,
  AUTHOR = {R.L. Putland and L. Ranjard and R. Constantine and C.A. Radford},
  JOURNAL = {Ecological Indicators},
  PAGES = {479--487},
  TITLE = {A hidden Markov model approach to indicate Bryde's whale acoustics},
  VOLUME = {},
  YEAR = {2018},
  URL = {https://www.sciencedirect.com/science/article/pii/S1470160X17305939},
  KEYWORDS = {Acoustic detection, Cetacean, Hidden markov models, Noise mitigation, Passive acoustic monitoring, Temporal variation},
  DOI = {https://doi.org/10.1016/j.ecolind.2017.09.025},
  ISSN = {1470-160X},
  ABSTRACT = {Increasing sound in the ocean from human activity potentially threatens marine animals that use sound to communicate, detect prey, avoid predators and function within their ecosystem. The detection and classification of sound produced by marine animals, such as whales and fish, is an important component in noise mitigation strategies, while also providing valuable insights into their ecology. Traditionally, visual surveys are conducted to assess how these animals utilize a specific area, often underestimating the number of individuals as they don’t spend much time at the surface. Long-term passive acoustic monitoring efforts have become more prevalent to monitor such animals. The large datasets collected can be impractical to manually process, necessitating the development of automated detection methods, which often produce mixed results owing to the broad frequency range and variable duration of many biological sounds. Here we describe a novel approach for automated detection of underwater biophonic sounds employing hidden Markov models (HMM). Acoustic data was collected at a single listening station in Hauraki Gulf, from October 2014 to April 2016. HMM detection models were developed for Bryde’s whales (Balaenoptera edeni) that were used as a model organism because they are notoriously hard to study with traditional visual surveys and produce a characteristic call. Bryde’s whale calls also directly overlap the sounds of anthropogenic activity, in particular the sound of vessels transiting to the busiest port in New Zealand; therefore monitoring whale calls is of utmost importance when confronting increasing sound in the ocean. Vocalizations were detected with a sensitivity of 77% and false positive rate of 23%. Bryde’s whale vocalizations were detected on 11% of all recordings. Overall, there were significantly more detections during summer (n = 1716) than winter (n = 447), and significantly more during the day (n = 1991) compared to night (n = 1264). This study shows the feasibility of using HMMs on long-term acoustic datasets. The method has the potential to be used for a wide range of soniferous animals who, like the Bryde’s whale, also produce unique sounds. The detection method would be particularly useful for mitigation and management strategies of species that are difficult to detect using traditional visual methods.}
}


@Article{jzbg2020015,
  AUTHOR = {Jones, Brittany L. and Oswald, Michael and Tufano, Samantha and Baird, Mark and Mulsow, Jason and Ridgway, Sam H.},
  JOURNAL = {Journal of Zoological and Botanical Gardens},
  NUMBER = {2},
  PAGES = {222--233},
  TITLE = {A System for Monitoring Acoustics to Supplement an Animal Welfare Plan for Bottlenose Dolphins},
  VOLUME = {2},
  YEAR = {2021},
  URL = {https://www.mdpi.com/2673-5636/2/2/15},
  DOI = {10.3390/jzbg2020015},
  ISSN = {2673-5636},
  ABSTRACT = {Animal sounds are commonly used by humans to infer information about their motivations and their health, yet, acoustic data is an underutilized welfare biomarker especially for aquatic animals. Here, we describe an acoustic monitoring system that is being implemented at the U.S. Navy Marine Mammal Program where dolphins live in groups in ocean enclosures in San Diego Bay. A four-element bottom mounted hydrophone array is used to continuously record, detect and localize acoustic detections from this focal group. Software provides users an automated comparison of the current acoustic behavior to group historical data which can be used to identify periods of normal, healthy thriving dolphins, and allows rare instances of deviations from typical behavior to stand out. Variations in a group or individual’s call rates can be correlated with independent veterinary examinations and behavioral observations in order to better assess dolphin health and welfare. Additionally, the monitoring system identifies time periods in which a sound source from San Diego Bay is of high-enough amplitude that the received level at our array is considered a potential concern for the focal animals. These time stamps can be used to identify and potentially mitigate exposures to acoustic sources that may otherwise not be obvious to human listeners. We hope this application inspires zoos and aquaria to innovate and create ways to incorporate acoustic information into their own animal welfare management programs.}
}


@Article{Halliday_2019,
  AUTHOR = {Halliday, W.D. and Pine, M.K. and Insley, S.J. and Soares, R.N. and Kortsalo, P. and Mouy, X.},
  JOURNAL = {Canadian Journal of Zoology},
  MONTH = {01},
  NUMBER = {},
  PAGES = {72-80},
  PUBLISHER = {Canadian Science Publishing},
  TITLE = {Acoustic detections of Arctic marine mammals near Ulukhaktok, Northwest Territories, Canada},
  VOLUME = {},
  YEAR = {2019},
  URL = {http://dx.doi.org/10.1139/cjz-2018-0077},
  DOI = {10.1139/cjz-2018-0077},
  ISSN = {1480-3283}
}


@Article{Papin_2018,
  AUTHOR = {Papin, Morgane and Pichenot, Julian and Guérold, François and Germain, Estelle},
  JOURNAL = {Frontiers in Zoology},
  MONTH = {04},
  NUMBER = {},
  PUBLISHER = {Springer Science and Business Media LLC},
  TITLE = {Acoustic localization at large scales: a promising method for grey wolf monitoring},
  VOLUME = {},
  YEAR = {2018},
  URL = {http://dx.doi.org/10.1186/s12983-018-0260-2},
  DOI = {10.1186/s12983-018-0260-2},
  ISSN = {1742-9994}
}


@Article{https://doi.org/10.1111/2041-210X.12730,
  AUTHOR = {Wrege, Peter H. and Rowland, Elizabeth D. and Keen, Sara and Shiu, Yu},
  JOURNAL = {Methods in Ecology and Evolution},
  NUMBER = {},
  PAGES = {1292--1301},
  TITLE = {Acoustic monitoring for conservation in tropical forests: examples from forest elephants},
  VOLUME = {},
  YEAR = {2017},
  URL = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12730},
  KEYWORDS = {anthropogenic impacts, anti-poaching, bioacoustics, cryptic species, density estimate, law enforcement monitoring, Loxodonta cyclotis, passive acoustic monitoring, signal detection, smart},
  DOI = {https://doi.org/10.1111/2041-210X.12730},
  ABSTRACT = {Summary The accelerating loss of biodiversity worldwide demands effective tools for monitoring animal populations and informing conservation action. In habitats where direct observation is difficult (rain forests, oceans), or for cryptic species (shy, nocturnal), passive acoustic monitoring (PAM) provides cost-effective, unbiased data collection. PAM has broad applicability in terrestrial environments, particularly tropical rain forests. Using examples from studies of forest elephants in Central African rain forest, we show how PAM can be used to investigate cryptic behaviour, mechanisms of communication, estimate population size, quantify threats, and assess the efficacy of conservation strategies. We discuss the methodologies, requirements, and challenges of obtaining these data using acoustics. Where applicable, we compare these methods to more traditional approaches. While PAM methods and associated analysis are maturing rapidly, mechanisms are needed for processing the dense raw data efficiently with standard computer hardware, speeding development of detection algorithms, and harnessing communication networks to move data from the field to research facilities. Passive acoustic monitoring is a viable and cost-effective tool for conservation and should be incorporated in monitoring schemes much more broadly. The capability to quickly assess changes in behaviour, population size, and landscape use, simultaneously over large geographical areas, makes this approach attractive for detecting human-induced impacts and for assessing the success of conservation strategies.},
  EPRINT = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12730}
}


@Article{Berman_2024,
  AUTHOR = {Berman, Laura and Xuan Tan, Wei and Grafe, Ulmar and Rheindt, Frank},
  JOURNAL = {Journal of Avian Biology},
  MONTH = {06},
  PUBLISHER = {Wiley},
  TITLE = {Acoustic phenology of tropical resident birds differs between native forest species and parkland colonizer species},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1111/jav.03298},
  DOI = {10.1111/jav.03298},
  ISSN = {1600-048X}
}


@Techreport{WHOI2007,
  AUTHOR = {{Woods Hole Oceanographic Institution}},
  INSTITUTION = {Defense Technical Information Center},
  MONTH = {03},
  NUMBER = {ADA606315},
  TITLE = {Acoustic Response and Detection of Marine Mammals Using an Advanced Digital Acoustic Recording Tag (Rev 3)},
  YEAR = {2007},
  URL = {https://apps.dtic.mil/sti/pdfs/ADA606315.pdf}
}


@Book{Murugaiya_2022,
  AUTHOR = {Murugaiya, Ramashini and Mahagammulle Gamage, Manisha Milani and Murugiah, Krishani and Perumal, Madhumathy},
  JOURNAL = {SpringerBriefs in Applied Sciences and Technology},
  PUBLISHER = {Springer International Publishing},
  TITLE = {Acoustic-Based Applications for Vertebrate Vocalization},
  YEAR = {2022},
  URL = {http://dx.doi.org/10.1007/978-3-030-85773-8},
  DOI = {10.1007/978-3-030-85773-8},
  ISSN = {2191-5318},
  ABSTRACT = {Acoustic-Based Applications for Vertebrate Vocalization is designed to help researchers improve their findings and knowledge of vertebrate vocalization by focusing on the integration of acoustic features with new technologies, such as the Internet of Things (IoT), cloud computing, and virtual and cognitive reality. The book addresses the most common challenges in vertebrate vocalization-based research via suitable audio signal processing techniques, data collection, data pre-processing, acoustic feature engineering, extraction, and selection for multidisciplinary applications, i.e. feature classification, vertebrate communication, behavioral analysis, and signal pattern analysis. The book is an important reference for scholars, researchers, industry practitioners, teachers, and students across a number of disciplines, including bioengineering, audio engineering, systems engineering, biotechnology, signal processing, biology, zoology, and animal sciences.},
  ISBN = {}
}


@Article{Sugimatsu14,
  AUTHOR = {Sugimatsu, Harumi and Kojima, Junichi and Ura, Tamaki and Bahl, R. and Behera, Sandeep and Sagar, Vivek and Singh, Hari and De, Rupak},
  JOURNAL = {Marine Technology Society Journal},
  MONTH = {05},
  PAGES = {},
  TITLE = {Advanced Technique for Automatic Detection and Discrimination of a Click Train With Short Interclick Intervals From the Clicks of Ganges River Dolphins (Platanista gangetica gangetica) Recorded by a Passive Acoustic Monitoring System Using Hydrophone Arrays},
  VOLUME = {},
  YEAR = {2014},
  DOI = {10.4031/MTSJ.48.3.15}
}


@Article{Singer_2024,
  AUTHOR = {Singer, David and Hagge, Jonas and Kamp, Johannes and Hondong, Hermann and Schuldt, Andreas},
  JOURNAL = {Remote Sensing in Ecology and Conservation},
  MONTH = {02},
  NUMBER = {},
  PAGES = {517-530},
  PUBLISHER = {Wiley},
  TITLE = {Aggregated time-series features boost species-specific differentiation of true and false positives in passive acoustic monitoring of bird assemblages},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1002/rse2.385},
  DOI = {10.1002/rse2.385},
  ISSN = {2056-3485}
}


@Article{10.3897/BDJ.11.e97811,
  AUTHOR = {Shih-Hung Wu and Jerome Chie-Jen Ko and Ruey-Shing Lin and Wen-Ling Tsai and Hsueh-Wen Chang},
  JOURNAL = {Biodiversity Data Journal},
  NUMBER = {},
  PAGES = {e97811},
  PUBLISHER = {Pensoft Publishers},
  TITLE = {An acoustic detection dataset of birds (Aves) in montane forests using a deep learning approach},
  VOLUME = {},
  YEAR = {2023},
  URL = {https://doi.org/10.3897/BDJ.11.e97811},
  DOI = {10.3897/BDJ.11.e97811},
  ISSN = {1314-2836},
  ABSTRACT = {Long-term monitoring is needed to understand the statuses and trends of wildlife communities in montane forests, such as those in Yushan National Park (YSNP), Taiwan. Integrating passive acoustic monitoring (PAM) with an automated sound identifier, a long-term biodiversity monitoring project containing six PAM stations, was launched in YSNP in January 2020 and is currently ongoing. SILIC, an automated wildlife sound identification model, was used to extract sounds and species information from the recordings collected. Animal vocal activity can reflect their breeding status, behaviour, population, movement and distribution, which may be affected by factors, such as habitat loss, climate change and human activity. This massive amount of wildlife vocalisation dataset can provide essential information for the National Park's headquarters on resource management and decision-making. It can also be valuable for those studying the effects of climate change on animal distribution and behaviour at a regional or global scale.To our best knowledge, this is the first open-access dataset with species occurrence data extracted from sounds in soundscape recordings by artificial intelligence. We obtained seven bird species for the first release, with more bird species and other taxa, such as mammals and frogs, to be updated annually. Raw recordings containing over 1.7 million one-minute recordings collected between the years 2020 and 2021 were analysed and SILIC identified 6,243,820 vocalisations of seven bird species in 439,275 recordings. The automatic detection had a precision of 0.95 and the recall ranged from 0.48 to 0.80. In terms of the balance between precision and recall, we prioritised increasing precision over recall in order to minimise false positive detections. In this dataset, we summarised the count of vocalisations detected per sound class per recording which resulted in 802,670 occurrence records. Unlike data from traditional human observation methods, the number of observations in the Darwin Core "organismQuantity" column refers to the number of vocalisations detected for a specific bird species and cannot be directly linked to the number of individuals.We expect our dataset will be able to help fill the data gaps of fine-scale avian temporal activity patterns in montane forests and contribute to studies concerning the impacts of climate change on montane forest ecosystems on regional or global scales.},
  EPRINT = {https://doi.org/10.3897/BDJ.11.e97811}
}


@Article{Lin_2013,
  AUTHOR = {Lin, Tzu-Hao and Chou, Lien-Siang and Akamatsu, Tomonari and Chan, Hsiang-Chih and Chen, Chi-Fang},
  JOURNAL = {The Journal of the Acoustical Society of America},
  MONTH = {09},
  NUMBER = {},
  PAGES = {2477-2485},
  PUBLISHER = {Acoustical Society of America (ASA)},
  TITLE = {An automatic detection algorithm for extracting the representative frequency of cetacean tonal sounds},
  VOLUME = {},
  YEAR = {2013},
  URL = {http://dx.doi.org/10.1121/1.4816572},
  DOI = {10.1121/1.4816572},
  ISSN = {1520-8524}
}


@Article{Kershenbaum2013,
  AUTHOR = {Kershenbaum, Arik and Roch, Marie A.},
  JOURNAL = {The Journal of the Acoustical Society of America},
  MONTH = {},
  NUMBER = {},
  PAGES = {4435--4445},
  PUBLISHER = {Acoustical Society of America (ASA)},
  TITLE = {An image processing based paradigm for the extraction of tonal sounds in cetacean communications},
  VOLUME = {},
  YEAR = {2013},
  URL = {http://dx.doi.org/10.1121/1.4828821},
  DOI = {10.1121/1.4828821},
  ISSN = {1520-8524}
}


@Article{Buchan_2019,
  AUTHOR = {Buchan, Susannah J. and Mahú, Rodrigo and Wuth, Jorge and Balcazar-Cabrera, Naysa and Gutierrez, Laura and Neira, Sergio and Yoma, Néstor Becerra},
  JOURNAL = {Bioacoustics},
  MONTH = {01},
  NUMBER = {2},
  PAGES = {140-167},
  PUBLISHER = {Informa UK Limited},
  TITLE = {An unsupervised Hidden Markov Model-based system for the detection and classification of blue whale vocalizations off Chile},
  VOLUME = {29},
  YEAR = {2019},
  URL = {http://dx.doi.org/10.1080/09524622.2018.1563758},
  DOI = {10.1080/09524622.2018.1563758},
  ISSN = {2165-0586}
}


@Article{Diepstraten2021,
  AUTHOR = {Diepstraten, Johan and Willie, Jacob},
  JOURNAL = {Global Ecology and Conservation},
  PAGES = {e01850},
  TITLE = {Assessing the structure and drivers of biological sounds along a disturbance gradient},
  VOLUME = {},
  YEAR = {2021},
  URL = {https://doi.org/10.1016/j.gecco.2021.e01850},
  DOI = {10.1016/j.gecco.2021.e01850}
}


@Report{Scheidat2009,
  AUTHOR = {Scheidat, M and Aarts, G and Bakker, A and Brasseur, S and Carstensen, J and van Leeuwen, P and Leopold, M and van Polanen Petel, T and Reijnders, P and Teilmann, J and Tougaard, J and Verdaat, H},
  INSTITUTION = {IMARES - Wageningen UR},
  MONTH = {09},
  PAGES = {},
  TITLE = {Assessment of the Effects of the Offshore Wind Farm Egmond aan Zee (OWEZ) for Harbour Porpoise (comparison T0 and T1)},
  YEAR = {2009},
  URL = {http://www.wur.nl/en/Publication-details.htm?publicationId = publication-way-343338363132},
  KEYWORDS = {Wind Energy, Fixed Offshore Wind, Habitat Change, Marine Mammals},
  ABSTRACT = {The aim of this study was to investigate if the Offshore Wind farm Egmond aan Zee (OWEZ) influenced the occurrence of harbour porpoises. In order to evaluate the environmental impacts of OWEZ, porpoise occurrence in the area was monitored: during a baseline (T0) study 2003/2004 (Brasseur at al. 2004) after the construction of the wind park (T1) from 2007 to 2009. The comparison between the T0 and the T1 was conducted to determine if and how harbour porpoises react to the presence of the wind park. This report describes the results and analyses of this comparison.},
  LANGUAGE = {English}
}


@Article{Mcloughlin_2019,
  AUTHOR = {Mcloughlin, Michael P. and Stewart, Rebecca and McElligott, Alan G.},
  JOURNAL = {Journal of The Royal Society Interface},
  MONTH = {06},
  NUMBER = {},
  PAGES = {20190225},
  PUBLISHER = {The Royal Society},
  TITLE = {Automated bioacoustics: methods in ecology and conservation and their potential for animal welfare monitoring},
  VOLUME = {},
  YEAR = {2019},
  URL = {http://dx.doi.org/10.1098/rsif.2019.0225},
  DOI = {10.1098/rsif.2019.0225},
  ISSN = {1742-5662}
}


@Inproceedings{10032472,
  AUTHOR = {Vargas-Masís, Roberto and Segura-Sequeira, David and Alfaro-Rojas, Danny and Díaz, Daniel Rosen},
  BOOKTITLE = {2022 IEEE 4th International Conference on BioInspired Processing (BIP)},
  NUMBER = {},
  PAGES = {1--8},
  TITLE = {Automated bird acoustic detection at Las Arrieras Nature Reserve in Sarapiquí, Costa Rica},
  VOLUME = {},
  YEAR = {2022},
  KEYWORDS = {Biological system modeling;Ecosystems;Decision making;Birds;Acoustics;Ecology;Behavioral sciences;Acoustic detection;Passive acoustic monitoring;Random forest model;Pattern matching;conservation;Costa Rica},
  DOI = {10.1109/BIP56202.2022.10032472}
}


@Article{van_Kuijk_2023,
  AUTHOR = {van Kuijk, Silvy M. and O'Brien, Sun and Clink, Dena J. and Blake, John G. and Di Fiore, Anthony},
  JOURNAL = {Frontiers in Ecology and Evolution},
  MONTH = {08},
  PUBLISHER = {Frontiers Media SA},
  TITLE = {Automated detection and detection range of primate duets: a case study of the red titi monkey (Plecturocebus discolor) using passive acoustic monitoring},
  VOLUME = {},
  YEAR = {2023},
  URL = {http://dx.doi.org/10.3389/fevo.2023.1173722},
  DOI = {10.3389/fevo.2023.1173722},
  ISSN = {2296-701X}
}


@Inproceedings{Ho_Chun_Huang_2016,
  AUTHOR = {Ho Chun Huang and Joseph, John and Ming Jer Huang and Margolina, Tetyana},
  BOOKTITLE = {OCEANS 2016 MTS/IEEE Monterey},
  MONTH = {09},
  NUMBER = {},
  PAGES = {1-7},
  PUBLISHER = {IEEE},
  TITLE = {Automated detection and identification of blue and fin whale foraging calls by combining pattern recognition and machine learning techniques},
  VOLUME = {},
  YEAR = {2016},
  DOI = {10.1109/oceans.2016.7761269}
}


@Article{Barker_2014,
  AUTHOR = {Barker, David J. and Herrera, Christopher and West, Mark O.},
  JOURNAL = {Journal of Neuroscience Methods},
  MONTH = {10},
  PAGES = {68-75},
  PUBLISHER = {Elsevier BV},
  TITLE = {Automated detection of 50-kHz ultrasonic vocalizations using template matching in XBAT},
  VOLUME = {236},
  YEAR = {2014},
  URL = {http://dx.doi.org/10.1016/j.jneumeth.2014.08.007},
  DOI = {10.1016/j.jneumeth.2014.08.007},
  ISSN = {0165-0270}
}


@Article{Owens2024.04.15.589517,
  AUTHOR = {Owens, A. F. and Hockings, Kimberley J. and Imron, Muhammed Ali and Madhusudhana, Shyam and Mariaty and Setia, Tatang Mitra and Sharma, Manmohan and Maimunah, Siti and Van Veen, F. J. F. and Erb, Wendy M.},
  JOURNAL = {bioRxiv},
  PUBLISHER = {Cold Spring Harbor Laboratory},
  TITLE = {Automated detection of Bornean white-bearded gibbon (Hylobates albibarbis) vocalisations using an open-source framework for deep learning},
  YEAR = {2024},
  URL = {https://www.biorxiv.org/content/early/2024/07/21/2024.04.15.589517},
  DOI = {10.1101/2024.04.15.589517},
  ABSTRACT = {Passive acoustic monitoring is a promising tool for monitoring at-risk populations of vocal species, yet extracting relevant information from large acoustic datasets can be time-consuming, creating a bottleneck at the point of analysis. To address this, we adapted an open-source framework for deep learning in bioacoustics to automatically detect Bornean white-bearded gibbon (Hylobates albibarbis) {\textquotedblleft}great call{\textquotedblright} vocalisations in a long-term acoustic dataset from a rainforest location in Borneo. We describe the steps involved in developing this solution, including collecting audio recordings, developing training and testing datasets, training neural network models, and evaluating model performance. Our best model performed at a satisfactory level (F score = 0.87), identifying 98\% of the highest-quality calls from 90 hours of manually-annotated audio recordings and greatly reduced analysis times when compared to a human observer. We found no significant difference in the temporal distribution of great call detections between the manual annotations and the model{\textquoteright}s output. Future work should seek to apply our model to long-term acoustic datasets to understand spatiotemporal variations in H. albibarbis{\textquoteright} calling activity. Overall, we present a roadmap for applying deep learning to identify the vocalisations of species of interest which can be adapted for monitoring other endangered vocalising species.Competing Interest StatementThe authors have declared no competing interest.},
  EPRINT = {https://www.biorxiv.org/content/early/2024/07/21/2024.04.15.589517.full.pdf},
  ELOCATION.ID = {2024.04.15.589517}
}


@Article{Stowell_2018,
  AUTHOR = {Stowell, Dan and Wood, Michael D. and Pamuła, Hanna and Stylianou, Yannis and Glotin, Hervé},
  EDITOR = {Orme, David},
  JOURNAL = {Methods in Ecology and Evolution},
  MONTH = {11},
  NUMBER = {},
  PAGES = {368-380},
  PUBLISHER = {Wiley},
  TITLE = {Automatic acoustic detection of birds through deep learning: The first Bird Audio Detection challenge},
  VOLUME = {},
  YEAR = {2018},
  URL = {http://dx.doi.org/10.1111/2041-210X.13103},
  KEYWORDS = {bird, deep learning, machine learning, passive acoustic monitoring, sound},
  DOI = {10.1111/2041-210x.13103},
  ISSN = {2041-210X},
  ABSTRACT = {Abstract Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus, passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here, we report outcomes from a collaborative data challenge. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects. Multiple methods were able to attain performance of around 88\% area under the receiver operating characteristic (ROC) curve (AUC), much higher performance than previous general-purpose methods. With modern machine learning, including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data, with no manual recalibration, and no pretraining of the detector for the target species or the acoustic conditions in the target environment.},
  EPRINT = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13103}
}


@Article{FUNDEL2023102288,
  AUTHOR = {Frank Fundel and Daniel A. Braun and Sebastian Gottwald},
  JOURNAL = {Ecological Informatics},
  PAGES = {},
  TITLE = {Automatic bat call classification using transformer networks},
  VOLUME = {},
  YEAR = {2023},
  URL = {https://www.sciencedirect.com/science/article/pii/S1574954123003175},
  KEYWORDS = {Computational bioacoustics, Attention, Transformer, Echolocation, Species identification, Acoustic monitoring, Bat calls},
  DOI = {https://doi.org/10.1016/j.ecoinf.2023.102288},
  ISSN = {1574-9541},
  ABSTRACT = {Automatically identifying bat species from their echolocation calls is a difficult but important task for monitoring bats and the ecosystem they live in. Major challenges in automatic bat call identification are high call variability, similarities between species, interfering calls and lack of annotated data. Many currently available models suffer from relatively poor performance on real-life data due to being trained on single call datasets and, moreover, are often too slow for real-time classification. Here, we propose a Transformer architecture for multi-label classification with potential applications in real-time classification scenarios. We train our model on synthetically generated multi-species recordings by merging multiple bats calls into a single recording with multiple simultaneous calls. Our approach achieves a single species accuracy of 88.92% (F1-score of 84.23%) and a multi species macro F1-score of 74.40% on our test set. In comparison to three other tools on the independent and publicly available dataset ChiroVox, our model achieves at least 25.82% better accuracy for single species classification and at least 6.9% better macro F1-score for multi species classification.}
}


@Article{Zseb_k_2019,
  AUTHOR = {Zsebők, Sándor and Nagy-Egri, Máté Ferenc and Barnaföldi, Gergely Gábor and Laczi, Miklós and Nagy, Gergely and Vaskuti, Éva and Garamszegi, László Zsolt},
  JOURNAL = {Ornis Hungarica},
  MONTH = {12},
  NUMBER = {2},
  PAGES = {59-66},
  PUBLISHER = {Walter de Gruyter GmbH},
  TITLE = {Automatic bird song and syllable segmentation with an open-source deep-learning object detection method - a case study in the Collared Flycatcher (Ficedula albicollis)},
  VOLUME = {27},
  YEAR = {2019},
  URL = {http://dx.doi.org/10.2478/orhu-2019-0015},
  DOI = {10.2478/orhu-2019-0015},
  ISSN = {2061-9588}
}


@Inproceedings{Jancovic_2017,
  AUTHOR = {Jancovic, Peter and Kokuer, Munevver},
  BOOKTITLE = {2017 25th European Signal Processing Conference (EUSIPCO)},
  MONTH = {08},
  NUMBER = {},
  PAGES = {1779-1783},
  PUBLISHER = {IEEE},
  TITLE = {Automatic detection of bird species from audio field recordings using HMM-based modelling of frequency tracks},
  VOLUME = {},
  YEAR = {2017},
  URL = {http://dx.doi.org/10.23919/EUSIPCO.2017.8081515},
  KEYWORDS = {Hidden Markov models;Birds;Feature extraction;Training;Acoustics;Data models;bird species detection;field recording;hidden Markov model;HMM;score normalisation;cohort;outlier;vocalisation;element;unsupervised training;sinusoid detection;sinusoidal modelling;frequency track},
  DOI = {10.23919/eusipco.2017.8081515}
}


@Inproceedings{Ntalampiras_2020,
  AUTHOR = {Ntalampiras, Stavros and Pezzuolo, Andrea and Mattiello, Silvana and Battini, Monica and Brscic, Marta},
  BOOKTITLE = {2020 43rd International Conference on Telecommunications and Signal Processing (TSP)},
  MONTH = {07},
  NUMBER = {},
  PAGES = {41-45},
  PUBLISHER = {IEEE},
  TITLE = {Automatic detection of cow/calf vocalizations in free-stall barn},
  VOLUME = {},
  YEAR = {2020},
  URL = {http://dx.doi.org/10.1109/TSP49548.2020.9163522},
  KEYWORDS = {Ethics;Animals;Signal processing algorithms;Hidden Markov models;Production;Agriculture;Acoustics;Precision livestock farming;cow/calf vocalization;acoustic signal processing;vocalization detection;audio pattern recognition},
  DOI = {10.1109/tsp49548.2020.9163522}
}


@Article{Tani_2013,
  AUTHOR = {Tani, Yukinori and Yokota, Yasunari and Yayota, Masato and Ohtani, Shigeru},
  JOURNAL = {Computers and Electronics in Agriculture},
  MONTH = {03},
  PAGES = {54-65},
  PUBLISHER = {Elsevier BV},
  TITLE = {Automatic recognition and classification of cattle chewing activity by an acoustic monitoring method with a single-axis acceleration sensor},
  VOLUME = {},
  YEAR = {2013},
  URL = {http://dx.doi.org/10.1016/j.compag.2013.01.001},
  DOI = {10.1016/j.compag.2013.01.001},
  ISSN = {0168-1699}
}


@Article{Mouy_2009,
  AUTHOR = {Mouy, Xavier and Bahoura, Mohammed and Simard, Yvan},
  JOURNAL = {The Journal of the Acoustical Society of America},
  MONTH = {12},
  NUMBER = {},
  PAGES = {2918-2928},
  PUBLISHER = {Acoustical Society of America (ASA)},
  TITLE = {Automatic recognition of fin and blue whale calls for real-time monitoring in the St. Lawrence},
  VOLUME = {},
  YEAR = {2009},
  URL = {http://dx.doi.org/10.1121/1.3257588},
  DOI = {10.1121/1.3257588},
  ISSN = {1520-8524}
}


@Article{Mac_Aodha_2018,
  AUTHOR = {Mac Aodha, Oisin and Gibb, Rory and Barlow, Kate E. and Browning, Ella and Firman, Michael and Freeman, Robin and Harder, Briana and Kinsey, Libby and Mead, Gary R. and Newson, Stuart E. and Pandourski, Ivan and Parsons, Stuart and Russ, Jon and Szodoray-Paradi, Abigel and Szodoray-Paradi, Farkas and Tilova, Elena and Girolami, Mark and Brostow, Gabriel and Jones, Kate E.},
  EDITOR = {Fenton, Brock},
  JOURNAL = {PLOS Computational Biology},
  MONTH = {03},
  NUMBER = {},
  PAGES = {e1005995},
  PUBLISHER = {Public Library of Science (PLoS)},
  TITLE = {Bat detective—Deep learning tools for bat acoustic signal detection},
  VOLUME = {},
  YEAR = {2018},
  URL = {http://dx.doi.org/10.1371/journal.pcbi.1005995},
  DOI = {10.1371/journal.pcbi.1005995},
  ISSN = {1553-7358}
}


@Inproceedings{10617409,
  AUTHOR = {S, Salini and K, Suresh},
  BOOKTITLE = {2024 International Conference on Advancements in Power, Communication and Intelligent Systems (APCI)},
  NUMBER = {},
  PAGES = {1--5},
  TITLE = {BAT-CNN: BirdNet Assisted Training for CNN},
  VOLUME = {},
  YEAR = {2024},
  KEYWORDS = {Training;Deep learning;Accuracy;Biological system modeling;Sociology;Birds;Audio recording;BAT-CNN;BirdNet;Bird Sound Classification;MelSpectrogram;Xeno-Canto},
  DOI = {10.1109/APCI61480.2024.10617409}
}


@Article{Clink2024.08.17.608420,
  AUTHOR = {Clink, Dena J. and Cross-Jaya, Hope and Kim, Jinsung and Ahmad, Abdul Hamid and Hong, Moeurk and Sala, Roeun and Birot, H{\'e}l{\`e}ne and Agger, Cain and Vu, Thinh Tien and Thi, Hoa Nguyen and Chi, Thanh Nguyen and Klinck, Holger},
  JOURNAL = {bioRxiv},
  PUBLISHER = {Cold Spring Harbor Laboratory},
  TITLE = {Benchmarking automated detection and classification approaches for monitoring of endangered species: a case study on gibbons from Cambodia},
  YEAR = {2024},
  URL = {https://www.biorxiv.org/content/early/2024/08/22/2024.08.17.608420},
  DOI = {10.1101/2024.08.17.608420},
  ABSTRACT = {Recent advances in deep and transfer learning have revolutionized our ability for the automated detection and classification of acoustic signals from long-term recordings. Here, we provide a benchmark for the automated detection of southern yellow-cheeked crested gibbon (Nomascus gabriellae) calls collected using autonomous recording units (ARUs) in Andoung Kraleung Village, Cambodia. We compared the performance of support vector machines (SVMs), a quasi-DenseNet architecture (Koogu), transfer learning with pretrained convolutional neural network (ResNet50) models trained on the {\textquoteleft}ImageNet{\textquoteright} dataset, and transfer learning with embeddings from a global birdsong model (BirdNET) based on an EfficientNet architecture. We also investigated the impact of varying the number of training samples on the performance of these models. We found that BirdNET had superior performance with a smaller number of training samples, whereas Koogu and ResNet50 models only had acceptable performance with a larger number of training samples (\&gt;200 gibbon samples). Effective automated detection approaches are critical for monitoring endangered species, like gibbons. It is unclear how generalizable these results are for other signals, and future work on other vocal species will be informative. Code and data are publicly available for future benchmarking.Competing Interest StatementThe authors have declared no competing interest.},
  EPRINT = {https://www.biorxiv.org/content/early/2024/08/22/2024.08.17.608420.full.pdf},
  ELOCATION.ID = {2024.08.17.608420}
}


@Article{Enari_2023,
  AUTHOR = {Enari, Hiroto and Enari, Haruka S.},
  JOURNAL = {American Journal of Primatology},
  MONTH = {10},
  NUMBER = {},
  PUBLISHER = {Wiley},
  TITLE = {Bioacoustic monitoring to determine addiction levels of primates to the human sphere: A feasibility study on Japanese macaques},
  VOLUME = {},
  YEAR = {2023},
  URL = {http://dx.doi.org/10.1002/ajp.23558},
  DOI = {10.1002/ajp.23558},
  ISSN = {1098-2345}
}


@Article{Akamatsu_2005,
  AUTHOR = {Akamatsu, Tomonari and Wang, Ding and Wang, Kexiong and Naito, Yasuhiko},
  JOURNAL = {Proceedings of the Royal Society B: Biological Sciences},
  MONTH = {04},
  NUMBER = {},
  PAGES = {797-801},
  PUBLISHER = {The Royal Society},
  TITLE = {Biosonar behaviour of free-ranging porpoises},
  VOLUME = {272},
  YEAR = {2005},
  URL = {http://dx.doi.org/10.1098/rspb.2004.3024},
  DOI = {10.1098/rspb.2004.3024},
  ISSN = {1471-2954}
}


@Article{van_Merri_nboer_2024,
  AUTHOR = {van Merriënboer, Bart and Hamer, Jenny and Dumoulin, Vincent and Triantafillou, Eleni and Denton, Tom},
  JOURNAL = {Frontiers in Bird Science},
  MONTH = {07},
  PUBLISHER = {Frontiers Media SA},
  TITLE = {Birds, bats and beyond: evaluating generalization in bioacoustics models},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.3389/fbirs.2024.1369756},
  DOI = {10.3389/fbirs.2024.1369756},
  ISSN = {2813-3870}
}


@Article{Vieira2015,
  AUTHOR = {Vieira, Manuel and Fonseca, Paulo J. and Amorim, M. Clara P. and Teixeira, Carlos J. C.},
  JOURNAL = {The Journal of the Acoustical Society of America},
  MONTH = {},
  NUMBER = {},
  PAGES = {3941--3950},
  PUBLISHER = {Acoustical Society of America (ASA)},
  TITLE = {Call recognition and individual identification of fish vocalizations based on automatic speech recognition: An example with the Lusitanian toadfish},
  VOLUME = {},
  YEAR = {2015},
  URL = {http://dx.doi.org/10.1121/1.4936858},
  DOI = {10.1121/1.4936858},
  ISSN = {1520-8524}
}


@Article{Guo_2024,
  AUTHOR = {Guo, Huimin and Jian, Haifang and Wang, Yiyu and Wang, Hongchang and Zheng, Shuaikang and Cheng, Qinghua and Li, Yuehao},
  JOURNAL = {Applied Intelligence},
  MONTH = {02},
  NUMBER = {},
  PAGES = {3152-3168},
  PUBLISHER = {Springer Science and Business Media LLC},
  TITLE = {CDPNet: conformer-based dual path joint modeling network for bird sound recognition},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1007/s10489-024-05362-9},
  DOI = {10.1007/s10489-024-05362-9},
  ISSN = {1573-7497}
}


@Article{Sun_2022,
  AUTHOR = {Sun, Yuren and Midori Maeda, Tatiana and Solís-Lemus, Claudia and Pimentel-Alarcón, Daniel and Buřivalová, Zuzana},
  JOURNAL = {Ecological Indicators},
  MONTH = {12},
  PAGES = {},
  PUBLISHER = {Elsevier BV},
  TITLE = {Classification of animal sounds in a hyperdiverse rainforest using convolutional neural networks with data augmentation},
  VOLUME = {},
  YEAR = {2022},
  URL = {http://dx.doi.org/10.1016/j.ecolind.2022.109621},
  DOI = {10.1016/j.ecolind.2022.109621},
  ISSN = {1470-160X}
}


@Inproceedings{8203533,
  AUTHOR = {Lin, Tzu-Hao and Tsao, Yu and Wang, Yu-Huang and Yen, Han-Wei and Lu, Sheng-Shan},
  BOOKTITLE = {2017 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC)},
  NUMBER = {},
  PAGES = {128--133},
  TITLE = {Computing biodiversity change via a soundscape monitoring network},
  VOLUME = {},
  YEAR = {2017},
  KEYWORDS = {Biodiversity;Monitoring;Animals;Transient analysis;Large Hadron Collider;Acoustics;Biodiversity assessment;soundscape;biological sounds;machine learning;open data},
  DOI = {10.23919/PNC.2017.8203533}
}


@Article{jimaging8040096,
  AUTHOR = {Trapanotto, Martino and Nanni, Loris and Brahnam, Sheryl and Guo, Xiang},
  JOURNAL = {Journal of Imaging},
  NUMBER = {},
  TITLE = {Convolutional Neural Networks for the Identification of African Lions from Individual Vocalizations},
  VOLUME = {},
  YEAR = {2022},
  URL = {https://www.mdpi.com/2313-433X/8/4/96},
  DOI = {10.3390/jimaging8040096},
  ISSN = {2313-433X},
  ABSTRACT = {The classification of vocal individuality for passive acoustic monitoring (PAM) and census of animals is becoming an increasingly popular area of research. Nearly all studies in this field of inquiry have relied on classic audio representations and classifiers, such as Support Vector Machines (SVMs) trained on spectrograms or Mel-Frequency Cepstral Coefficients (MFCCs). In contrast, most current bioacoustic species classification exploits the power of deep learners and more cutting-edge audio representations. A significant reason for avoiding deep learning in vocal identity classification is the tiny sample size in the collections of labeled individual vocalizations. As is well known, deep learners require large datasets to avoid overfitting. One way to handle small datasets with deep learning methods is to use transfer learning. In this work, we evaluate the performance of three pretrained CNNs (VGG16, ResNet50, and AlexNet) on a small, publicly available lion roar dataset containing approximately 150 samples taken from five male lions. Each of these networks is retrained on eight representations of the samples: MFCCs, spectrogram, and Mel spectrogram, along with several new ones, such as VGGish and stockwell, and those based on the recently proposed LM spectrogram. The performance of these networks, both individually and in ensembles, is analyzed and corroborated using the Equal Error Rate and shown to surpass previous classification attempts on this dataset; the best single network achieved over 95% accuracy and the best ensembles over 98% accuracy. The contributions this study makes to the field of individual vocal classification include demonstrating that it is valuable and possible, with caution, to use transfer learning with single pretrained CNNs on the small datasets available for this problem domain. We also make a contribution to bioacoustics generally by offering a comparison of the performance of many state-of-the-art audio representations, including for the first time the LM spectrogram and stockwell representations. All source code for this study is available on GitHub.},
  ARTICLE.NUMBER = {},
  PUBMEDID = {}
}


@Article{Schall_2024,
  AUTHOR = {Schall, Elena and Kaya, Idil Ilgaz and Debusschere, Elisabeth and Devos, Paul and Parcerisas, Clea},
  JOURNAL = {Remote Sensing in Ecology and Conservation},
  MONTH = {04},
  NUMBER = {},
  PAGES = {642-654},
  PUBLISHER = {Wiley},
  TITLE = {Deep learning in marine bioacoustics: a benchmark for baleen whale detection},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1002/rse2.392},
  DOI = {10.1002/rse2.392},
  ISSN = {2056-3485}
}


@Article{Shiu2020,
  AUTHOR = {Shiu, Yu and Palmer, K. J. and Roch, Marie A. and Fleishman, Erica and Liu, Xiaobai and Nosal, Eva-Marie and Helble, Tyler and Cholewiak, Danielle and Gillespie, Douglas and Klinck, Holger},
  JOURNAL = {Scientific Reports},
  MONTH = {01},
  NUMBER = {},
  PUBLISHER = {Springer Science and Business Media LLC},
  TITLE = {Deep neural networks for automated detection of marine mammal species},
  VOLUME = {},
  YEAR = {2020},
  URL = {http://dx.doi.org/10.1038/s41598-020-57549-y},
  DOI = {10.1038/s41598-020-57549-y},
  ISSN = {2045-2322}
}


@Techreport{nerc533486,
  ADDRESS = {Wallingford, UK},
  AUTHOR = {D.B. Roy and C. Abrahams and T. August and J. Christelow and F. Gerard and K. Howell and M. Logie and M. McCracken and D. Pallett and M. Pocock and D.S. Read and D. Sadykova and J.T. Staley},
  MONTH = {01},
  NOTE = {Freely available via Official URL link.},
  PUBLISHER = {UK Centre for Ecology \& Hydrology},
  TITLE = {Developing technologies for Agri-environment monitoring},
  TYPE = {Project Report},
  YEAR = {2022},
  URL = {http://nora.nerc.ac.uk/id/eprint/533486/},
  ABSTRACT = {}
}


@Article{Bartrina_2024,
  AUTHOR = {Bartrina, Carme and Llanos-Guerrero, César and Valls, Núria and Freixas, Lídia and López-Baucells, Adrià},
  JOURNAL = {Bioacoustics},
  MONTH = {07},
  NUMBER = {},
  PAGES = {332-353},
  PUBLISHER = {Informa UK Limited},
  TITLE = {Diversity and plasticity of vocalisations in an elusive and arboreal small mammal: the edible dormouse ( Glis glis )},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1080/09524622.2024.2353651},
  DOI = {10.1080/09524622.2024.2353651},
  ISSN = {2165-0586}
}


@Article{Elie_2011,
  AUTHOR = {Elie, Julie E. and Soula, Hédi A. and Mathevon, Nicolas and Vignal, Clémentine},
  JOURNAL = {The Journal of the Acoustical Society of America},
  MONTH = {06},
  NUMBER = {},
  PAGES = {4037-4046},
  PUBLISHER = {Acoustical Society of America (ASA)},
  TITLE = {Dynamics of communal vocalizations in a social songbird, the zebra finch (Taeniopygia guttata)},
  VOLUME = {},
  YEAR = {2011},
  URL = {http://dx.doi.org/10.1121/1.3570959},
  DOI = {10.1121/1.3570959},
  ISSN = {1520-8524}
}


@Article{Sanchez_2017,
  AUTHOR = {Sanchez, Lida and Moreno, Christian R. and Mora, Emanuel C.},
  EDITOR = {Burda, Hynek},
  JOURNAL = {Cogent Biology},
  MONTH = {01},
  NUMBER = {},
  PAGES = {},
  PUBLISHER = {Informa UK Limited},
  TITLE = {Echolocation calls ofNatalus primus(Chiroptera: Natalidae): Implications for conservation monitoring of this species},
  VOLUME = {},
  YEAR = {2017},
  URL = {http://dx.doi.org/10.1080/23312025.2017.1355027},
  DOI = {10.1080/23312025.2017.1355027},
  ISSN = {2331-2025}
}


@Article{Farina2022,
  AUTHOR = {Farina, Almo and Eldridge, Alice and Fuller, Susan and Pavan, Gianni},
  JOURNAL = {Frontiers in Ecology and Evolution},
  MONTH = {07},
  PUBLISHER = {Frontiers Media SA},
  TITLE = {Editorial: Advances in ecoacoustics},
  VOLUME = {},
  YEAR = {2022},
  URL = {http://dx.doi.org/10.3389/fevo.2022.978516},
  DOI = {10.3389/fevo.2022.978516},
  ISSN = {2296-701X}
}


@Inproceedings{Solomes_2020,
  AUTHOR = {Solomes, Alexandru-Marius and Stowell, Dan},
  BOOKTITLE = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  MONTH = {05},
  PAGES = {746-750},
  PUBLISHER = {IEEE},
  TITLE = {Efficient Bird Sound Detection on the Bela Embedded System},
  YEAR = {2020},
  URL = {http://dx.doi.org/10.1109/ICASSP40776.2020.9053533},
  DOI = {10.1109/icassp40776.2020.9053533}
}


@Article{Sheard_2024,
  AUTHOR = {Sheard, Julie Koch and Adriaens, Tim and Bowler, Diana E. and Büermann, Andrea and Callaghan, Corey T. and Camprasse, Elodie C. M. and Chowdhury, Shawan and Engel, Thore and Finch, Elizabeth A. and von Gönner, Julia and Hsing, Pen-Yuan and Mikula, Peter and Rachel Oh, Rui Ying and Peters, Birte and Phartyal, Shyam S. and Pocock, Michael J. O. and Wäldchen, Jana and Bonn, Aletta},
  JOURNAL = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  MONTH = {05},
  NUMBER = {},
  PUBLISHER = {The Royal Society},
  TITLE = {Emerging technologies in citizen science and potential for insect monitoring},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1098/rstb.2023.0106},
  DOI = {10.1098/rstb.2023.0106},
  ISSN = {1471-2970}
}


@Techreport{Copping2012,
  ADDRESS = {Richland, WA},
  AUTHOR = {Copping, Andrea E. and Hanna, Luke A. and Butner, R. Scott and Carlson, Thomas J. and Halvorsen, Michele B. and Duberstein, Corey A. and Matzner, Shari},
  INSTITUTION = {Pacific Northwest National Laboratory},
  NUMBER = {PNNL-21852},
  TITLE = {Environmental Effects of Offshore Wind Development. Fiscal Year 2012 Progress Report},
  YEAR = {2012},
  URL = {https://www.pnnl.gov/publications/environmental-effects-offshore-wind-development-fiscal-year-2012-progress-report}
}


@Article{Marques_2012,
  AUTHOR = {Marques, Tiago A. and Thomas, Len and Martin, Stephen W. and Mellinger, David K. and Ward, Jessica A. and Moretti, David J. and Harris, Danielle and Tyack, Peter L.},
  JOURNAL = {Biological Reviews},
  MONTH = {11},
  NUMBER = {2},
  PAGES = {287-309},
  PUBLISHER = {Wiley},
  TITLE = {Estimating animal population density using passive acoustics},
  VOLUME = {},
  YEAR = {2012},
  URL = {http://dx.doi.org/10.1111/brv.12001},
  DOI = {10.1111/brv.12001},
  ISSN = {1469-185X}
}


@Article{Hutschenreiter_2023,
  AUTHOR = {Hutschenreiter, Anja and Sosa-López, J. Roberto and González-García, Fernando and Aureli, Filippo},
  JOURNAL = {Bioacoustics},
  MONTH = {08},
  NUMBER = {},
  PAGES = {660-678},
  PUBLISHER = {Informa UK Limited},
  TITLE = {Evaluating factors affecting species detection using passive acoustic monitoring in neotropical forests: a playback experiment},
  VOLUME = {},
  YEAR = {2023},
  URL = {http://dx.doi.org/10.1080/09524622.2023.2246413},
  DOI = {10.1080/09524622.2023.2246413},
  ISSN = {2165-0586}
}


@Article{Jordan2024,
  AUTHOR = {Jordan, Celine and Markolf, Matthias},
  JOURNAL = {Madagascar Conservation &amp; Development},
  MONTH = {01},
  NUMBER = {},
  PAGES = {39--47},
  PUBLISHER = {African Journals Online (AJOL)},
  TITLE = {Exploring the potential of occupancy modelling using passive acoustics in &lt;i&gt;Coua gigas&lt;/i&gt; and &lt;i&gt;Coua coquereli&lt;/i&gt;},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.4314/mcd.v18i1.6},
  DOI = {10.4314/mcd.v18i1.6},
  ISSN = {1662-2510}
}


@Article{Marck_2022,
  AUTHOR = {Marck, Aya and Vortman, Yoni and Kolodny, Oren and Lavner, Yizhar},
  JOURNAL = {Frontiers in Behavioral Neuroscience},
  MONTH = {02},
  PUBLISHER = {Frontiers Media SA},
  TITLE = {Identification, Analysis and Characterization of Base Units of Bird Vocal Communication: The White Spectacled Bulbul (Pycnonotus xanthopygos) as a Case Study},
  VOLUME = {},
  YEAR = {2022},
  URL = {http://dx.doi.org/10.3389/fnbeh.2021.812939},
  DOI = {10.3389/fnbeh.2021.812939},
  ISSN = {1662-5153}
}


@Article{Madhusudhana_2021,
  AUTHOR = {Madhusudhana, Shyam and Shiu, Yu and Klinck, Holger and Fleishman, Erica and Liu, Xiaobai and Nosal, Eva-Marie and Helble, Tyler and Cholewiak, Danielle and Gillespie, Douglas and Širović, Ana and Roch, Marie A.},
  JOURNAL = {Journal of The Royal Society Interface},
  MONTH = {07},
  NUMBER = {},
  PAGES = {20210297},
  PUBLISHER = {The Royal Society},
  TITLE = {Improve automatic detection of animal call sequences with temporal context},
  VOLUME = {},
  YEAR = {2021},
  URL = {http://dx.doi.org/10.1098/rsif.2021.0297},
  DOI = {10.1098/rsif.2021.0297},
  ISSN = {1742-5662}
}


@Article{JEANTET2023102256,
  AUTHOR = {Lorène Jeantet and Emmanuel Dufourq},
  JOURNAL = {Ecological Informatics},
  PAGES = {},
  TITLE = {Improving deep learning acoustic classifiers with contextual information for wildlife monitoring},
  VOLUME = {},
  YEAR = {2023},
  URL = {https://www.sciencedirect.com/science/article/pii/S1574954123002856},
  KEYWORDS = {Bioacoustics, Deep learning, Convolutional neural networks, Passive acoustic monitoring, Species identification, Birds, Hainan gibbons},
  DOI = {https://doi.org/10.1016/j.ecoinf.2023.102256},
  ISSN = {1574-9541},
  ABSTRACT = {Bioacoustics, the exploration of animal vocalizations and natural soundscapes, has emerged as a valuable tool for studying species within their habitats, particularly those that are challenging to observe. This approach has broadened the horizons of biodiversity assessment and ecological research. However, monitoring wildlife with acoustic recorders produces large volumes of data that can be labor-intensive to analyze. Deep learning has recently transformed many computational disciplines by enabling the automated processing of large and complex datasets and has gained attention within the bioacoustics community. Despite the revolutionary impact of deep learning on acoustic detection and classification, attaining both high detection accuracy and low false positive rates in bioacoustics remains a significant challenge. An intriguing yet unexplored avenue for enhancing deep learning in bioacoustics involves the utilization of contextual information, such as time and location, to discern animal vocalizations within acoustic recordings. As a first case study, a multi-branch Convolutional Neural Network (CNN) was developed to classify 22 different bird songs using spectrograms as a first input, and spatial metadata as a secondary input. A comparison was made to a baseline model with only spectrogram input. A geographical prior neural network was trained, separately, to estimate the probability of a species occurring at a given location. The output of this network was combined with the baseline CNN. As a second case study, temporal data and spectrograms were used as input to a multi-branch CNN for the detection of Hainan gibbon (Nomascus hainanus) calls, the world’s rarest primate. Our findings demonstrate that adding metadata to the bird song classifier significantly improves classification performance, with the highest improvement achieved using the geographical prior model (F1-score of 87.78% compared to 61.02% for the baseline model). The multi-branch CNNs also proved efficient (F1-scores of 76.87% and 78.77%) and simpler to use than the geographical prior. In the second case study, our findings revealed a decrease in false positives by 63% (94% of the calls were detected) when the metadata was used by the multi-branch CNN, and an increase of 19% in gibbon detection. This study has uncovered an exciting new avenue for improving classifier performance in bioacoustics. The methodology described in this study can assist ecologists, wildlife management teams, and researchers in reducing the amount of time spent analyzing large acoustic datasets obtained from passive acoustic monitoring studies. Our approach can be adapted and applied to other calling species, and thus tailored to other use cases.}
}


@Article{Arnaud2022.06.26.497684,
  AUTHOR = {Arnaud, Vincent and Pellegrino, Fran{\c c}ois and Keenan, Sumir and St-Gelais, Xavier and Mathevon, Nicolas and Levr{\'e}ro, Florence and Coup{\'e}, Christophe},
  JOURNAL = {bioRxiv},
  PUBLISHER = {Cold Spring Harbor Laboratory},
  TITLE = {Improving the workflow to crack Small, Unbalanced, Noisy, but Genuine (SUNG) datasets in bioacoustics: the case of bonobo calls},
  YEAR = {2022},
  URL = {https://www.biorxiv.org/content/early/2022/06/29/2022.06.26.497684},
  DOI = {10.1101/2022.06.26.497684},
  ABSTRACT = {Despite the accumulation of data and studies, deciphering animal vocal communication remains highly challenging. While progress has been made with some species for which we now understand the information exchanged through vocal signals, researchers are still left struggling with sparse recordings composing Small, Unbalanced, Noisy, but Genuine (SUNG) datasets. SUNG datasets offer a valuable but distorted vision of communication systems. Adopting the best practices in their analysis is therefore essential to effectively extract the available information and draw reliable conclusions. Here we show that the most recent advances in machine learning applied to a SUNG dataset succeed in unraveling the complex vocal repertoire of the bonobo, and we propose a workflow that can be effective with other animal species. We implement acoustic parameterization in three feature spaces along with three classification algorithms (Support Vector Machine, xgboost, neural networks) and their combination to explore the structure and variability of bonobo calls, as well as the robustness of the individual signature they encode. We underscore how classification performance is affected by the feature set and identify the most informative features. We highlight the need to address data leakage in the evaluation of classification performance to avoid misleading interpretations. Finally, using a Uniform Manifold Approximation and Projection (UMAP), we show that classifiers generate parsimonious data descriptions which help to understand the clustering of the bonobo acoustic space. Our results lead to identifying several practical approaches that are generalizable to any other animal communication system. To improve the reliability and replicability of vocal communication studies with SUNG datasets, we thus recommend: i) comparing several acoustic parameterizations; ii) adopting Support Vector Machines as the baseline classification approach; iii) explicitly evaluating data leakage and possibly implementing a mitigation strategy; iv) visualizing the dataset with UMAPs applied to classifier predictions rather than to raw acoustic features.Competing Interest StatementThe authors have declared no competing interest.},
  EPRINT = {https://www.biorxiv.org/content/early/2022/06/29/2022.06.26.497684.full.pdf},
  ELOCATION.ID = {2022.06.26.497684}
}


@Article{make6040115,
  AUTHOR = {Garcia, Tiago and Pina, Luís and Robb, Magnus and Maria, Jorge and May, Roel and Oliveira, Ricardo},
  JOURNAL = {Machine Learning and Knowledge Extraction},
  NUMBER = {},
  PAGES = {2336--2354},
  TITLE = {Long-Range Bird Species Identification Using Directional Microphones and CNNs},
  VOLUME = {},
  YEAR = {2024},
  URL = {https://www.mdpi.com/2504-4990/6/4/115},
  DOI = {10.3390/make6040115},
  ISSN = {2504-4990},
  ABSTRACT = {This study explores the integration of directional microphones with convolutional neural networks (CNNs) for long-range bird species identification. By employing directional microphones, we aimed to capture high-resolution audio from specific directions, potentially improving the clarity of bird calls over extended distances. Our approach involved processing these recordings with CNNs trained on a diverse dataset of bird calls. The results demonstrated that the system is capable of systematically identifying bird species up to 150 m, reaching 280 m for species vocalizing at frequencies greater than 1000 Hz and clearly distinct from background noise. The furthest successful detection was obtained at 510 m. While the method showed promise in enhancing the identification process compared to traditional techniques, there were notable limitations in the clarity of the audio recordings. These findings suggest that while the integration of directional microphones and CNNs for long-range bird species identification is promising, further refinement is needed to fully realize the benefits of this approach. Future efforts should focus on improving the audio-capture technology to reduce ambient noise and enhance the system’s overall performance in long-range bird species identification.}
}


@Article{Caruso2017,
  AUTHOR = {Caruso, Francesco and Alonge, Giuseppe and Bellia, Giorgio and De Domenico, Emilio and Grammauta, Rosario and Larosa, Giuseppina and Mazzola, Salvatore and Riccobene, Giorgio and Pavan, Gianni and Papale, Elena and Pellegrino, Carmelo and Pulvirenti, Sara and Sciacca, Virginia and Simeone, Francesco and Speziale, Fabrizio and Viola, Salvatore and Buscaino, Giuseppa},
  JOURNAL = {Scientific Reports},
  MONTH = {06},
  NUMBER = {},
  PUBLISHER = {Springer Science and Business Media LLC},
  TITLE = {Long-Term Monitoring of Dolphin Biosonar Activity in Deep Pelagic Waters of the Mediterranean Sea},
  VOLUME = {},
  YEAR = {2017},
  URL = {http://dx.doi.org/10.1038/s41598-017-04608-6},
  DOI = {10.1038/s41598-017-04608-6},
  ISSN = {2045-2322}
}


@Article{Munger2022,
  AUTHOR = {Munger, JE and Herrera, DP and Haver, SM and Waterhouse, L and McKenna, MF and Dziak, RP and Gedamke, J and Heppell, SA and Haxel, JH},
  JOURNAL = {Marine Ecology Progress Series},
  MONTH = {01},
  PAGES = {197--210},
  PUBLISHER = {Inter-Research Science Center},
  TITLE = {Machine learning analysis reveals relationship between pomacentrid calls and environmental cues},
  VOLUME = {},
  YEAR = {2022},
  URL = {http://dx.doi.org/10.3354/meps13912},
  DOI = {10.3354/meps13912},
  ISSN = {1616-1599}
}


@Article{ZHOU2023110908,
  AUTHOR = {Xiaotao Zhou and Kunrong Hu and Zhenhua Guan and Chunjiang Yu and Shuai Wang and Meng Fan and Yongke Sun and Yong Cao and Yijie Wang and Guangting Miao},
  JOURNAL = {Ecological Indicators},
  PAGES = {},
  TITLE = {Methods for processing and analyzing passive acoustic monitoring data: An example of song recognition in western black-crested gibbons},
  VOLUME = {},
  YEAR = {2023},
  URL = {https://www.sciencedirect.com/science/article/pii/S1470160X23010506},
  KEYWORDS = {Western black-crested gibbons song recognition, Passive acoustic monitoring, Bioacoustics, Deep learning, Attentional mechanisms},
  DOI = {https://doi.org/10.1016/j.ecolind.2023.110908},
  ISSN = {1470-160X},
  ABSTRACT = {The extraction of species-specific calls from passive acoustic recordings is a common preliminary step in ecological analysis. But for many species, especially those occupying noisy, acoustically variable habitats, the call extraction process remains largely manual, a time-consuming, and increasingly unsustainable process. Deep neural networks have been shown to provide excellent performance in a range of acoustic classification applications. We take as an example the recognition of four songs of one of the rarest mammals in the world, the western black-crested gibbon (Nomascus concolors).The process of recognition in this paper which includes distributed BIC ambient sound segmentation based on the OpenPAI platform; DNN-based western black-crested gibbon song enhancement processing; data pre-processing, labeling samples; proposed DNN + ResNet34 + CBAM + GRU + Attention recognition model; comparing other classical neural network models.Our best model converts acoustic recordings into spectrogram images on the mel frequency scale and uses these images to train convolutional neural networks.Our proposed model proved to be very accurate in predicting the segmented western black-crested gibbon songs with an accuracy of 99.8%, and almost a few western black-crested gibbon songs were incorrectly identified when all segmented data were recognized. In the four consecutive years of the acoustic monitoring system deployed in the Ailao Mountain National Nature Reserve, the western black-crested gibbon was most active near the monitoring site from March to August each year, and least active in January and February. Based on call sound intensity analysis, we monitored a total of two different western black-crested gibbon groups (G1 and G2) during the monitoring cycle. We demonstrate that passive acoustic monitoring combined with CNN classifiers is an effective tool for the remote detection of one of the rarest and most threatened species in the world.}
}


@Article{Risch2013,
  AUTHOR = {Risch, D and Clark, CW and Dugan, PJ and Popescu, M and Siebert, U and Van Parijs, SM},
  JOURNAL = {Marine Ecology Progress Series},
  MONTH = {08},
  PAGES = {279-295},
  PUBLISHER = {Inter-Research Science Center},
  TITLE = {Minke whale acoustic behavior and multi-year seasonal and diel vocalization patterns in Massachusetts Bay, USA},
  VOLUME = {},
  YEAR = {2013},
  URL = {http://dx.doi.org/10.3354/meps10426},
  DOI = {10.3354/meps10426},
  ISSN = {1616-1599}
}


@Article{Caruso_2020,
  AUTHOR = {Caruso, Francesco and Dong, Lijun and Lin, Mingli and Liu, Mingming and Gong, Zining and Xu, Wanxue and Alonge, Giuseppe and Li, Songhai},
  JOURNAL = {Frontiers in Marine Science},
  MONTH = {04},
  PUBLISHER = {Frontiers Media SA},
  TITLE = {Monitoring of a Nearshore Small Dolphin Species Using Passive Acoustic Platforms and Supervised Machine Learning Techniques},
  VOLUME = {},
  YEAR = {2020},
  URL = {http://dx.doi.org/10.3389/fmars.2020.00267},
  DOI = {10.3389/fmars.2020.00267},
  ISSN = {2296-7745}
}


@Article{Varun_Prakash_2024,
  AUTHOR = {Varun Prakash, R. and Karthikeyan, V. and Vishali, S. and Karthika, M.},
  JOURNAL = {The Visual Computer},
  MONTH = {08},
  PUBLISHER = {Springer Science and Business Media LLC},
  TITLE = {Multi-level LSTM framework with hybrid sonic features for human-animal conflict evasion},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1007/s00371-024-03588-9},
  DOI = {10.1007/s00371-024-03588-9},
  ISSN = {1432-2315}
}


@Article{Symes_2024,
  AUTHOR = {Symes, Laurel B. and Madhusudhana, Shyam and Martinson, Sharon J. and Geipel, Inga and ter Hofstede, Hannah M.},
  JOURNAL = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  MONTH = {05},
  NUMBER = {},
  PUBLISHER = {The Royal Society},
  TITLE = {Multi-year soundscape recordings and automated call detection reveals varied impact of moonlight on calling activity of neotropical forest katydids},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1098/rstb.2023.0110},
  DOI = {10.1098/rstb.2023.0110},
  ISSN = {1471-2970}
}


@Article{Potamitis_2009,
  AUTHOR = {Potamitis, Ilyas and Ganchev, Todor and Kontodimas, Dimitris},
  JOURNAL = {Journal of Economic Entomology},
  MONTH = {08},
  NUMBER = {},
  PAGES = {1681-1690},
  PUBLISHER = {Oxford University Press (OUP)},
  TITLE = {On Automatic Bioacoustic Detection of Pests: The Cases of &lt;I&gt;Rhynchophorus ferrugineus&lt;/I&gt; and &lt;I&gt;Sitophilus oryzae&lt;/I&gt;},
  VOLUME = {},
  YEAR = {2009},
  URL = {http://dx.doi.org/10.1603/029.102.0436},
  DOI = {10.1603/029.102.0436},
  ISSN = {0022-0493}
}


@Article{RANKIN2024102511,
  AUTHOR = {Shannon Rankin and Taiki Sakai and Frederick I. Archer and Jay Barlow and Danielle Cholewiak and Annamaria I. DeAngelis and Jennifer L.K. McCullough and Erin M. Oleson and Anne E. Simonis and Melissa S. Soldevilla and Jennifer S. Trickey},
  JOURNAL = {Ecological Informatics},
  PAGES = {},
  TITLE = {Open-source machine learning BANTER acoustic classification of beaked whale echolocation pulses},
  VOLUME = {},
  YEAR = {2024},
  URL = {https://www.sciencedirect.com/science/article/pii/S1574954124000530},
  KEYWORDS = {Bioacoustics, Machine learning, Random forest, Species classification, Passive acoustic monitoring, Beaked whale},
  DOI = {https://doi.org/10.1016/j.ecoinf.2024.102511},
  ISSN = {1574-9541},
  ABSTRACT = {Passive acoustic monitoring is increasingly used for assessing populations of marine mammals; however, analysis of large datasets is limited by our ability to easily classify sounds detected. Classification of beaked whale acoustic events, in particular, requires evaluation of multiple lines of evidence by expert analysts. Here we present a highly automated approach to acoustic detection and classification using supervised machine learning and open source software methods. Data from four large scale surveys of beaked whales (northwestern North Atlantic, southwestern North Atlantic, Hawaii, and eastern North Pacific) were analyzed using PAMGuard (acoustic detection), PAMpal (acoustic analysis) and BANTER (hierarchical random forest classifier). Overall classification accuracy ranged from 88% for the southwestern North Atlantic data to 97% for the northwestern North Atlantic. Results for many species could likely be improved with increased sample sizes, consideration of alternative automated detectors, and addition of relevant environmental features. These methods provide a highly automated approach to acoustic detection and classification using open source methods that can be readily adopted for other species and geographic regions.}
}


@Article{Brinkl_v_2023,
  AUTHOR = {Brinkløv, Signe M. M. and Macaulay, Jamie and Bergler, Christian and Tougaard, Jakob and Beedholm, Kristian and Elmeros, Morten and Madsen, Peter Teglberg},
  JOURNAL = {Methods in Ecology and Evolution},
  MONTH = {05},
  NUMBER = {},
  PAGES = {1747-1763},
  PUBLISHER = {Wiley},
  TITLE = {Open-source workflow approaches to passive acoustic monitoring of bats},
  VOLUME = {},
  YEAR = {2023},
  URL = {http://dx.doi.org/10.1111/2041-210X.14131},
  DOI = {10.1111/2041-210x.14131},
  ISSN = {2041-210X}
}


@Article{Metcalf2021,
  AUTHOR = {Metcalf, Oliver C. and Barlow, Jos and Marsden, Stuart and Gomes de Moura, Nárgila and Berenguer, Erika and Ferreira, Joice and Lees, Alexander C.},
  EDITOR = {Pettorelli, Nathalie and Astaras, Christos},
  JOURNAL = {Remote Sensing in Ecology and Conservation},
  MONTH = {07},
  NUMBER = {},
  PAGES = {45--56},
  PUBLISHER = {Wiley},
  TITLE = {Optimizing tropical forest bird surveys using passive acoustic monitoring and high temporal resolution sampling},
  VOLUME = {},
  YEAR = {2021},
  URL = {http://dx.doi.org/10.1002/rse2.227},
  DOI = {10.1002/rse2.227},
  ISSN = {2056-3485}
}


@Article{10.1371/journal.pone.0229058,
  AUTHOR = {Gillespie, Douglas AND Palmer, Laura AND Macaulay, Jamie AND Sparling, Carol AND Hastie, Gordon},
  JOURNAL = {PLOS ONE},
  MONTH = {05},
  NUMBER = {},
  PAGES = {1--16},
  PUBLISHER = {Public Library of Science},
  TITLE = {Passive acoustic methods for tracking the 3D movements of small cetaceans around marine structures},
  VOLUME = {},
  YEAR = {2020},
  URL = {https://doi.org/10.1371/journal.pone.0229058},
  DOI = {10.1371/journal.pone.0229058},
  ABSTRACT = {A wide range of anthropogenic structures exist in the marine environment with the extent of these set to increase as the global offshore renewable energy industry grows. Many of these pose acute risks to marine wildlife; for example, tidal energy generators have the potential to injure or kill seals and small cetaceans through collisions with moving turbine parts. Information on fine scale behaviour of animals close to operational turbines is required to understand the likely impact of these new technologies. There are inherent challenges associated with measuring the underwater movements of marine animals which have, so far, limited data collection. Here, we describe the development and application of a system for monitoring the three-dimensional movements of cetaceans in the immediate vicinity of a subsea structure. The system comprises twelve hydrophones and software for the detection and localisation of vocal marine mammals. We present data demonstrating the systems practical performance during a deployment on an operational tidal turbine between October 2017 and October 2019. Three-dimensional locations of cetaceans were derived from the passive acoustic data using time of arrival differences on each hydrophone. Localisation accuracy was assessed with an artificial sound source at known locations and a refined method of error estimation is presented. Calibration trials show that the system can accurately localise sounds to 2m accuracy within 20m of the turbine but that localisations become highly inaccurate at distances greater than 35m. The system is currently being used to provide data on rates of encounters between cetaceans and the turbine and to provide high resolution tracking data for animals close to the turbine. These data can be used to inform stakeholders and regulators on the likely impact of tidal turbines on cetaceans.}
}


@Article{DUFOURQ2022101688,
  AUTHOR = {Emmanuel Dufourq and Carly Batist and Ruben Foquet and Ian Durbach},
  JOURNAL = {Ecological Informatics},
  PAGES = {},
  TITLE = {Passive acoustic monitoring of animal populations with transfer learning},
  VOLUME = {},
  YEAR = {2022},
  URL = {https://www.sciencedirect.com/science/article/pii/S1574954122001388},
  KEYWORDS = {Transfer learning, Convolutional neural networks, Deep learning, Vocalisation classification, Bioacoustics},
  DOI = {https://doi.org/10.1016/j.ecoinf.2022.101688},
  ISSN = {1574-9541},
  ABSTRACT = {Progress in deep learning, more specifically in using convolutional neural networks (CNNs) for the creation of classification models, has been tremendous in recent years. Within bioacoustics research, there has been a large number of recent studies that use CNNs. Designing CNN architectures from scratch is non-trivial and requires knowledge of machine learning. Furthermore, hyper-parameter tuning associated with CNNs is extremely time consuming and requires expensive hardware. In this paper we assess whether it is possible to build good bioacoustic classifiers by adapting and re-using existing CNNs pre-trained on the ImageNet dataset - instead of designing them from scratch, a strategy known as transfer learning that has proved highly successful in other domains. This study is a first attempt to conduct a large-scale investigation on how transfer learning can be used for passive acoustic monitoring (PAM), to simplify the implementation of CNNs and the design decisions when creating them, and to remove time consuming hyper-parameter tuning phases. We compare 12 modern CNN architectures across 4 passive acoustic datasets that target calls of the Hainan gibbon Nomascus hainanus, the critically endangered black-and-white ruffed lemur Varecia variegata, the vulnerable Thyolo alethe Chamaetylas choloensis, and the Pin-tailed whydah Vidua macroura. We focus our work on data scarcity issues by training PAM binary classification models very small datasets, with as few as 25 verified examples. Our findings reveal that transfer learning can result in up to 82% F1 score while keeping CNN implementation details to a minimum, thus rendering this approach accessible, easier to design, and speeding up further vocalisation annotations to create PAM robust models.}
}


@Inproceedings{Riera2012PatternsOS,
  AUTHOR = {Amalis Riera},
  TITLE = {Patterns of seasonal occurrence of sympatric killer whale lineages in waters off Southern Vancouver Island and Washington state, as determined by passive acoustic monitoring},
  YEAR = {2012},
  URL = {https://api.semanticscholar.org/CorpusID:89916138}
}


@Article{Ruff_2023,
  AUTHOR = {Ruff, Zachary J. and Lesmeister, Damon B. and Jenkins, Julianna M.A. and Sullivan, Christopher M.},
  JOURNAL = {SoftwareX},
  MONTH = {07},
  PAGES = {},
  PUBLISHER = {Elsevier BV},
  TITLE = {PNW-Cnet v4: Automated species identification for passive acoustic monitoring},
  VOLUME = {23},
  YEAR = {2023},
  URL = {http://dx.doi.org/10.1016/j.softx.2023.101473},
  DOI = {10.1016/j.softx.2023.101473},
  ISSN = {2352-7110}
}


@Techreport{Frouin-Mouy2016,
  AUTHOR = {Frouin-Mouy, H. and Yurk, H. and Mouy, X. and Martin, B.},
  INSTITUTION = {JASCO Applied Sciences},
  NUMBER = {Document 01129, Version 3.0},
  TITLE = {Prince Rupert - Aurora LNG Acoustic Monitoring Study: Chatham Sound Region},
  TYPE = {Technical Report},
  YEAR = {2016},
  URL = {https://projects.eao.gov.bc.ca/api/document/58923173b637cc02bea163f0/fetch/Appendix_O_Acoustic_Monitoring_Final_screening.pdf}
}


@Inproceedings{Chen_2021,
  AUTHOR = {Chen, Yankun and Wang, Weiping and Liang, Yinian and Zhou, Defu and Dong, Chao and Li, Jie},
  BOOKTITLE = {2021 OES China Ocean Acoustics (COA)},
  MONTH = {07},
  PAGES = {1027-1031},
  PUBLISHER = {IEEE},
  TITLE = {Real-time Detection and Classification for Targeted Marine Mammals},
  YEAR = {2021},
  URL = {http://dx.doi.org/10.1109/COA50123.2021.9519906},
  DOI = {10.1109/coa50123.2021.9519906}
}


@Article{Rivas2024.11.04.621915,
  AUTHOR = {Rivas, Francisco and Brizio, Cesare and Buzzetti, Filippo M. and Pijanowski, Bryan},
  JOURNAL = {bioRxiv},
  PUBLISHER = {Cold Spring Harbor Laboratory},
  TITLE = {Rthoptera: Standardised Insect Bioacoustics in R},
  YEAR = {2024},
  URL = {https://www.biorxiv.org/content/early/2024/11/08/2024.11.04.621915},
  DOI = {10.1101/2024.11.04.621915},
  ABSTRACT = {Crickets were the first study subjects at the dawn of bioacoustics, yet, almost 100 years later, freely available and robust analysis tools and protocols are still needed.We introduce Rthoptera, a new open-source R package for the analysis of insect acoustic signals, offering accurate graphics and standardised temporal and spectral measurements through a streamlined workflow. Our package delivers results in a fraction of the time typically required by multi-software methods. Most of the functions have interactive versions (Shiny applications), facilitating their adoption by researchers with any level of technical expertise.New acoustic metrics are introduced: Spectral Excursion, Pattern Complexity Index, Temporal Excursion, Dynamic Excursion, and Broadband Activity Index.Accompanied by an appropriate recording protocol, our tool can become the backbone of a standard analysis protocol for comparative insect bioacoustics.Competing Interest StatementThe authors have declared no competing interest.},
  EPRINT = {https://www.biorxiv.org/content/early/2024/11/08/2024.11.04.621915.full.pdf},
  ELOCATION.ID = {2024.11.04.621915}
}


@Article{10.1371/journal.pcbi.1008698,
  AUTHOR = {Lin, Tzu-Hao AND Akamatsu, Tomonari AND Tsao, Yu},
  JOURNAL = {PLOS Computational Biology},
  MONTH = {02},
  NUMBER = {2},
  PAGES = {1--23},
  PUBLISHER = {Public Library of Science},
  TITLE = {Sensing ecosystem dynamics via audio source separation: A case study of marine soundscapes off northeastern Taiwan},
  VOLUME = {},
  YEAR = {2021},
  URL = {https://doi.org/10.1371/journal.pcbi.1008698},
  DOI = {10.1371/journal.pcbi.1008698},
  ABSTRACT = {Remote acquisition of information on ecosystem dynamics is essential for conservation management, especially for the deep ocean. Soundscape offers unique opportunities to study the behavior of soniferous marine animals and their interactions with various noise-generating activities at a fine temporal resolution. However, the retrieval of soundscape information remains challenging owing to limitations in audio analysis techniques that are effective in the face of highly variable interfering sources. This study investigated the application of a seafloor acoustic observatory as a long-term platform for observing marine ecosystem dynamics through audio source separation. A source separation model based on the assumption of source-specific periodicity was used to factorize time-frequency representations of long-duration underwater recordings. With minimal supervision, the model learned to discriminate source-specific spectral features and prove to be effective in the separation of sounds made by cetaceans, soniferous fish, and abiotic sources from the deep-water soundscapes off northeastern Taiwan. Results revealed phenological differences among the sound sources and identified diurnal and seasonal interactions between cetaceans and soniferous fish. The application of clustering to source separation results generated a database featuring the diversity of soundscapes and revealed a compositional shift in clusters of cetacean vocalizations and fish choruses during diurnal and seasonal cycles. The source separation model enables the transformation of single-channel audio into multiple channels encoding the dynamics of biophony, geophony, and anthropophony, which are essential for characterizing the community of soniferous animals, quality of acoustic habitat, and their interactions. Our results demonstrated the application of source separation could facilitate acoustic diversity assessment, which is a crucial task in soundscape-based ecosystem monitoring. Future implementation of soundscape information retrieval in long-term marine observation networks will lead to the use of soundscapes as a new tool for conservation management in an increasingly noisy ocean.}
}


@Article{Monczak2019,
  AUTHOR = {Monczak, A and Mueller, C and Miller, ME and Ji, Y and Borgianini, SA and Montie, EW},
  JOURNAL = {Marine Ecology Progress Series},
  MONTH = {01},
  PAGES = {49--68},
  PUBLISHER = {Inter-Research Science Center},
  TITLE = {Sound patterns of snapping shrimp, fish, and dolphins in an estuarine soundscape of the southeastern USA},
  VOLUME = {},
  YEAR = {2019},
  URL = {http://dx.doi.org/10.3354/meps12813},
  DOI = {10.3354/meps12813},
  ISSN = {1616-1599}
}


@Misc{bressler2023soundbaydeeplearningframework,
  AUTHOR = {Noam Bressler and Michael Faran and Amit Galor and Michael Moshe Michelashvili and Tomer Nachshon and Noa Weiss},
  TITLE = {Soundbay: Deep Learning Framework for Marine Mammals and Bioacoustic Research},
  YEAR = {2023},
  URL = {https://arxiv.org/abs/2311.04343},
  EPRINT = {2311.04343},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.SD}
}


@Article{10.1121/1.2932958,
  AUTHOR = {Baumann, Simone and Hildebrand, John A. and Wiggins, Sean M. and Schnitzler, Hans-Ulrich},
  JOURNAL = {The Journal of the Acoustical Society of America},
  MONTH = {05},
  NUMBER = {5_Supplement},
  PAGES = {3099--3099},
  TITLE = {Species identification and measurement of activity in odontocete species of Palmyra Atoll by acoustic monitoring},
  VOLUME = {},
  YEAR = {2008},
  URL = {https://doi.org/10.1121/1.2932958},
  DOI = {10.1121/1.2932958},
  ISSN = {0001-4966},
  ABSTRACT = {Acoustic monitoring has been used to study odontocete presence at Palmyra Atoll, a remote island in the Northern Line Islands chain. Long-term recordings of high-frequency, broadband acoustic data have become possible with recent technological advances. A High-frequency Autonomous Recording Package (HARP) has been developed which samples at 200 kHz with a duty cycle of 1/4 for up to seven months. This instrument has recorded since October 2006 at Palmyra Atoll. Visual and acoustic surveys were conducted around Palmyra Atoll using a four-element towed hydrophone array sampling real-time at 200 kHz to obtain species-specific acoustic data. These data are used as reference for automatic detection algorithms applied on the long-term recordings. To date, acoustically and visually detected odontocetes include bottlenose dolphins (Tursiops truncatus), spinner dolphins (Stenella longirostris), melon-headed whales (Peponocephala electra) and beaked whales of the genus Mesoplodon. The long-term HARP data reveal acoustic activity primarily at night time and predominantely odontocete clicks. Both the beaked as well as the melon-headed whales are present year round and show a distinct daily acoustic activity cycle.}
}


@Article{Bas-2017,
  AUTHOR = {Bas, Yves and Bas, Didier and Julien, Jean-François},
  JOURNAL = {Journal of Open Research Software},
  MONTH = {02},
  TITLE = {Tadarida: A Toolbox for Animal Detection on Acoustic Recordings},
  YEAR = {2017},
  DOI = {10.5334/jors.154},
  ABSTRACT = {Passive Acoustic Monitoring (PAM) recently extended to a very wide range of animals, but no available open software has been sufficiently generic to automatically treat several taxonomic groups. Here we present Tadarida, a software toolbox allowing for the detection and labelling of recorded sound events, and to classify any new acoustic data into known classes. It is made up of three modules handling Detection, Labelling and Classification and running on either Linux or Windows. This development resulted in the first open software (1) allowing generic sound event detection (multi-taxa), (2) providing graphical sound labelling at a single-instance level and (3) covering the whole process from sound detection to classification. This generic and modular design opens numerous reuse opportunities among (bio)acoustics researchers, especially for those managing and/or developing PAM schemes.The whole toolbox is openly developed in C++ (Detection and Labelling) and R (Classification) and stored at https://github.com/YvesBas.},
  KEYWORD = {en_US}
}


@Article{Milanelli2024,
  AUTHOR = {Milanelli, A.M. and Rossi-Santos, M.R. and Fruet, P.F. and Assump\c{c}ão, R. and Cavalcanti, A.M. and Dalla Rosa, L.},
  JOURNAL = {Estuarine, Coastal and Shelf Science},
  MONTH = {02},
  PAGES = {},
  PUBLISHER = {Elsevier BV},
  TITLE = {Temporal patterns in the soundscape of the port area in an urban estuary},
  VOLUME = {297},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1016/j.ecss.2023.108596},
  DOI = {10.1016/j.ecss.2023.108596},
  ISSN = {0272-7714},
  ABSTRACT = {Soundscape ecology, facilitated by the collection of environmental sounds, enables the determination of the area used by specific species, estimation of population abundance, verification of habitat health, and assistance in management and monitoring processes. Estuaries are key environments in the life cycle of many species. As coastal ecosystems, they face potential impacts by human activities. This study aims to characterize the soundscape of the Patos Lagoon estuary (PLE), Brazil, examining its temporal and seasonal variations. Additionally, it seeks to investigate the influence of temperature and salinity on sound pressure levels (SPL) across three different frequency bands throughout the year. For data collection, two autonomous devices - a SoundTrap digital recorder and an F-POD sampler - were used. The data collection spanned from September 2020 to August 2021 and covered one consecutive week per season. The analysis of audio files was performed manually using Raven software. Sound emissions by Lahille's bottlenose dolphin (Tursiops truncatus gephyreus) and fish were observed throughout the sampling period, with peak detections occurring during the summer and spring, respectively. Boat noise was detected throughout the year but more intensely in the summer. Generalized linear models (GLMs) revealed that, for the three SPL frequency bands, the season significantly influenced the composition of the PLE soundscape, with positive fluctuations in the warmer seasons (summer > spring > autumn > winter). Salinity, in turn, showed a slightly negative relationship with the SPL values in the three models, suggesting a decrease in frequencies with increasing salinity. Our results also indicate that the low-frequency SPL, originating mostly from various sources of anthropogenic noise, constitutes the main component of the PLE soundscape. In conclusion, our study provides novel insights into the temporal patterns of the soundscape in an urban estuary, including biotic and anthropogenic sounds, and their relationships with selected environmental variables.}
}


@Article{10.1371/journal.pone.0210364,
  AUTHOR = {Alpízar, Priscilla AND Rodríguez-Herrera, Bernal AND Jung, Kirsten},
  JOURNAL = {PLOS ONE},
  MONTH = {01},
  NUMBER = {},
  PAGES = {1--15},
  PUBLISHER = {Public Library of Science},
  TITLE = {The effect of local land use on aerial insectivorous bats (Chiroptera) within the two dominating crop types in the Northern-Caribbean lowlands of Costa Rica},
  VOLUME = {},
  YEAR = {2019},
  URL = {https://doi.org/10.1371/journal.pone.0210364},
  DOI = {10.1371/journal.pone.0210364},
  ABSTRACT = {Land transformation into agricultural areas and the intensification of management practices represent two of the most devastating threats to biodiversity worldwide. Within this study, we investigated the effect of intensively managed agroecosystems on bat activity and species composition within two focal areas differing in landscape structure. We sampled bats via acoustic monitoring and insects with flight interception traps in banana and pineapple monoculture plantations and two nearby protected forested areas within the area of Sarapiquí, Costa Rica. Our results revealed that general occurrence and feeding activity of bats was higher above plantations compared to forested areas. We also recorded higher species richness at recording sites in plantations. This trend was especially strong within a fragmented landscape, with only four species recorded in forests, but 12 above pineapple plantations. Several bat species, however, occurred only once or twice above plantations, and forest specialist species such as Centronycteris centralis, Myotis riparius and Pteronotus mesoamericanus were only recorded at forest sites. Our results indicated, that mostly mobile open space and edge foraging bat species can use plantations as potential foraging habitat and might even take advantage of temporal insect outbreaks. However, forests are vital refugia for several species, including slower flying forest specialists, and thus a prerequisite to safeguard bat diversity within agricultural dominated landscapes.}
}


@Article{Do_Nascimento_2024,
  AUTHOR = {Do Nascimento, Leandro A. and Pérez-Granados, Cristian and Alencar, Janderson B. Rodrigues and Beard, Karen H.},
  JOURNAL = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  MONTH = {05},
  NUMBER = {},
  PUBLISHER = {The Royal Society},
  TITLE = {Time and habitat structure shape insect acoustic activity in the Amazon},
  VOLUME = {},
  YEAR = {2024},
  URL = {http://dx.doi.org/10.1098/rstb.2023.0112},
  DOI = {10.1098/rstb.2023.0112},
  ISSN = {1471-2970},
  ABSTRACT = {Insects are the most diverse animal taxon on Earth and play a key role in ecosystem functioning. However, they are often neglected by ecological surveys owing to the difficulties involved in monitoring this small and hyper-diverse taxon. With technological advances in biomonitoring and analytical methods, these shortcomings may finally be addressed. Here, we performed passive acoustic monitoring at 141 sites (eight habitats) to investigate insect acoustic activity in the Viruá National Park, Brazil. We first describe the frequency range occupied by three soniferous insect groups (cicadas, crickets and katydids) to calculate the acoustic evenness index (AEI). Then, we assess how AEI varies spatially and temporally among habitat types, and finally we investigate the relationship between vegetation structure variables and AEI for each insect category. Overall, crickets occupied lower and narrower frequency bands than cicadas and katydids. AEI values varied among insect categories and across space and time. The highest acoustic activity occurred before sunrise and the lowest acoustic activity was recorded in pastures. Canopy cover was positively associated with cricket acoustic activity but not with katydids. Our findings contribute to a better understanding of the role of time, habitat and vegetation structure in shaping insect activity within diverse Amazonian ecosystems.}
}


@Article{Gomes2020-sb,
  AUTHOR = {Gomes, Dylan G E and Appel, Giulliana and Barber, Jesse R},
  JOURNAL = {PeerJ},
  MONTH = {},
  NUMBER = {e10591},
  PAGES = {e10591},
  PUBLISHER = {PeerJ},
  TITLE = {Time of night and moonlight structure vertical space use by insectivorous bats in a Neotropical rainforest: an acoustic monitoring study},
  VOLUME = {},
  YEAR = {2020},
  KEYWORDS = {Bat activity; Chiroptera; Daily cycle; Diel; Moon; Neotropics; Passive acoustic monitoring; Rainforest; Temporal patterns},
  ABSTRACT = {BACKGROUND: Previous research has shown diverse vertical space use by various taxa, highlighting the importance of forest vertical structure. Yet, we know little about vertical space use of tropical forests, and we often fail to explore how this three-dimensional space use changes over time. METHODS: Here we use canopy tower systems in French Guiana and passive acoustic monitoring to measure Neotropical bat activity above and below the forest canopy throughout nine nights. We use a Bayesian generalized linear mixed effect model and kernel density estimates to demonstrate patterns in space-use over time. RESULTS: We found that different bats use both canopy and understory space differently and that these patterns change throughout the night. Overall, bats were more active above the canopy (including Cormura brevirostris, Molossus molossus, Peropteryx kappleri and Peropteryx macrotis), but multiple species or acoustic complexes (when species identification was impossible) were more active in the understory (such as Centronycteris maximiliani, Myotis riparius, Pteronotus alitonus and Pteronotus rubiginosus). We also found that most bats showed temporally-changing preferences in hourly activity. Some species were less active (e.g., P. kappleri and P. macrotis), whereas others were more active (Pteronotus gymnonotus, C. brevirostris, and M. molossus) on nights with higher moon illuminance. DISCUSSION: Here we show that Neotropical bats use habitat above the forest canopy and within the forest understory differently throughout the night. While bats generally were more active above the forest canopy, we show that individual groups of bats use space differently over the course of a night, and some prefer the understory. This work highlights the need to consider diel cycles in studies of space use, as animals use different habitats during different periods of the day.},
  LANGUAGE = {en}
}


@Article{Rigakis_2021,
  AUTHOR = {Rigakis, Iraklis and Potamitis, Ilyas and Tatlas, Nicolaos-Alexandros and Potirakis, Stelios M. and Ntalampiras, Stavros},
  JOURNAL = {Smart Cities},
  MONTH = {02},
  NUMBER = {},
  PAGES = {271-285},
  PUBLISHER = {MDPI AG},
  TITLE = {TreeVibes: Modern Tools for Global Monitoring of Trees for Borers},
  VOLUME = {},
  YEAR = {2021},
  URL = {http://dx.doi.org/10.3390/smartcities4010017},
  DOI = {10.3390/smartcities4010017},
  ISSN = {2624-6511},
  ABSTRACT = {Is there a wood-feeding insect inside a tree or wooden structure? We investigate several ways of how deep learning approaches can massively scan recordings of vibrations stemming from probed trees to infer their infestation state with wood-boring insects that feed and move inside wood. The recordings come from remotely controlled devices that sample the internal soundscape of trees on a 24/7 basis and wirelessly transmit brief recordings of the registered vibrations to a cloud server. We discuss the different sources of vibrations that can be picked up from trees in urban environments and how deep learning methods can focus on those originating from borers. Our goal is to match the problem of the accelerated—due to global trade and climate change— establishment of invasive xylophagus insects by increasing the capacity of inspection agencies. We aim at introducing permanent, cost-effective, automatic monitoring of trees based on deep learning techniques, in commodity entry points as well as in wild, urban and cultivated areas in order to effect large-scale, sustainable pest-risk analysis and management of wood boring insects such as those from the Cerambycidae family (longhorn beetles).}
}


@Article{https://doi.org/10.1111/2041-210X.13520,
  AUTHOR = {Clink, Dena J. and Klinck, Holger},
  JOURNAL = {Methods in Ecology and Evolution},
  NUMBER = {2},
  PAGES = {328--341},
  TITLE = {Unsupervised acoustic classification of individual gibbon females and the implications for passive acoustic monitoring},
  VOLUME = {},
  YEAR = {2021},
  URL = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13520},
  KEYWORDS = {affinity propagation, Gaussian mixture models, Hylobates, K-medoids, normalized mutual information, passive acoustic monitoring, unsupervised clustering},
  DOI = {https://doi.org/10.1111/2041-210X.13520},
  ABSTRACT = {Abstract Passive acoustic monitoring (PAM) has the potential to greatly improve our ability to monitor cryptic yet vocal animals. Advances in automated signal detection have increased the scope of PAM, but distinguishing between individuals—which is necessary for density estimation—remains a major challenge. When individual identity is known, supervised classification techniques can be used to distinguish between individuals. Supervised methods require labelled training data, whereas unsupervised techniques do not. If the acoustic signals of individuals are sufficiently different, the number of clusters might represent the number of individuals sampled. The majority of applications of unsupervised techniques in animal vocalizations have focused on quantifying species-specific call repertoires. However, with increased interest in PAM applications, unsupervised methods that can distinguish between individuals are needed. Here we use an existing dataset of Bornean gibbon female calls with known identity from five sites on Malaysian Borneo to test the ability of three different unsupervised clustering algorithms (affinity propagation, K-medoids and Gaussian mixture model-based clustering) to distinguish between individuals. Calls from different gibbon females are readily distinguishable using supervised techniques. For internal validation of unsupervised cluster solutions, we calculated silhouette coefficients. For external validation, we compared clustering results with female identity labels using a standard metric: normalized mutual information. We also calculated classification accuracy by assigning unsupervised cluster solutions to females based on which cluster had the highest number of calls from a particular female. We found that affinity propagation clustering consistently outperformed the other algorithms for all metrics used. In particular, classification accuracy of affinity propagation clustering was more consistent as the number of females increased, and when we randomly sampled females across sites. We conclude that unsupervised techniques may be useful for providing additional information regarding individual identity for PAM applications. We stress that although we use gibbons as a case study, these methods will be applicable for any individually distinct vocal animal.},
  EPRINT = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13520}
}
