@phdthesis{best:tel-03826638,
	author       = {Best, Paul},
	year         = {2022},
	title        = {{Automated Detection and Classification of Cetacean Acoustic Signals}},
	month        = {09},
	url          = {https://hal.science/tel-03826638},
	school       = {{Universit{\'e} de toulon}},
	keywords     = {Bioacoustic ; Cetaceans ; Artificial neural netwoks ; Bioacoustique ; C{\'e}tac{\'e}s ; R{\'e}seaux de neurones artificiels},
	type         = {Theses},
	pdf          = {https://hal.science/tel-03826638v1/file/2022___PhD_these___Paul_Best.pdf},
	hal_id       = {tel-03826638},
	hal_version  = {v1}
}
@article{MoscosoRosero2018,
	author       = {Paola Moscoso Rosero},
	year         = {2018},
	title        = {{The use of acoustic analyses to evaluate ecological and social impacts of habitat degradation in contemporary conservation biology}},
	month        = 8,
	url          = {https://sussex.figshare.com/articles/thesis/The_use_of_acoustic_analyses_to_evaluate_ecological_and_social_impacts_of_habitat_degradation_in_contemporary_conservation_biology/23461094}
}
@article{Silva2022,
	author       = {Silva,  Bruno and Mestre,  Frederico and Barreiro,  Sílvia and Alves,  Pedro J. and Herrera,  José M.},
	year         = {2022},
	title        = {<scp>sound</scp>C<scp>lass</scp>: An automatic sound classification tool for biodiversity monitoring using machine learning},
	month        = {08},
	journal      = {Methods in Ecology and Evolution},
	publisher    = {Wiley},
	volume       = 13,
	number       = 11,
	pages        = {2356--2362},
	doi          = {10.1111/2041-210x.13964},
	issn         = {2041-210X},
	url          = {http://dx.doi.org/10.1111/2041-210X.13964}
}
@book{Hasselman2020,
	author       = {Hasselman,  Daniel and Barclay,  David and Cavagnaro,  Robert and Chandler,  Craig and Cotter,  Emma and Gillespie,  Douglas and Hastie,  Gordon and Horne,  John and Joslin,  James and Long,  Caitlin and McGarry,  Louise and Mueller,  Robert and Sparling,  Carol and Williamson,  Benjamin},
	year         = {2020},
	title        = {2020 State of the Science Report,  Chapter 10: Environmental Monitoring Technologies and Techniques for Detecting Interactions of Marine Animals with Turbines},
	month        = {09},
	doi          = {10.2172/1633202},
	url          = {http://dx.doi.org/10.2172/1633202},
	institution  = {Office of Scientific and Technical Information (OSTI)}
}
@article{Picciulin2022,
	author       = {Picciulin,  Marta and Bolgan,  Marta and Rako-Gospić,  Nikolina and Petrizzo,  Antonio and Radulović,  Marko and Falkner,  Raffaela},
	year         = {2022},
	title        = {A Fish and Dolphin Biophony in the Boat Noise-Dominated Soundscape of the Cres-Lošinj Archipelago (Croatia)},
	month        = {02},
	journal      = {Journal of Marine Science and Engineering},
	publisher    = {MDPI AG},
	volume       = 10,
	number       = {2},
	pages        = 300,
	doi          = {10.3390/jmse10020300},
	issn         = {2077-1312},
	url          = {http://dx.doi.org/10.3390/jmse10020300}
}
@article{PUTLAND2018479,
	author       = {R.L. Putland and L. Ranjard and R. Constantine and C.A. Radford},
	year         = {2018},
	title        = {A hidden Markov model approach to indicate Bryde's whale acoustics},
	journal      = {Ecological Indicators},
	volume       = 84,
	pages        = {479--487},
	doi          = {https://doi.org/10.1016/j.ecolind.2017.09.025},
	issn         = {1470-160X},
	url          = {https://www.sciencedirect.com/science/article/pii/S1470160X17305939},
	keywords     = {Acoustic detection, Cetacean, Hidden markov models, Noise mitigation, Passive acoustic monitoring, Temporal variation},
	abstract     = {Increasing sound in the ocean from human activity potentially threatens marine animals that use sound to communicate, detect prey, avoid predators and function within their ecosystem. The detection and classification of sound produced by marine animals, such as whales and fish, is an important component in noise mitigation strategies, while also providing valuable insights into their ecology. Traditionally, visual surveys are conducted to assess how these animals utilize a specific area, often underestimating the number of individuals as they don’t spend much time at the surface. Long-term passive acoustic monitoring efforts have become more prevalent to monitor such animals. The large datasets collected can be impractical to manually process, necessitating the development of automated detection methods, which often produce mixed results owing to the broad frequency range and variable duration of many biological sounds. Here we describe a novel approach for automated detection of underwater biophonic sounds employing hidden Markov models (HMM). Acoustic data was collected at a single listening station in Hauraki Gulf, from October 2014 to April 2016. HMM detection models were developed for Bryde’s whales (Balaenoptera edeni) that were used as a model organism because they are notoriously hard to study with traditional visual surveys and produce a characteristic call. Bryde’s whale calls also directly overlap the sounds of anthropogenic activity, in particular the sound of vessels transiting to the busiest port in New Zealand; therefore monitoring whale calls is of utmost importance when confronting increasing sound in the ocean. Vocalizations were detected with a sensitivity of 77% and false positive rate of 23%. Bryde’s whale vocalizations were detected on 11% of all recordings. Overall, there were significantly more detections during summer (n=1716) than winter (n=447), and significantly more during the day (n=1991) compared to night (n=1264). This study shows the feasibility of using HMMs on long-term acoustic datasets. The method has the potential to be used for a wide range of soniferous animals who, like the Bryde’s whale, also produce unique sounds. The detection method would be particularly useful for mitigation and management strategies of species that are difficult to detect using traditional visual methods.}
}
@article{jzbg2020015,
	author       = {Jones, Brittany L. and Oswald, Michael and Tufano, Samantha and Baird, Mark and Mulsow, Jason and Ridgway, Sam H.},
	year         = {2021},
	title        = {A System for Monitoring Acoustics to Supplement an Animal Welfare Plan for Bottlenose Dolphins},
	journal      = {Journal of Zoological and Botanical Gardens},
	volume       = {2},
	number       = {2},
	pages        = {222--233},
	doi          = {10.3390/jzbg2020015},
	issn         = {2673-5636},
	url          = {https://www.mdpi.com/2673-5636/2/2/15},
	abstract     = {Animal sounds are commonly used by humans to infer information about their motivations and their health, yet, acoustic data is an underutilized welfare biomarker especially for aquatic animals. Here, we describe an acoustic monitoring system that is being implemented at the U.S. Navy Marine Mammal Program where dolphins live in groups in ocean enclosures in San Diego Bay. A four-element bottom mounted hydrophone array is used to continuously record, detect and localize acoustic detections from this focal group. Software provides users an automated comparison of the current acoustic behavior to group historical data which can be used to identify periods of normal, healthy thriving dolphins, and allows rare instances of deviations from typical behavior to stand out. Variations in a group or individual’s call rates can be correlated with independent veterinary examinations and behavioral observations in order to better assess dolphin health and welfare. Additionally, the monitoring system identifies time periods in which a sound source from San Diego Bay is of high-enough amplitude that the received level at our array is considered a potential concern for the focal animals. These time stamps can be used to identify and potentially mitigate exposures to acoustic sources that may otherwise not be obvious to human listeners. We hope this application inspires zoos and aquaria to innovate and create ways to incorporate acoustic information into their own animal welfare management programs.}
}
@article{article,
	author       = {Favrot, Sylvain and Buchholz, Jörg},
	year         = {2008},
	title        = {A virtual auditory environment for investigating the auditory signal processing of realistic sounds},
	month        = {06},
	journal      = {The Journal of the Acoustical Society of America},
	volume       = 123,
	pages        = 3935,
	doi          = {10.1121/1.2936003}
}
@article{Halliday_2019,
	author       = {Halliday, W.D. and Pine, M.K. and Insley, S.J. and Soares, R.N. and Kortsalo, P. and Mouy, X.},
	year         = {2019},
	title        = {Acoustic detections of Arctic marine mammals near Ulukhaktok, Northwest Territories, Canada},
	month        = jan,
	journal      = {Canadian Journal of Zoology},
	publisher    = {Canadian Science Publishing},
	volume       = 97,
	number       = 1,
	pages        = {72-80},
	doi          = {10.1139/cjz-2018-0077},
	issn         = {1480-3283},
	url          = {http://dx.doi.org/10.1139/cjz-2018-0077}
}
@article{Papin_2018,
	author       = {Papin, Morgane and Pichenot, Julian and Guérold, François and Germain, Estelle},
	year         = {2018},
	title        = {Acoustic localization at large scales: a promising method for grey wolf monitoring},
	month        = apr,
	journal      = {Frontiers in Zoology},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 15,
	number       = 1,
	doi          = {10.1186/s12983-018-0260-2},
	issn         = {1742-9994},
	url          = {http://dx.doi.org/10.1186/s12983-018-0260-2}
}
@article{https://doi.org/10.1111/2041-210X.12730,
	author       = {Wrege, Peter H. and Rowland, Elizabeth D. and Keen, Sara and Shiu, Yu},
	year         = {2017},
	title        = {Acoustic monitoring for conservation in tropical forests: examples from forest elephants},
	journal      = {Methods in Ecology and Evolution},
	volume       = 8,
	number       = 10,
	pages        = {1292--1301},
	doi          = {https://doi.org/10.1111/2041-210X.12730},
	url          = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12730},
	keywords     = {anthropogenic impacts, anti-poaching, bioacoustics, cryptic species, density estimate, law enforcement monitoring, Loxodonta cyclotis, passive acoustic monitoring, signal detection, smart},
	eprint       = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12730},
	abstract     = {Summary The accelerating loss of biodiversity worldwide demands effective tools for monitoring animal populations and informing conservation action. In habitats where direct observation is difficult (rain forests, oceans), or for cryptic species (shy, nocturnal), passive acoustic monitoring (PAM) provides cost-effective, unbiased data collection. PAM has broad applicability in terrestrial environments, particularly tropical rain forests. Using examples from studies of forest elephants in Central African rain forest, we show how PAM can be used to investigate cryptic behaviour, mechanisms of communication, estimate population size, quantify threats, and assess the efficacy of conservation strategies. We discuss the methodologies, requirements, and challenges of obtaining these data using acoustics. Where applicable, we compare these methods to more traditional approaches. While PAM methods and associated analysis are maturing rapidly, mechanisms are needed for processing the dense raw data efficiently with standard computer hardware, speeding development of detection algorithms, and harnessing communication networks to move data from the field to research facilities. Passive acoustic monitoring is a viable and cost-effective tool for conservation and should be incorporated in monitoring schemes much more broadly. The capability to quickly assess changes in behaviour, population size, and landscape use, simultaneously over large geographical areas, makes this approach attractive for detecting human-induced impacts and for assessing the success of conservation strategies.}
}
@phdthesis{hockman2018acoustic,
	author       = {Hockman, Emily Vera},
	year         = {2018},
	title        = {Acoustic monitoring of wildlife in inaccessible areas and automatic detection of bird songs from continuous recordings},
	url          = {https://trace.tennessee.edu/utk_graddiss/4874},
	school       = {University of Tennessee},
	type         = {PhD dissertation}
}
@article{Berman_2024,
	author       = {Berman, Laura and Xuan Tan, Wei and Grafe, Ulmar and Rheindt, Frank},
	year         = {2024},
	title        = {Acoustic phenology of tropical resident birds differs between native forest species and parkland colonizer species},
	month        = jun,
	journal      = {Journal of Avian Biology},
	publisher    = {Wiley},
	doi          = {10.1111/jav.03298},
	issn         = {1600-048X},
	url          = {http://dx.doi.org/10.1111/jav.03298}
}
@techreport{WHOI2007,
	author       = {{Woods Hole Oceanographic Institution}},
	year         = {2007},
	title        = {Acoustic Response and Detection of Marine Mammals Using an Advanced Digital Acoustic Recording Tag (Rev 3)},
	month        = {03},
	number       = {ADA606315},
	url          = {https://apps.dtic.mil/sti/pdfs/ADA606315.pdf},
	institution  = {Defense Technical Information Center}
}
@book{Murugaiya_2022,
	author       = {Murugaiya, Ramashini and Mahagammulle Gamage, Manisha Milani and Murugiah, Krishani and Perumal, Madhumathy},
	year         = {2022},
	title        = {Acoustic-Based Applications for Vertebrate Vocalization},
	journal      = {SpringerBriefs in Applied Sciences and Technology},
	publisher    = {Springer International Publishing},
	doi          = {10.1007/978-3-030-85773-8},
	isbn         = 9783030857738,
	issn         = {2191-5318},
	url          = {http://dx.doi.org/10.1007/978-3-030-85773-8}
}
@article{https://doi.org/10.24377/ljmu.t.00016120,
	author       = {Crunchant,  A-S},
	year         = {2022},
	title        = {Acoustically assessing apes: chimpanzee conservation with passive acoustic monitoring},
	publisher    = {Liverpool John Moores University},
	doi          = {10.24377/LJMU.T.00016120},
	url          = {https://researchonline.ljmu.ac.uk/id/eprint/16120},
	copyright    = {Creative Commons Attribution Non Commercial 3.0 Unported}
}
@article{Sugimatsu14,
	author       = {Sugimatsu, Harumi and Kojima, Junichi and Ura, Tamaki and Bahl, R. and Behera, Sandeep and Sagar, Vivek and Singh, Hari and De, Rupak},
	year         = {2014},
	title        = {Advanced Technique for Automatic Detection and Discrimination of a Click Train With Short Interclick Intervals From the Clicks of Ganges River Dolphins (Platanista gangetica gangetica) Recorded by a Passive Acoustic Monitoring System Using Hydrophone Arrays},
	month        = {05},
	journal      = {Marine Technology Society Journal},
	volume       = 48,
	pages        = {},
	doi          = {10.4031/MTSJ.48.3.15}
}
@article{Singer_2024,
	author       = {Singer, David and Hagge, Jonas and Kamp, Johannes and Hondong, Hermann and Schuldt, Andreas},
	year         = {2024},
	title        = {Aggregated time‐series features boost species‐specific differentiation of true and false positives in passive acoustic monitoring of bird assemblages},
	month        = {02},
	journal      = {Remote Sensing in Ecology and Conservation},
	publisher    = {Wiley},
	volume       = 10,
	number       = 4,
	pages        = {517–530},
	doi          = {10.1002/rse2.385},
	issn         = {2056-3485},
	url          = {http://dx.doi.org/10.1002/rse2.385}
}
@article{DoNascimento2023,
	author       = {Do Nascimento,  Leandro and Pérez-Granados,  Cristian and Rodrigues Alencar,  Janderson and Beard,  Karen},
	year         = {2023},
	title        = {Amazonian soundscapes: unravelling the secrets of insect acoustic niches in diverse habitats},
	month        = 11,
	publisher    = {California Digital Library (CDL)},
	doi          = {10.32942/x2102p},
	url          = {http://dx.doi.org/10.32942/X2102P}
}
@article{10.3897/BDJ.11.e97811,
	author       = {Shih-Hung Wu and Jerome Chie-Jen Ko and Ruey-Shing Lin and Wen-Ling Tsai and Hsueh-Wen Chang},
	year         = {2023},
	title        = {An acoustic detection dataset of birds (Aves) in montane forests using a deep learning approach},
	journal      = {Biodiversity Data Journal},
	publisher    = {Pensoft Publishers},
	volume       = 11,
	number       = {},
	pages        = {e97811},
	doi          = {10.3897/BDJ.11.e97811},
	issn         = {1314-2836},
	url          = {https://doi.org/10.3897/BDJ.11.e97811},
	abstract     = {Long-term monitoring is needed to understand the statuses and trends of wildlife communities in montane forests, such as those in Yushan National Park (YSNP), Taiwan. Integrating passive acoustic monitoring (PAM) with an automated sound identifier, a long-term biodiversity monitoring project containing six PAM stations, was launched in YSNP in January 2020 and is currently ongoing. SILIC, an automated wildlife sound identification model, was used to extract sounds and species information from the recordings collected. Animal vocal activity can reflect their breeding status, behaviour, population, movement and distribution, which may be affected by factors, such as habitat loss, climate change and human activity. This massive amount of wildlife vocalisation dataset can provide essential information for the National Park's headquarters on resource management and decision-making. It can also be valuable for those studying the effects of climate change on animal distribution and behaviour at a regional or global scale.To our best knowledge, this is the first open-access dataset with species occurrence data extracted from sounds in soundscape recordings by artificial intelligence. We obtained seven bird species for the first release, with more bird species and other taxa, such as mammals and frogs, to be updated annually. Raw recordings containing over 1.7 million one-minute recordings collected between the years 2020 and 2021 were analysed and SILIC identified 6,243,820 vocalisations of seven bird species in 439,275 recordings. The automatic detection had a precision of 0.95 and the recall ranged from 0.48 to 0.80. In terms of the balance between precision and recall, we prioritised increasing precision over recall in order to minimise false positive detections. In this dataset, we summarised the count of vocalisations detected per sound class per recording which resulted in 802,670 occurrence records. Unlike data from traditional human observation methods, the number of observations in the Darwin Core "organismQuantity" column refers to the number of vocalisations detected for a specific bird species and cannot be directly linked to the number of individuals.We expect our dataset will be able to help fill the data gaps of fine-scale avian temporal activity patterns in montane forests and contribute to studies concerning the impacts of climate change on montane forest ecosystems on regional or global scales.},
	eprint       = {https://doi.org/10.3897/BDJ.11.e97811}
}
@article{Lin_2013,
	author       = {Lin, Tzu-Hao and Chou, Lien-Siang and Akamatsu, Tomonari and Chan, Hsiang-Chih and Chen, Chi-Fang},
	year         = {2013},
	title        = {An automatic detection algorithm for extracting the representative frequency of cetacean tonal sounds},
	month        = {09},
	journal      = {The Journal of the Acoustical Society of America},
	publisher    = {Acoustical Society of America (ASA)},
	volume       = 134,
	number       = 3,
	pages        = {2477–2485},
	doi          = {10.1121/1.4816572},
	issn         = {1520-8524},
	url          = {http://dx.doi.org/10.1121/1.4816572}
}
@article{Kershenbaum2013,
	author       = {Kershenbaum,  Arik and Roch,  Marie A.},
	year         = {2013},
	title        = {An image processing based paradigm for the extraction of tonal sounds in cetacean communications},
	month        = 12,
	journal      = {The Journal of the Acoustical Society of America},
	publisher    = {Acoustical Society of America (ASA)},
	volume       = 134,
	number       = 6,
	pages        = {4435--4445},
	doi          = {10.1121/1.4828821},
	issn         = {1520-8524},
	url          = {http://dx.doi.org/10.1121/1.4828821}
}
@article{Buchan_2019,
	author       = {Buchan, Susannah J. and Mahú, Rodrigo and Wuth, Jorge and Balcazar-Cabrera, Naysa and Gutierrez, Laura and Neira, Sergio and Yoma, Néstor Becerra},
	year         = {2019},
	title        = {An unsupervised Hidden Markov Model-based system for the detection and classification of blue whale vocalizations off Chile},
	month        = {01},
	journal      = {Bioacoustics},
	publisher    = {Informa UK Limited},
	volume       = {29},
	number       = {2},
	pages        = {140–167},
	doi          = {10.1080/09524622.2018.1563758},
	issn         = {2165-0586},
	url          = {http://dx.doi.org/10.1080/09524622.2018.1563758}
}
@inbook{Schoeman2022,
	author       = {Schoeman, Ren{\'e}e P. and Erbe, Christine and Pavan, Gianni and Righini, Roberta and Thomas, Jeanette A.},
	year         = {2022},
	title        = {Analysis of Soundscapes as an Ecological Tool},
	booktitle    = {Exploring Animal Behavior Through Sound: Volume 1: Methods},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {217--267},
	doi          = {10.1007/978-3-030-97540-1_7},
	isbn         = {978-3-030-97540-1},
	url          = {https://doi.org/10.1007/978-3-030-97540-1_7},
	editor       = {Erbe, Christine and Thomas, Jeanette A.},
	abstract     = {Soundscapes have been likened to acoustic landscapes, encompassing all the acoustic features of an area. The sounds that make up a soundscape can be grouped according to their source into biophony (sounds from animals), geophony (sounds from atmospheric and geophysical events), and anthropophony (sounds from human activities). Natural soundscapes have changed over time because of human activities that generate sound, alter land-use patterns, remove animals from natural settings, and result in climate change. These human activities have direct and indirect effects on animal distribution patterns and (acoustic) behavior. Consequently, current soundscapes may be very different from those a few hundred years ago. This is of concern as natural soundscapes have ecological value. Losing natural soundscapes may, therefore, result in a loss of biodiversity and ecosystem functioning. The study of soundscapes can identify ecosystems undergoing change and potentially document causes (such as noise from human activities). Methods for studying soundscapes range from listening and creating visual (spectrographic) displays to the computation of acoustic indices and advanced statistical modeling. Passive acoustic recording has become an ecological tool for research, monitoring, and ultimately conservation management. This chapter introduces terrestrial and aquatic soundscapes, soundscape analysis tools, and soundscape management.}
}
@article{Diepstraten2021,
	author       = {Diepstraten, Johan and Willie, Jacob},
	year         = {2021},
	title        = {Assessing the structure and drivers of biological sounds along a disturbance gradient},
	journal      = {Global Ecology and Conservation},
	volume       = 31,
	pages        = {e01850},
	doi          = {10.1016/j.gecco.2021.e01850},
	url          = {https://doi.org/10.1016/j.gecco.2021.e01850}
}
@techreport{Booth2017,
	author       = {Booth, C. G. and Oedekoven, C. S. and Gillespie, D. and Macaulay, J. and Plunkett, R. and Joy, R. and Harris, D. and Wood, J. and Marques, T. A. and Marshall, L. and Verfuss, U. K. and Tyack, P. and Johnson, M. and Thomas, L.},
	year         = {2017},
	title        = {Assessing the Viability of Density Estimation for Cetaceans from Passive Acoustic Fixed Sensors throughout the Life Cycle of an Offshore E\&P Field Development},
	number       = {SMRUC-OGP-2017-001},
	note         = {Submitted to IOGP Sound and Marine Life Joint Industry Programme (Unpublished)},
	institution  = {SMRU Consulting},
	type         = {Report}
}
@report{Scheidat2009,
	author       = {Scheidat, M and Aarts, G and Bakker, A and Brasseur, S and Carstensen, J and van Leeuwen, P and Leopold, M and van Polanen Petel, T and Reijnders, P and Teilmann, J and Tougaard, J and Verdaat, H},
	year         = {2009},
	title        = {Assessment of the Effects of the Offshore Wind Farm Egmond aan Zee (OWEZ) for Harbour Porpoise (comparison T0 and T1)},
	month        = {09},
	pages        = 47,
	url          = {http://www.wur.nl/en/Publication-details.htm?publicationId=publication-way-343338363132},
	abstract     = {The aim of this study was to investigate if the Offshore Wind farm Egmond aan Zee (OWEZ) influenced the occurrence of harbour porpoises. In order to evaluate the environmental impacts of OWEZ, porpoise occurrence in the area was monitored: during a baseline (T0) study 2003/2004 (Brasseur at al. 2004) after the construction of the wind park (T1) from 2007 to 2009. The comparison between the T0 and the T1 was conducted to determine if and how harbour porpoises react to the presence of the wind park. This report describes the results and analyses of this comparison.},
	institution  = {IMARES - Wageningen UR},
	language     = {English},
	keywords     = {Wind Energy, Fixed Offshore Wind, Habitat Change, Marine Mammals}
}
@article{Mcloughlin_2019,
	author       = {Mcloughlin, Michael P. and Stewart, Rebecca and McElligott, Alan G.},
	year         = {2019},
	title        = {Automated bioacoustics: methods in ecology and conservation and their potential for animal welfare monitoring},
	month        = {06},
	journal      = {Journal of The Royal Society Interface},
	publisher    = {The Royal Society},
	volume       = 16,
	number       = 155,
	pages        = {20190225},
	doi          = {10.1098/rsif.2019.0225},
	issn         = {1742-5662},
	url          = {http://dx.doi.org/10.1098/rsif.2019.0225}
}
@inproceedings{10032472,
	author       = {Vargas-Masís, Roberto and Segura-Sequeira, David and Alfaro-Rojas, Danny and Díaz, Daniel Rosen},
	year         = {2022},
	title        = {Automated bird acoustic detection at Las Arrieras Nature Reserve in Sarapiquí, Costa Rica},
	booktitle    = {2022 IEEE 4th International Conference on BioInspired Processing (BIP)},
	volume       = {},
	number       = {},
	pages        = {1--8},
	doi          = {10.1109/BIP56202.2022.10032472},
	keywords     = {Biological system modeling;Ecosystems;Decision making;Birds;Acoustics;Ecology;Behavioral sciences;Acoustic detection;Passive acoustic monitoring;Random forest model;Pattern matching;conservation;Costa Rica}
}
@article{Priyadarshani2018,
	author       = {Priyadarshani,  Nirosha and Marsland,  Stephen and Castro,  Isabel},
	year         = {2018},
	title        = {Automated birdsong recognition in complex acoustic environments: a review},
	month        = {05},
	journal      = {Journal of Avian Biology},
	publisher    = {Wiley},
	volume       = 49,
	number       = 5,
	doi          = {10.1111/jav.01447},
	issn         = {1600-048X},
	url          = {http://dx.doi.org/10.1111/jav.01447}
}
@article{van_Kuijk_2023,
	author       = {van Kuijk, Silvy M. and O’Brien, Sun and Clink, Dena J. and Blake, John G. and Di Fiore, Anthony},
	year         = {2023},
	title        = {Automated detection and detection range of primate duets: a case study of the red titi monkey (Plecturocebus discolor) using passive acoustic monitoring},
	month        = {08},
	journal      = {Frontiers in Ecology and Evolution},
	publisher    = {Frontiers Media SA},
	volume       = 11,
	doi          = {10.3389/fevo.2023.1173722},
	issn         = {2296-701X},
	url          = {http://dx.doi.org/10.3389/fevo.2023.1173722}
}
@inproceedings{Ho_Chun_Huang_2016,
	author       = {Ho Chun Huang and Joseph, John and Ming Jer Huang and Margolina, Tetyana},
	year         = {2016},
	title        = {Automated detection and identification of blue and fin whale foraging calls by combining pattern recognition and machine learning techniques},
	month        = {09},
	booktitle    = {OCEANS 2016 MTS/IEEE Monterey},
	publisher    = {IEEE},
	volume       = {},
	number       = {},
	pages        = {1–7},
	doi          = {10.1109/oceans.2016.7761269}
}
@article{Barker_2014,
	author       = {Barker, David J. and Herrera, Christopher and West, Mark O.},
	year         = {2014},
	title        = {Automated detection of 50-kHz ultrasonic vocalizations using template matching in XBAT},
	month        = {10},
	journal      = {Journal of Neuroscience Methods},
	publisher    = {Elsevier BV},
	volume       = {236},
	pages        = {68–75},
	doi          = {10.1016/j.jneumeth.2014.08.007},
	issn         = {0165-0270},
	url          = {http://dx.doi.org/10.1016/j.jneumeth.2014.08.007}
}
@article{Owens2024.04.15.589517,
	author       = {Owens, A. F. and Hockings, Kimberley J. and Imron, Muhammed Ali and Madhusudhana, Shyam and Mariaty and Setia, Tatang Mitra and Sharma, Manmohan and Maimunah, Siti and Van Veen, F. J. F. and Erb, Wendy M.},
	year         = {2024},
	title        = {Automated detection of Bornean white-bearded gibbon (Hylobates albibarbis) vocalisations using an open-source framework for deep learning},
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.04.15.589517},
	url          = {https://www.biorxiv.org/content/early/2024/07/21/2024.04.15.589517},
	elocation-id = {2024.04.15.589517},
	abstract     = {Passive acoustic monitoring is a promising tool for monitoring at-risk populations of vocal species, yet extracting relevant information from large acoustic datasets can be time-consuming, creating a bottleneck at the point of analysis. To address this, we adapted an open-source framework for deep learning in bioacoustics to automatically detect Bornean white-bearded gibbon (Hylobates albibarbis) {\textquotedblleft}great call{\textquotedblright} vocalisations in a long-term acoustic dataset from a rainforest location in Borneo. We describe the steps involved in developing this solution, including collecting audio recordings, developing training and testing datasets, training neural network models, and evaluating model performance. Our best model performed at a satisfactory level (F score = 0.87), identifying 98\% of the highest-quality calls from 90 hours of manually-annotated audio recordings and greatly reduced analysis times when compared to a human observer. We found no significant difference in the temporal distribution of great call detections between the manual annotations and the model{\textquoteright}s output. Future work should seek to apply our model to long-term acoustic datasets to understand spatiotemporal variations in H. albibarbis{\textquoteright} calling activity. Overall, we present a roadmap for applying deep learning to identify the vocalisations of species of interest which can be adapted for monitoring other endangered vocalising species.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2024/07/21/2024.04.15.589517.full.pdf}
}
@article{Radig_2021,
	author       = {Radig, Bernd and Bodesheim, Paul and Korsch, Dimitri and Denzler, Joachim and Haucke, Timm and Klasen, Morris and Steinhage, Volker},
	year         = {2021},
	title        = {Automated Visual Large Scale Monitoring of Faunal Biodiversity},
	month        = {07},
	journal      = {Pattern Recognition and Image Analysis},
	publisher    = {Pleiades Publishing Ltd},
	volume       = 31,
	number       = 3,
	pages        = {477–488},
	doi          = {10.1134/s1054661821030214},
	issn         = {1555-6212},
	url          = {http://dx.doi.org/10.1134/S1054661821030214},
	abstract     = {To observe biodiversity, the variety of plant and animal life in the world or in a particular habitat, human observers make the most common examinations, often assisted by technical equipment. Measuring objectively the number of different species of animals, plants, fungi, and microbes that make up the ecosystem can be difficult. In order to monitor changes in biodiversity, data have to be compared across space and time. Cameras are an essential sensor to determine the species range, abundance, and behavior of animals. The millions of recordings from camera traps set up in natural environments can no longer be analyzed by biologists. We started research on doing this analysis automatically without human interaction. The focus of our present sensor is on image capture of wildlife and moths. Special hardware elements for the detection of different species are designed, implemented, tested, and improved, as well as the algorithms for classification and counting of samples from images and image sequences, e.g., to calculate presence, absence, and abundance values or the duration of characteristic activities related to the spatial mobilities. For this purpose, we are developing stereo camera traps that allow spatial reconstruction of the observed animals. This allows three-dimensional coordinates to be recorded and the shape to be characterized. With this additional feature data, species identification and movement detection are facilitated. To classify and count moths, they are attracted to an illuminated screen, which is then photographed at intervals by a high-resolution color camera. To greatly reduce the volume of data, redundant elements and elements that are consistent from image to image are eliminated. All design decisions take into account that at remote sites and in fully autonomous operation, power supply on the one hand and possibilities for data exchange with central servers on the other hand are limited. Installation at hard-to-reach locations requires a sophisticated and demanding system design with an optimal balance between power requirements, bandwidth for data transmission, required service and operation in all environmental conditions for at least ten years.}
}
@article{Stowell_2018,
	author       = {Stowell, Dan and Wood, Michael D. and Pamuła, Hanna and Stylianou, Yannis and Glotin, Hervé},
	year         = {2018},
	title        = {Automatic acoustic detection of birds through deep learning: The first Bird Audio Detection challenge},
	month        = {11},
	journal      = {Methods in Ecology and Evolution},
	publisher    = {Wiley},
	volume       = 10,
	number       = 3,
	pages        = {368-380},
	doi          = {10.1111/2041-210x.13103},
	issn         = {2041-210X},
	url          = {http://dx.doi.org/10.1111/2041-210X.13103},
	editor       = {Orme, David},
	keywords     = {bird, deep learning, machine learning, passive acoustic monitoring, sound},
	eprint       = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13103},
	abstract     = {Abstract Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus, passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here, we report outcomes from a collaborative data challenge. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects. Multiple methods were able to attain performance of around 88\% area under the receiver operating characteristic (ROC) curve (AUC), much higher performance than previous general-purpose methods. With modern machine learning, including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data, with no manual recalibration, and no pretraining of the detector for the target species or the acoustic conditions in the target environment.}
}
@article{FUNDEL2023102288,
	author       = {Frank Fundel and Daniel A. Braun and Sebastian Gottwald},
	year         = {2023},
	title        = {Automatic bat call classification using transformer networks},
	journal      = {Ecological Informatics},
	volume       = 78,
	pages        = 102288,
	doi          = {https://doi.org/10.1016/j.ecoinf.2023.102288},
	issn         = {1574-9541},
	url          = {https://www.sciencedirect.com/science/article/pii/S1574954123003175},
	keywords     = {Computational bioacoustics, Attention, Transformer, Echolocation, Species identification, Acoustic monitoring, Bat calls},
	abstract     = {Automatically identifying bat species from their echolocation calls is a difficult but important task for monitoring bats and the ecosystem they live in. Major challenges in automatic bat call identification are high call variability, similarities between species, interfering calls and lack of annotated data. Many currently available models suffer from relatively poor performance on real-life data due to being trained on single call datasets and, moreover, are often too slow for real-time classification. Here, we propose a Transformer architecture for multi-label classification with potential applications in real-time classification scenarios. We train our model on synthetically generated multi-species recordings by merging multiple bats calls into a single recording with multiple simultaneous calls. Our approach achieves a single species accuracy of 88.92% (F1-score of 84.23%) and a multi species macro F1-score of 74.40% on our test set. In comparison to three other tools on the independent and publicly available dataset ChiroVox, our model achieves at least 25.82% better accuracy for single species classification and at least 6.9% better macro F1-score for multi species classification.}
}
@article{Zseb_k_2019,
	author       = {Zsebők, Sándor and Nagy-Egri, Máté Ferenc and Barnaföldi, Gergely Gábor and Laczi, Miklós and Nagy, Gergely and Vaskuti, Éva and Garamszegi, László Zsolt},
	year         = {2019},
	title        = {Automatic bird song and syllable segmentation with an open-source deep-learning object detection method – a case study in the Collared Flycatcher (Ficedula albicollis)},
	month        = {12},
	journal      = {Ornis Hungarica},
	publisher    = {Walter de Gruyter GmbH},
	volume       = {27},
	number       = {2},
	pages        = {59–66},
	doi          = {10.2478/orhu-2019-0015},
	issn         = {2061-9588},
	url          = {http://dx.doi.org/10.2478/orhu-2019-0015}
}
@inproceedings{Jancovic_2017,
	author       = {Jancovic, Peter and Kokuer, Munevver},
	year         = {2017},
	title        = {Automatic detection of bird species from audio field recordings using HMM-based modelling of frequency tracks},
	month        = {08},
	booktitle    = {2017 25th European Signal Processing Conference (EUSIPCO)},
	publisher    = {IEEE},
	volume       = {},
	number       = {},
	pages        = {1779–1783},
	doi          = {10.23919/eusipco.2017.8081515},
	url          = {http://dx.doi.org/10.23919/EUSIPCO.2017.8081515},
	keywords     = {Hidden Markov models;Birds;Feature extraction;Training;Acoustics;Data models;bird species detection;field recording;hidden Markov model;HMM;score normalisation;cohort;outlier;vocalisation;element;unsupervised training;sinusoid detection;sinusoidal modelling;frequency track}
}
@inproceedings{Ntalampiras_2020,
	author       = {Ntalampiras, Stavros and Pezzuolo, Andrea and Mattiello, Silvana and Battini, Monica and Brscic, Marta},
	year         = {2020},
	title        = {Automatic detection of cow/calf vocalizations in free-stall barn},
	month        = {07},
	booktitle    = {2020 43rd International Conference on Telecommunications and Signal Processing (TSP)},
	publisher    = {IEEE},
	volume       = {},
	number       = {},
	pages        = {41–45},
	doi          = {10.1109/tsp49548.2020.9163522},
	url          = {http://dx.doi.org/10.1109/TSP49548.2020.9163522},
	keywords     = {Ethics;Animals;Signal processing algorithms;Hidden Markov models;Production;Agriculture;Acoustics;Precision livestock farming;cow/calf vocalization;acoustic signal processing;vocalization detection;audio pattern recognition}
}
@article{Tani_2013,
	author       = {Tani, Yukinori and Yokota, Yasunari and Yayota, Masato and Ohtani, Shigeru},
	year         = {2013},
	title        = {Automatic recognition and classification of cattle chewing activity by an acoustic monitoring method with a single-axis acceleration sensor},
	month        = {03},
	journal      = {Computers and Electronics in Agriculture},
	publisher    = {Elsevier BV},
	volume       = 92,
	pages        = {54–65},
	doi          = {10.1016/j.compag.2013.01.001},
	issn         = {0168-1699},
	url          = {http://dx.doi.org/10.1016/j.compag.2013.01.001}
}
@article{Mouy_2009,
	author       = {Mouy, Xavier and Bahoura, Mohammed and Simard, Yvan},
	year         = {2009},
	title        = {Automatic recognition of fin and blue whale calls for real-time monitoring in the St. Lawrence},
	month        = {12},
	journal      = {The Journal of the Acoustical Society of America},
	publisher    = {Acoustical Society of America (ASA)},
	volume       = 126,
	number       = 6,
	pages        = {2918–2928},
	doi          = {10.1121/1.3257588},
	issn         = {1520-8524},
	url          = {http://dx.doi.org/10.1121/1.3257588}
}
@article{Mac_Aodha_2018,
	author       = {Mac Aodha, Oisin and Gibb, Rory and Barlow, Kate E. and Browning, Ella and Firman, Michael and Freeman, Robin and Harder, Briana and Kinsey, Libby and Mead, Gary R. and Newson, Stuart E. and Pandourski, Ivan and Parsons, Stuart and Russ, Jon and Szodoray-Paradi, Abigel and Szodoray-Paradi, Farkas and Tilova, Elena and Girolami, Mark and Brostow, Gabriel and Jones, Kate E.},
	year         = {2018},
	title        = {Bat detective—Deep learning tools for bat acoustic signal detection},
	month        = {03},
	journal      = {PLOS Computational Biology},
	publisher    = {Public Library of Science (PLoS)},
	volume       = 14,
	number       = 3,
	pages        = {e1005995},
	doi          = {10.1371/journal.pcbi.1005995},
	issn         = {1553-7358},
	url          = {http://dx.doi.org/10.1371/journal.pcbi.1005995},
	editor       = {Fenton, Brock}
}
@inproceedings{10617409,
	author       = {S, Salini and K, Suresh},
	year         = {2024},
	title        = {BAT-CNN: BirdNet Assisted Training for CNN},
	booktitle    = {2024 International Conference on Advancements in Power, Communication and Intelligent Systems (APCI)},
	volume       = {},
	number       = {},
	pages        = {1--5},
	doi          = {10.1109/APCI61480.2024.10617409},
	keywords     = {Training;Deep learning;Accuracy;Biological system modeling;Sociology;Birds;Audio recording;BAT-CNN;BirdNet;Bird Sound Classification;MelSpectrogram;Xeno-Canto}
}
@article{Clink2024.08.17.608420,
	author       = {Clink, Dena J. and Cross-Jaya, Hope and Kim, Jinsung and Ahmad, Abdul Hamid and Hong, Moeurk and Sala, Roeun and Birot, H{\'e}l{\`e}ne and Agger, Cain and Vu, Thinh Tien and Thi, Hoa Nguyen and Chi, Thanh Nguyen and Klinck, Holger},
	year         = {2024},
	title        = {Benchmarking automated detection and classification approaches for monitoring of endangered species: a case study on gibbons from Cambodia},
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.08.17.608420},
	url          = {https://www.biorxiv.org/content/early/2024/08/22/2024.08.17.608420},
	elocation-id = {2024.08.17.608420},
	abstract     = {Recent advances in deep and transfer learning have revolutionized our ability for the automated detection and classification of acoustic signals from long-term recordings. Here, we provide a benchmark for the automated detection of southern yellow-cheeked crested gibbon (Nomascus gabriellae) calls collected using autonomous recording units (ARUs) in Andoung Kraleung Village, Cambodia. We compared the performance of support vector machines (SVMs), a quasi-DenseNet architecture (Koogu), transfer learning with pretrained convolutional neural network (ResNet50) models trained on the {\textquoteleft}ImageNet{\textquoteright} dataset, and transfer learning with embeddings from a global birdsong model (BirdNET) based on an EfficientNet architecture. We also investigated the impact of varying the number of training samples on the performance of these models. We found that BirdNET had superior performance with a smaller number of training samples, whereas Koogu and ResNet50 models only had acceptable performance with a larger number of training samples (\&gt;200 gibbon samples). Effective automated detection approaches are critical for monitoring endangered species, like gibbons. It is unclear how generalizable these results are for other signals, and future work on other vocal species will be informative. Code and data are publicly available for future benchmarking.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2024/08/22/2024.08.17.608420.full.pdf}
}
@article{Enari_2023,
	author       = {Enari, Hiroto and Enari, Haruka S.},
	year         = {2023},
	title        = {Bioacoustic monitoring to determine addiction levels of primates to the human sphere: A feasibility study on Japanese macaques},
	month        = {10},
	journal      = {American Journal of Primatology},
	publisher    = {Wiley},
	volume       = 85,
	number       = 12,
	doi          = {10.1002/ajp.23558},
	issn         = {1098-2345},
	url          = {http://dx.doi.org/10.1002/ajp.23558}
}
@misc{Farina_2017,
	author       = {Farina, Almo and Pieretti, Nadia},
	year         = {2017},
	title        = {Biodiversity Assessment in Temperate Biomes using Ecoacoustics},
	month        = {05},
	journal      = {Ecoacoustics},
	publisher    = {Wiley},
	pages        = {109–127},
	doi          = {10.1002/9781119230724.ch7},
	isbn         = 9781119230724,
	url          = {http://dx.doi.org/10.1002/9781119230724.ch7}
}
@article{Akamatsu_2005,
	author       = {Akamatsu, Tomonari and Wang, Ding and Wang, Kexiong and Naito, Yasuhiko},
	year         = {2005},
	title        = {Biosonar behaviour of free-ranging porpoises},
	month        = {04},
	journal      = {Proceedings of the Royal Society B: Biological Sciences},
	publisher    = {The Royal Society},
	volume       = {272},
	number       = 1565,
	pages        = {797–801},
	doi          = {10.1098/rspb.2004.3024},
	issn         = {1471-2954},
	url          = {http://dx.doi.org/10.1098/rspb.2004.3024}
}
@article{van_Merri_nboer_2024,
	author       = {van Merriënboer, Bart and Hamer, Jenny and Dumoulin, Vincent and Triantafillou, Eleni and Denton, Tom},
	year         = {2024},
	title        = {Birds, bats and beyond: evaluating generalization in bioacoustics models},
	month        = {07},
	journal      = {Frontiers in Bird Science},
	publisher    = {Frontiers Media SA},
	volume       = 3,
	doi          = {10.3389/fbirs.2024.1369756},
	issn         = {2813-3870},
	url          = {http://dx.doi.org/10.3389/fbirs.2024.1369756}
}
@article{Vieira2015,
	author       = {Vieira,  Manuel and Fonseca,  Paulo J. and Amorim,  M. Clara P. and Teixeira,  Carlos J. C.},
	year         = {2015},
	title        = {Call recognition and individual identification of fish vocalizations based on automatic speech recognition: An example with the Lusitanian toadfish},
	month        = 12,
	journal      = {The Journal of the Acoustical Society of America},
	publisher    = {Acoustical Society of America (ASA)},
	volume       = 138,
	number       = 6,
	pages        = {3941--3950},
	doi          = {10.1121/1.4936858},
	issn         = {1520-8524},
	url          = {http://dx.doi.org/10.1121/1.4936858}
}
@article{Guo_2024,
	author       = {Guo, Huimin and Jian, Haifang and Wang, Yiyu and Wang, Hongchang and Zheng, Shuaikang and Cheng, Qinghua and Li, Yuehao},
	year         = {2024},
	title        = {CDPNet: conformer-based dual path joint modeling network for bird sound recognition},
	month        = {02},
	journal      = {Applied Intelligence},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 54,
	number       = 4,
	pages        = {3152-3168},
	doi          = {10.1007/s10489-024-05362-9},
	issn         = {1573-7497},
	url          = {http://dx.doi.org/10.1007/s10489-024-05362-9}
}
@mastersthesis{Weirathmueller2017b,
	author       = {Weirathmueller, M. J.},
	year         = {2017},
	title        = {Characteristics of fin whale vocalizations recorded on instruments in the northeast Pacific Ocean},
	url          = {http://hdl.handle.net/1773/38189},
	school       = {University of Washington},
	type         = {phdPh.D.}
}
@article{Sun_2022,
	author       = {Sun, Yuren and Midori Maeda, Tatiana and Solís-Lemus, Claudia and Pimentel-Alarcón, Daniel and Buřivalová, Zuzana},
	year         = {2022},
	title        = {Classification of animal sounds in a hyperdiverse rainforest using convolutional neural networks with data augmentation},
	month        = {12},
	journal      = {Ecological Indicators},
	publisher    = {Elsevier BV},
	volume       = 145,
	pages        = 109621,
	doi          = {10.1016/j.ecolind.2022.109621},
	issn         = {1470-160X},
	url          = {http://dx.doi.org/10.1016/j.ecolind.2022.109621}
}
@inbook{Stowell_2017,
	author       = {Stowell, Dan},
	year         = {2017},
	title        = {Computational Bioacoustic Scene Analysis},
	month        = {09},
	booktitle    = {Computational Analysis of Sound Scenes and Events},
	publisher    = {Springer International Publishing},
	pages        = {303-333},
	doi          = {10.1007/978-3-319-63450-0_11},
	isbn         = 9783319634500,
	url          = {http://dx.doi.org/10.1007/978-3-319-63450-0_11}
}
@inproceedings{8203533,
	author       = {Lin, Tzu-Hao and Tsao, Yu and Wang, Yu-Huang and Yen, Han-Wei and Lu, Sheng-Shan},
	year         = {2017},
	title        = {Computing biodiversity change via a soundscape monitoring network},
	booktitle    = {2017 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC)},
	volume       = {},
	number       = {},
	pages        = {128--133},
	doi          = {10.23919/PNC.2017.8203533},
	keywords     = {Biodiversity;Monitoring;Animals;Transient analysis;Large Hadron Collider;Acoustics;Biodiversity assessment;soundscape;biological sounds;machine learning;open data}
}
@article{jimaging8040096,
	author       = {Trapanotto, Martino and Nanni, Loris and Brahnam, Sheryl and Guo, Xiang},
	year         = {2022},
	title        = {Convolutional Neural Networks for the Identification of African Lions from Individual Vocalizations},
	journal      = {Journal of Imaging},
	volume       = 8,
	number       = 4,
	doi          = {10.3390/jimaging8040096},
	issn         = {2313-433X},
	url          = {https://www.mdpi.com/2313-433X/8/4/96},
	article-number = 96,
	pubmedid     = 35448223,
	abstract     = {The classification of vocal individuality for passive acoustic monitoring (PAM) and census of animals is becoming an increasingly popular area of research. Nearly all studies in this field of inquiry have relied on classic audio representations and classifiers, such as Support Vector Machines (SVMs) trained on spectrograms or Mel-Frequency Cepstral Coefficients (MFCCs). In contrast, most current bioacoustic species classification exploits the power of deep learners and more cutting-edge audio representations. A significant reason for avoiding deep learning in vocal identity classification is the tiny sample size in the collections of labeled individual vocalizations. As is well known, deep learners require large datasets to avoid overfitting. One way to handle small datasets with deep learning methods is to use transfer learning. In this work, we evaluate the performance of three pretrained CNNs (VGG16, ResNet50, and AlexNet) on a small, publicly available lion roar dataset containing approximately 150 samples taken from five male lions. Each of these networks is retrained on eight representations of the samples: MFCCs, spectrogram, and Mel spectrogram, along with several new ones, such as VGGish and stockwell, and those based on the recently proposed LM spectrogram. The performance of these networks, both individually and in ensembles, is analyzed and corroborated using the Equal Error Rate and shown to surpass previous classification attempts on this dataset; the best single network achieved over 95% accuracy and the best ensembles over 98% accuracy. The contributions this study makes to the field of individual vocal classification include demonstrating that it is valuable and possible, with caution, to use transfer learning with single pretrained CNNs on the small datasets available for this problem domain. We also make a contribution to bioacoustics generally by offering a comparison of the performance of many state-of-the-art audio representations, including for the first time the LM spectrogram and stockwell representations. All source code for this study is available on GitHub.}
}
@article{Schall_2024,
	author       = {Schall, Elena and Kaya, Idil Ilgaz and Debusschere, Elisabeth and Devos, Paul and Parcerisas, Clea},
	year         = {2024},
	title        = {Deep learning in marine bioacoustics: a benchmark for baleen whale detection},
	month        = {04},
	journal      = {Remote Sensing in Ecology and Conservation},
	publisher    = {Wiley},
	volume       = 10,
	number       = 5,
	pages        = {642–654},
	doi          = {10.1002/rse2.392},
	issn         = {2056-3485},
	url          = {http://dx.doi.org/10.1002/rse2.392}
}
@article{Shiu2020,
	author       = {Shiu,  Yu and Palmer,  K. J. and Roch,  Marie A. and Fleishman,  Erica and Liu,  Xiaobai and Nosal,  Eva-Marie and Helble,  Tyler and Cholewiak,  Danielle and Gillespie,  Douglas and Klinck,  Holger},
	year         = {2020},
	title        = {Deep neural networks for automated detection of marine mammal species},
	month        = {01},
	journal      = {Scientific Reports},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 10,
	number       = 1,
	doi          = {10.1038/s41598-020-57549-y},
	issn         = {2045-2322},
	url          = {http://dx.doi.org/10.1038/s41598-020-57549-y}
}
@inbook{Oswald_2022,
	author       = {Oswald, Julie N. and Erbe, Christine and Gannon, William L. and Madhusudhana, Shyam and Thomas, Jeanette A.},
	year         = {2022},
	title        = {Detection and Classification Methods for Animal Sounds},
	booktitle    = {Exploring Animal Behavior Through Sound: Volume 1},
	publisher    = {Springer International Publishing},
	pages        = {269-317},
	doi          = {10.1007/978-3-030-97540-1_8},
	isbn         = 9783030975401,
	url          = {http://dx.doi.org/10.1007/978-3-030-97540-1_8}
}
@techreport{nerc533486,
	author       = {D.B. Roy and C. Abrahams and T. August and J. Christelow and F. Gerard and K. Howell and M. Logie and M. McCracken and D. Pallett and M. Pocock and D.S. Read and D. Sadykova and J.T. Staley},
	year         = {2022},
	title        = {Developing technologies for Agri-environment monitoring},
	month        = {01},
	publisher    = {UK Centre for Ecology \& Hydrology},
	address      = {Wallingford, UK},
	url          = {http://nora.nerc.ac.uk/id/eprint/533486/},
	note         = {Freely available via Official URL link.},
	type         = {Project Report},
	abstract     = {
		Overview of the project

		Approximately {\pounds}400 million is invested annually in agri environment schemes in England, designed to compensate farmers for loss of production (income foregone) and additional costs, to meet environmental objectives. A further {\pounds}1.8 billion in subsidies is paid to comply with environmental conditions of cross compliance and greening. In December 2019, it was estimated there were ca. 50,000 agri-environment agreements covering 1.8 million hectares.

		Current and previous monitoring of agri-environment schemes is diverse and varied, and conducted at scales of specific management options, whole AES agreements and 1 km squares covering multiple agreements. The Monitoring and Evaluation Programme of these schemes aims to deliver evidence to achieve the following: ? Evaluate the delivery of agri-environment schemes and their effectiveness in achieving their intended policy objectives; ? Inform current and future agri-environment policy, scheme delivery and development; ? Fulfil domestic and European reporting requirements.

		The existing Monitoring and Evaluation Programme (2015-2020) for Agri-environment schemes (AES) in England includes four elements: integrated monitoring, landscape scale, thematic, and evaluation and synthesis.

		To date, monitoring has been primarily based on tried-and-tested approaches, which provide widely recognised metrics of quality, condition and species-resolution biodiversity data, and that have counterfactuals often derived from national monitoring schemes to enable evaluation of AES. Developing technologies will need to integrate with existing tried-and-tested approaches but offer huge potential for more extensive and efficient evaluation of AES. In this project we focus on technologies with greatest potential to deliver enhanced and more cost-effective monitoring of environmental ?outcomes? of direct land management and interventions, in support of the Environmental Land Management Scheme (ELMS). The technology areas covered by this project are: bioacoustics, DNA-methods, Earth Observation, computer vision and machine learning.

		For each technology area, the objectives were to: ? Carry out horizon scanning of developing technologies which may be relevant for agrienvironment monitoring; ? Review how these developing technologies could be used for Agri-environment monitoring; ? Review what outputs/outcomes are relevant for the developing technology; ? Propose how developing technologies could be used in existing and future agrienvironment monitoring; ? Pilot or proof of concept studies to demonstrate how technology could be used for agrienvironment monitoring.
	}
}
@article{Bartrina_2024,
	author       = {Bartrina, Carme and Llanos-Guerrero, César and Valls, Núria and Freixas, Lídia and López-Baucells, Adrià},
	year         = {2024},
	title        = {Diversity and plasticity of vocalisations in an elusive and arboreal small mammal: the edible dormouse ( Glis glis )},
	month        = {07},
	journal      = {Bioacoustics},
	publisher    = {Informa UK Limited},
	volume       = 33,
	number       = 4,
	pages        = {332-353},
	doi          = {10.1080/09524622.2024.2353651},
	issn         = {2165-0586},
	url          = {http://dx.doi.org/10.1080/09524622.2024.2353651}
}
@article{Elie_2011,
	author       = {Elie, Julie E. and Soula, Hédi A. and Mathevon, Nicolas and Vignal, Clémentine},
	year         = {2011},
	title        = {Dynamics of communal vocalizations in a social songbird, the zebra finch (Taeniopygia guttata)},
	month        = {06},
	journal      = {The Journal of the Acoustical Society of America},
	publisher    = {Acoustical Society of America (ASA)},
	volume       = 129,
	number       = 6,
	pages        = {4037-4046},
	doi          = {10.1121/1.3570959},
	issn         = {1520-8524},
	url          = {http://dx.doi.org/10.1121/1.3570959}
}
@article{Sanchez_2017,
	author       = {Sanchez, Lida and Moreno, Christian R. and Mora, Emanuel C.},
	year         = {2017},
	title        = {Echolocation calls ofNatalus primus(Chiroptera: Natalidae): Implications for conservation monitoring of this species},
	month        = {01},
	journal      = {Cogent Biology},
	publisher    = {Informa UK Limited},
	volume       = 3,
	number       = 1,
	pages        = 1355027,
	doi          = {10.1080/23312025.2017.1355027},
	issn         = {2331-2025},
	url          = {http://dx.doi.org/10.1080/23312025.2017.1355027},
	editor       = {Burda, Hynek}
}
@article{Farina2022,
	author       = {Farina,  Almo and Eldridge,  Alice and Fuller,  Susan and Pavan,  Gianni},
	year         = {2022},
	title        = {Editorial: Advances in ecoacoustics},
	month        = {07},
	journal      = {Frontiers in Ecology and Evolution},
	publisher    = {Frontiers Media SA},
	volume       = 10,
	doi          = {10.3389/fevo.2022.978516},
	issn         = {2296-701X},
	url          = {http://dx.doi.org/10.3389/fevo.2022.978516}
}
@inproceedings{Solomes_2020,
	author       = {Solomes, Alexandru-Marius and Stowell, Dan},
	year         = {2020},
	title        = {Efficient Bird Sound Detection on the Bela Embedded System},
	month        = {05},
	booktitle    = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	publisher    = {IEEE},
	pages        = {746-750},
	doi          = {10.1109/icassp40776.2020.9053533},
	url          = {http://dx.doi.org/10.1109/ICASSP40776.2020.9053533}
}
@article{Sheard_2024,
	author       = {Sheard, Julie Koch and Adriaens, Tim and Bowler, Diana E. and Büermann, Andrea and Callaghan, Corey T. and Camprasse, Elodie C. M. and Chowdhury, Shawan and Engel, Thore and Finch, Elizabeth A. and von Gönner, Julia and Hsing, Pen-Yuan and Mikula, Peter and Rachel Oh, Rui Ying and Peters, Birte and Phartyal, Shyam S. and Pocock, Michael J. O. and Wäldchen, Jana and Bonn, Aletta},
	year         = {2024},
	title        = {Emerging technologies in citizen science and potential for insect monitoring},
	month        = {05},
	journal      = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	publisher    = {The Royal Society},
	volume       = 379,
	number       = 1904,
	doi          = {10.1098/rstb.2023.0106},
	issn         = {1471-2970},
	url          = {http://dx.doi.org/10.1098/rstb.2023.0106}
}
@techreport{Copping2012,
	author       = {Copping, Andrea E. and Hanna, Luke A. and Butner, R. Scott and Carlson, Thomas J. and Halvorsen, Michele B. and Duberstein, Corey A. and Matzner, Shari},
	year         = {2012},
	title        = {Environmental Effects of Offshore Wind Development. Fiscal Year 2012 Progress Report},
	address      = {Richland, WA},
	number       = {PNNL-21852},
	url          = {https://www.pnnl.gov/publications/environmental-effects-offshore-wind-development-fiscal-year-2012-progress-report},
	institution  = {Pacific Northwest National Laboratory}
}
@article{Marques_2012,
	author       = {Marques, Tiago A. and Thomas, Len and Martin, Stephen W. and Mellinger, David K. and Ward, Jessica A. and Moretti, David J. and Harris, Danielle and Tyack, Peter L.},
	year         = {2012},
	title        = {Estimating animal population density using passive acoustics},
	month        = {11},
	journal      = {Biological Reviews},
	publisher    = {Wiley},
	volume       = 88,
	number       = {2},
	pages        = {287-309},
	doi          = {10.1111/brv.12001},
	issn         = {1469-185X},
	url          = {http://dx.doi.org/10.1111/brv.12001}
}
@article{Hutschenreiter_2023,
	author       = {Hutschenreiter, Anja and Sosa-López, J. Roberto and González-García, Fernando and Aureli, Filippo},
	year         = {2023},
	title        = {Evaluating factors affecting species detection using passive acoustic monitoring in neotropical forests: a playback experiment},
	month        = {08},
	journal      = {Bioacoustics},
	publisher    = {Informa UK Limited},
	volume       = 32,
	number       = 6,
	pages        = {660-678},
	doi          = {10.1080/09524622.2023.2246413},
	issn         = {2165-0586},
	url          = {http://dx.doi.org/10.1080/09524622.2023.2246413}
}
@article{https://doi.org/10.17192/z2023.0087,
	author       = {Gottwald,  Jannis},
	year         = {2022},
	title        = {Expert-driven development of conservation technologies to close knowledge gaps in small animal research},
	publisher    = {Philipps-Universit\"{a}t Marburg},
	doi          = {10.17192/Z2023.0087},
	url          = {https://archiv.ub.uni-marburg.de/diss/z2023/0087},
	copyright    = {Creative Commons Attribution Share Alike 4.0 International},
	keywords     = {Flederm\"{a}use,  Automatisiertes Radiotelemetriesystem,  Nyctalus leisle,  Geography &amp; travel,  Geografie,  Reisen,  behaviour,  Verhalten,  Maschinelles Lernen,  V\"{o}gel,  Automated radio-telemetry system,  birds,  Generalized Additive Models,  machine learning,  Nyctalus leisleri,  random forest,  Random Forest,  Myotis bech,  bats},
	language     = {en}
}
@article{Jordan2024,
	author       = {Jordan,  Celine and Markolf,  Matthias},
	year         = {2024},
	title        = {Exploring the potential of occupancy modelling using passive acoustics in &lt;i&gt;Coua gigas&lt;/i&gt; and &lt;i&gt;Coua coquereli&lt;/i&gt;},
	month        = {01},
	journal      = {Madagascar Conservation &amp; Development},
	publisher    = {African Journals Online (AJOL)},
	volume       = 18,
	number       = 1,
	pages        = {39--47},
	doi          = {10.4314/mcd.v18i1.6},
	issn         = {1662-2510},
	url          = {http://dx.doi.org/10.4314/mcd.v18i1.6}
}
@phdthesis{Fleischer2000,
	author       = {Fleischer, Jürgen},
	year         = {2000},
	title        = {Fundamentals on the Miniaturization of Sheet Metal Working Processes},
	url          = {https://open.fau.de/handle/openfau/13646},
	school       = {Friedrich-Alexander-Universität Erlangen-Nürnberg}
}
@phdthesis{VanCise2017,
	author       = {Van Cise, Amy M.},
	year         = {2017},
	title        = {Gene-culture coevolution in a social cetacean: integrating acoustic and genetic data to understand population structure in the short-finned pilot whale (Globicephala macrorhynchus)},
	url          = {https://escholarship.org/uc/item/050004gv},
	school       = {University of California, San Diego}
}
@article{Marck_2022,
	author       = {Marck, Aya and Vortman, Yoni and Kolodny, Oren and Lavner, Yizhar},
	year         = {2022},
	title        = {Identification, Analysis and Characterization of Base Units of Bird Vocal Communication: The White Spectacled Bulbul (Pycnonotus xanthopygos) as a Case Study},
	month        = {02},
	journal      = {Frontiers in Behavioral Neuroscience},
	publisher    = {Frontiers Media SA},
	volume       = 15,
	doi          = {10.3389/fnbeh.2021.812939},
	issn         = {1662-5153},
	url          = {http://dx.doi.org/10.3389/fnbeh.2021.812939}
}
@article{Madhusudhana_2021,
	author       = {Madhusudhana, Shyam and Shiu, Yu and Klinck, Holger and Fleishman, Erica and Liu, Xiaobai and Nosal, Eva-Marie and Helble, Tyler and Cholewiak, Danielle and Gillespie, Douglas and Širović, Ana and Roch, Marie A.},
	year         = {2021},
	title        = {Improve automatic detection of animal call sequences with temporal context},
	month        = {07},
	journal      = {Journal of The Royal Society Interface},
	publisher    = {The Royal Society},
	volume       = 18,
	number       = 180,
	pages        = {20210297},
	doi          = {10.1098/rsif.2021.0297},
	issn         = {1742-5662},
	url          = {http://dx.doi.org/10.1098/rsif.2021.0297}
}
@article{JEANTET2023102256,
	author       = {Lorène Jeantet and Emmanuel Dufourq},
	year         = {2023},
	title        = {Improving deep learning acoustic classifiers with contextual information for wildlife monitoring},
	journal      = {Ecological Informatics},
	volume       = 77,
	pages        = 102256,
	doi          = {https://doi.org/10.1016/j.ecoinf.2023.102256},
	issn         = {1574-9541},
	url          = {https://www.sciencedirect.com/science/article/pii/S1574954123002856},
	keywords     = {Bioacoustics, Deep learning, Convolutional neural networks, Passive acoustic monitoring, Species identification, Birds, Hainan gibbons},
	abstract     = {Bioacoustics, the exploration of animal vocalizations and natural soundscapes, has emerged as a valuable tool for studying species within their habitats, particularly those that are challenging to observe. This approach has broadened the horizons of biodiversity assessment and ecological research. However, monitoring wildlife with acoustic recorders produces large volumes of data that can be labor-intensive to analyze. Deep learning has recently transformed many computational disciplines by enabling the automated processing of large and complex datasets and has gained attention within the bioacoustics community. Despite the revolutionary impact of deep learning on acoustic detection and classification, attaining both high detection accuracy and low false positive rates in bioacoustics remains a significant challenge. An intriguing yet unexplored avenue for enhancing deep learning in bioacoustics involves the utilization of contextual information, such as time and location, to discern animal vocalizations within acoustic recordings. As a first case study, a multi-branch Convolutional Neural Network (CNN) was developed to classify 22 different bird songs using spectrograms as a first input, and spatial metadata as a secondary input. A comparison was made to a baseline model with only spectrogram input. A geographical prior neural network was trained, separately, to estimate the probability of a species occurring at a given location. The output of this network was combined with the baseline CNN. As a second case study, temporal data and spectrograms were used as input to a multi-branch CNN for the detection of Hainan gibbon (Nomascus hainanus) calls, the world’s rarest primate. Our findings demonstrate that adding metadata to the bird song classifier significantly improves classification performance, with the highest improvement achieved using the geographical prior model (F1-score of 87.78% compared to 61.02% for the baseline model). The multi-branch CNNs also proved efficient (F1-scores of 76.87% and 78.77%) and simpler to use than the geographical prior. In the second case study, our findings revealed a decrease in false positives by 63% (94% of the calls were detected) when the metadata was used by the multi-branch CNN, and an increase of 19% in gibbon detection. This study has uncovered an exciting new avenue for improving classifier performance in bioacoustics. The methodology described in this study can assist ecologists, wildlife management teams, and researchers in reducing the amount of time spent analyzing large acoustic datasets obtained from passive acoustic monitoring studies. Our approach can be adapted and applied to other calling species, and thus tailored to other use cases.}
}
@article{Arnaud2022.06.26.497684,
	author       = {Arnaud, Vincent and Pellegrino, Fran{\c c}ois and Keenan, Sumir and St-Gelais, Xavier and Mathevon, Nicolas and Levr{\'e}ro, Florence and Coup{\'e}, Christophe},
	year         = {2022},
	title        = {Improving the workflow to crack Small, Unbalanced, Noisy, but Genuine (SUNG) datasets in bioacoustics: the case of bonobo calls},
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2022.06.26.497684},
	url          = {https://www.biorxiv.org/content/early/2022/06/29/2022.06.26.497684},
	elocation-id = {2022.06.26.497684},
	abstract     = {Despite the accumulation of data and studies, deciphering animal vocal communication remains highly challenging. While progress has been made with some species for which we now understand the information exchanged through vocal signals, researchers are still left struggling with sparse recordings composing Small, Unbalanced, Noisy, but Genuine (SUNG) datasets. SUNG datasets offer a valuable but distorted vision of communication systems. Adopting the best practices in their analysis is therefore essential to effectively extract the available information and draw reliable conclusions. Here we show that the most recent advances in machine learning applied to a SUNG dataset succeed in unraveling the complex vocal repertoire of the bonobo, and we propose a workflow that can be effective with other animal species. We implement acoustic parameterization in three feature spaces along with three classification algorithms (Support Vector Machine, xgboost, neural networks) and their combination to explore the structure and variability of bonobo calls, as well as the robustness of the individual signature they encode. We underscore how classification performance is affected by the feature set and identify the most informative features. We highlight the need to address data leakage in the evaluation of classification performance to avoid misleading interpretations. Finally, using a Uniform Manifold Approximation and Projection (UMAP), we show that classifiers generate parsimonious data descriptions which help to understand the clustering of the bonobo acoustic space. Our results lead to identifying several practical approaches that are generalizable to any other animal communication system. To improve the reliability and replicability of vocal communication studies with SUNG datasets, we thus recommend: i) comparing several acoustic parameterizations; ii) adopting Support Vector Machines as the baseline classification approach; iii) explicitly evaluating data leakage and possibly implementing a mitigation strategy; iv) visualizing the dataset with UMAPs applied to classifier predictions rather than to raw acoustic features.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2022/06/29/2022.06.26.497684.full.pdf}
}
@article{Jones2020,
	author       = {Jones,  Brittany L and Oswald,  Michael and Tufano,  Samantha and Baird,  Mark and Ridgway,  Sam H.},
	year         = {2020},
	title        = {Introducing NMMF WAMS,  an open-source PAMGuard plug-in,  and some pilot data for its use as a welfare acoustic monitoring system},
	month        = {08},
	publisher    = {Research Square Platform LLC},
	doi          = {10.21203/rs.3.rs-55999/v2},
	url          = {http://dx.doi.org/10.21203/rs.3.rs-55999/v2}
}
@inbook{Murugaiya_2021,
	author       = {Murugaiya, Ramashini and Mahagammulle Gamage, Manisha Milani and Murugiah, Krishani and Perumal, Madhumathy},
	year         = {2021},
	title        = {Introduction to Applications on Vertebrate Vocalisation},
	month        = {12},
	booktitle    = {Acoustic-Based Applications for Vertebrate Vocalization},
	publisher    = {Springer International Publishing},
	pages        = {1-18},
	doi          = {10.1007/978-3-030-85773-8_1},
	isbn         = 9783030857738,
	issn         = {2191-5318},
	url          = {http://dx.doi.org/10.1007/978-3-030-85773-8_1}
}
@article{make6040115,
	author       = {Garcia, Tiago and Pina, Luís and Robb, Magnus and Maria, Jorge and May, Roel and Oliveira, Ricardo},
	year         = {2024},
	title        = {Long-Range Bird Species Identification Using Directional Microphones and CNNs},
	journal      = {Machine Learning and Knowledge Extraction},
	volume       = 6,
	number       = 4,
	pages        = {2336--2354},
	doi          = {10.3390/make6040115},
	issn         = {2504-4990},
	url          = {https://www.mdpi.com/2504-4990/6/4/115},
	abstract     = {This study explores the integration of directional microphones with convolutional neural networks (CNNs) for long-range bird species identification. By employing directional microphones, we aimed to capture high-resolution audio from specific directions, potentially improving the clarity of bird calls over extended distances. Our approach involved processing these recordings with CNNs trained on a diverse dataset of bird calls. The results demonstrated that the system is capable of systematically identifying bird species up to 150 m, reaching 280 m for species vocalizing at frequencies greater than 1000 Hz and clearly distinct from background noise. The furthest successful detection was obtained at 510 m. While the method showed promise in enhancing the identification process compared to traditional techniques, there were notable limitations in the clarity of the audio recordings. These findings suggest that while the integration of directional microphones and CNNs for long-range bird species identification is promising, further refinement is needed to fully realize the benefits of this approach. Future efforts should focus on improving the audio-capture technology to reduce ambient noise and enhance the system’s overall performance in long-range bird species identification.}
}
@article{Caruso2017,
	author       = {Caruso,  Francesco and Alonge,  Giuseppe and Bellia,  Giorgio and De Domenico,  Emilio and Grammauta,  Rosario and Larosa,  Giuseppina and Mazzola,  Salvatore and Riccobene,  Giorgio and Pavan,  Gianni and Papale,  Elena and Pellegrino,  Carmelo and Pulvirenti,  Sara and Sciacca,  Virginia and Simeone,  Francesco and Speziale,  Fabrizio and Viola,  Salvatore and Buscaino,  Giuseppa},
	year         = {2017},
	title        = {Long-Term Monitoring of Dolphin Biosonar Activity in Deep Pelagic Waters of the Mediterranean Sea},
	month        = {06},
	journal      = {Scientific Reports},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 7,
	number       = 1,
	doi          = {10.1038/s41598-017-04608-6},
	issn         = {2045-2322},
	url          = {http://dx.doi.org/10.1038/s41598-017-04608-6}
}
@article{Munger2022,
	author       = {Munger,  JE and Herrera,  DP and Haver,  SM and Waterhouse,  L and McKenna,  MF and Dziak,  RP and Gedamke,  J and Heppell,  SA and Haxel,  JH},
	year         = {2022},
	title        = {Machine learning analysis reveals relationship between pomacentrid calls and environmental cues},
	month        = {01},
	journal      = {Marine Ecology Progress Series},
	publisher    = {Inter-Research Science Center},
	volume       = 681,
	pages        = {197--210},
	doi          = {10.3354/meps13912},
	issn         = {1616-1599},
	url          = {http://dx.doi.org/10.3354/meps13912}
}
@article{ZHOU2023110908,
	author       = {Xiaotao Zhou and Kunrong Hu and Zhenhua Guan and Chunjiang Yu and Shuai Wang and Meng Fan and Yongke Sun and Yong Cao and Yijie Wang and Guangting Miao},
	year         = {2023},
	title        = {Methods for processing and analyzing passive acoustic monitoring data: An example of song recognition in western black-crested gibbons},
	journal      = {Ecological Indicators},
	volume       = 155,
	pages        = 110908,
	doi          = {https://doi.org/10.1016/j.ecolind.2023.110908},
	issn         = {1470-160X},
	url          = {https://www.sciencedirect.com/science/article/pii/S1470160X23010506},
	keywords     = {Western black-crested gibbons song recognition, Passive acoustic monitoring, Bioacoustics, Deep learning, Attentional mechanisms},
	abstract     = {The extraction of species-specific calls from passive acoustic recordings is a common preliminary step in ecological analysis. But for many species, especially those occupying noisy, acoustically variable habitats, the call extraction process remains largely manual, a time-consuming, and increasingly unsustainable process. Deep neural networks have been shown to provide excellent performance in a range of acoustic classification applications. We take as an example the recognition of four songs of one of the rarest mammals in the world, the western black-crested gibbon (Nomascus concolors).The process of recognition in this paper which includes distributed BIC ambient sound segmentation based on the OpenPAI platform; DNN-based western black-crested gibbon song enhancement processing; data pre-processing, labeling samples; proposed DNN + ResNet34 + CBAM + GRU + Attention recognition model; comparing other classical neural network models.Our best model converts acoustic recordings into spectrogram images on the mel frequency scale and uses these images to train convolutional neural networks.Our proposed model proved to be very accurate in predicting the segmented western black-crested gibbon songs with an accuracy of 99.8%, and almost a few western black-crested gibbon songs were incorrectly identified when all segmented data were recognized. In the four consecutive years of the acoustic monitoring system deployed in the Ailao Mountain National Nature Reserve, the western black-crested gibbon was most active near the monitoring site from March to August each year, and least active in January and February. Based on call sound intensity analysis, we monitored a total of two different western black-crested gibbon groups (G1 and G2) during the monitoring cycle. We demonstrate that passive acoustic monitoring combined with CNN classifiers is an effective tool for the remote detection of one of the rarest and most threatened species in the world.}
}
@article{Risch2013,
	author       = {Risch,  D and Clark,  CW and Dugan,  PJ and Popescu,  M and Siebert,  U and Van Parijs,  SM},
	year         = {2013},
	title        = {Minke whale acoustic behavior and multi-year seasonal and diel vocalization patterns in Massachusetts Bay,  USA},
	month        = {08},
	journal      = {Marine Ecology Progress Series},
	publisher    = {Inter-Research Science Center},
	volume       = 489,
	pages        = {279-295},
	doi          = {10.3354/meps10426},
	issn         = {1616-1599},
	url          = {http://dx.doi.org/10.3354/meps10426}
}
@phdthesis{katz15,
	author       = {Katz,Jonathan},
	year         = {2015},
	title        = {monitoR: Automation Tools For Landscape-scale Acoustic Monitoring},
	journal      = {ProQuest Dissertations and Theses},
	pages        = {212},
	isbn         = {978-1-321-55206-5},
	url          = {https://may.idm.oclc.org/login?url=https://www.proquest.com/dissertations-theses/monitor-automation-tools-landscape-scale-acoustic/docview/1656497151/se-2},
	note         = {Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2023-03-04},
	abstract     = {Climate change coupled with land-use change will likely alter habitats and affect state parameters of the animal populations that dwell in them. Affected parameters are anticipated to include site occupancy and abundance, population range, and phenophase cycles (e.g., arrival dates on breeding grounds for migrant bird species). Detecting these changes will require monitoring many sites for many years, a process that is well suited for an automated system. We developed and tested monitoR, an R package that is designed for long-term, multi-taxa automated passive acoustic monitoring programs. monitoR correctly identified presence for black-throated green warbler and ovenbird in 64% and 72% of the 52 surveys using binary point matching, respectively, and 73% and 72% of the 52 surveys using spectrogram cross-correlation, respectively. Of individual black-throated green warbler song events, 73% of 166 black-throated green warbler songs and 69% of 502 ovenbird songs were identified by binary point matching. Spectrogram cross correlation identified 64% of 166 black-throated green warbler songs and 64% of 502 ovenbird songs. False positive rates were less 1% for song event detection. We describe a method to identify the probability of survey presence in a template-based automated detection system using known false positive rates for each template. True and false positive detection rates were observed in 146 training surveys. These probabilities were used in a Bayesian approach that discriminates between detections in occupied surveys and unoccupied surveys. We evaluated this approach in 146 test surveys. A total of 1142 Black-throated green warbler (Setophaga virens) songs were observed in the training surveys and test surveys, which we attempted to locate with 3 different binary point matching templates. When only posterior probabilities greater than 0.5 were considered detections, the average ratio of accurate identifications of survey presence to false positive identifications in 500 bootstrapped samples improved from 1.2:1 using a standard score cutoff approach to 2.8:1 using all 3 templates and a likelihood-based discriminator. With the selected score cutoffs the average true positive and false positive rates for the combined three templates were 0.18 and 0.002, respectively. Automated detection methods are increasingly being used for identification and monitoring of landscape-scale responses to climate change and land-use change. Skepticism of automated acoustic monitoring software is largely due to higher false positive and negative error rates than those in traditional human surveys, but the false positive multiple method occupancy model is capable of estimating detection parameters and occupancy state when one method has occasional false positive detections. We test the accuracy of the model when automated detection of black-throated green warbler is mixed with human detection in 4 recorded surveys at 60 sites. Precision and accuracy are evaluated by simulation, and we use the results to optimize future sampling. In simulation, parameter estimates by the multiple method occupancy model are close to those we computed manually when two surveys are manually analyzed. Our results support the use of the multiple method false positive occupancy model to track detection rates in automated monitoring programs.},
	keywords     = {Pure sciences; Biological sciences; Health and environmental sciences; Bioacoustics; Cran r; Monitoring; Acoustics; Environmental management; Wildlife conservation; 0986:Acoustics; 0474:Environmental management; 0284:Wildlife Conservation},
	language     = {English}
}
@phdthesis{Barcellos2019,
	author       = {Barcellos, Diogo Destro},
	year         = {2019},
	title        = {Monitoramento acústico passivo: detecção de cetáceos odontocetos no litoral norte do Estado de São Paulo},
	url          = {http://www.teses.usp.br/teses/disponiveis/21/21134/tde-13032020-151153/},
	school       = {Universidade de São Paulo}
}
@phdthesis{Wilson2024,
	author       = {Wilson,Riggs O.},
	year         = {2024},
	title        = {Monitoring Northern Bobwhite (Colinus virginianus) Occupancy on the Landscape-Scale in Southern Iowa, USA},
	journal      = {ProQuest Dissertations and Theses},
	pages        = 120,
	isbn         = 9798896070498,
	url          = {https://may.idm.oclc.org/login?url=https://www.proquest.com/dissertations-theses/monitoring-northern-bobwhite-em-colinus/docview/3118586537/se-2},
	note         = {Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2024-11-14},
	abstract     = {Northern Bobwhites (Colinus virginianus) have experienced declines across their range over the last several decades. Interest continues to grow in finding ways to counteract these declines of this well-known, small, gallinaceous game bird. Bobwhites inhabit a wide range of areas and there is need for a greater understanding of what is driving bobwhite occupancy on the landscape-scale to aid natural resource agencies and practitioners in better implementing conservation efforts. Studies of bobwhites have largely been constrained from covering a broader landscape-scale due to limited large-scale data sources and logistical constraints of traditional bobwhite survey techniques. A potential solution to this is the use of Autonomous Recording Units (ARU) coupled with artificial intelligence software to identify bobwhite calls. I evaluated the use of ARUs for monitoring bobwhites in Iowa using ARU survey data with a subset of paired covey-call point counts during fall 2017-2021 in southern Iowa and then applied these findings to monitor for bobwhite covey-calls with ARUs during fall 2022-2023 across 35 southern Iowa counites identified as priority counties in the Working Lands For Wildlife Northern Bobwhite Grasslands and Savannas framework. Using the 2017-2021 ARU data, I compared in-person and ARU covey call surveys, examined the minimum processing time needed for recordings and ways to improve processing with the deep artificial neural network BirdNET, and evaluated the survey conditions needed to detect bobwhites and the number of recording days needed with good survey conditions to determine bobwhite occupancy. I found bobwhite detections from the in-person and ARU surveys corresponded well (89.2%) and that 95.7% of detections occurred during the time period from 50-minutes before sunrise to 15-minutes before sunrise. Wind speed and cloud coverage were associated with reduced detection of bobwhites. At our high occupancy sites, detection probability approached 1 after 4 survey days. While the deep artificial neural network BirdNET had lower detection probabilities than manual processing it was able to process recordings significantly faster with relatively few false positives. Using what I learned from this portion of my study I deployed ARUs across the 35 southern Iowa counties at 81 sites during fall 2022-2023 to survey for bobwhite occupancy with the specific goals of understanding the nature of bobwhite distribution in Iowa and the influence of landscape structure and composition on bobwhite occurrence. I surveyed landcover and vegetation at 3 scales; landscape-scale (2-km buffer), local-scale (250-m radius buffer), and woody patch scale along a 1,000-m roadside transect. Overall occupancy probability across my study area in southern Iowa was 0.40 (95% CrI: 0.31, 0.50). I found the total percentage of grass landcover had a significant positive effect on occupancy at both the landscape-scale and local-scale. Percentage of forest landcover had a significant negative effect at the landscape-scale. Total woody landcover, which included forests, had a significant negative effect on occupancy at the local-scale. Percentage of idle grass (i.e., grass not disturbed with practices such as mowing for hay or grazing) had a significant positive effect on occupancy at the local scale. The percentage of crop had a quadratic effect at both scales with maximum occupancy occurring at intermediate levels of crop land cover. Total length of woody edge (m) at the 250-m scale had an interactive effect with average exotic honeysuckle (Lonicera maackii or L. tatarica) presence with a significant negative effect of average honeysuckle presence on bobwhite occupancy probability. The relatively high overall occupancy probability for bobwhites in southern Iowa shows the potential this area has for bobwhite conservation efforts. Not all landscapes are suitable for bobwhites however, as my findings show that overall, bobwhites respond positively to grass vegetation and moderate amounts of crop and negatively to forest vegetation. Differing effects of landcover between the different scales shows the need for monitoring bobwhites at both local and landscape-scales. The potential negative impact of honeysuckle on bobwhite use of woody edges is a potential area for further research. My study showed the need for monitoring bobwhite populations at multiple scales and the effect of landcover and vegetation on bobwhite occupancy at multiple scales and the utility of ARUs for facilitating effective monitoring.},
	keywords     = {Covey call; Landscape-scale; Northern Bobwhites; Passive acoustic monitoring; Autonomous Recording Units; Wildlife conservation; Wildlife management; Ecology; Acoustics; 0329:Ecology; 0986:Acoustics; 0286:Wildlife Management; 0284:Wildlife Conservation},
	language     = {English}
}
@article{Caruso_2020,
	author       = {Caruso, Francesco and Dong, Lijun and Lin, Mingli and Liu, Mingming and Gong, Zining and Xu, Wanxue and Alonge, Giuseppe and Li, Songhai},
	year         = {2020},
	title        = {Monitoring of a Nearshore Small Dolphin Species Using Passive Acoustic Platforms and Supervised Machine Learning Techniques},
	month        = {04},
	journal      = {Frontiers in Marine Science},
	publisher    = {Frontiers Media SA},
	volume       = 7,
	doi          = {10.3389/fmars.2020.00267},
	issn         = {2296-7745},
	url          = {http://dx.doi.org/10.3389/fmars.2020.00267}
}
@article{Varun_Prakash_2024,
	author       = {Varun Prakash, R. and Karthikeyan, V. and Vishali, S. and Karthika, M.},
	year         = {2024},
	title        = {Multi-level LSTM framework with hybrid sonic features for human–animal conflict evasion},
	month        = {08},
	journal      = {The Visual Computer},
	publisher    = {Springer Science and Business Media LLC},
	doi          = {10.1007/s00371-024-03588-9},
	issn         = {1432-2315},
	url          = {http://dx.doi.org/10.1007/s00371-024-03588-9}
}
@article{Symes_2024,
	author       = {Symes, Laurel B. and Madhusudhana, Shyam and Martinson, Sharon J. and Geipel, Inga and ter Hofstede, Hannah M.},
	year         = {2024},
	title        = {Multi-year soundscape recordings and automated call detection reveals varied impact of moonlight on calling activity of neotropical forest katydids},
	month        = {05},
	journal      = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	publisher    = {The Royal Society},
	volume       = 379,
	number       = 1904,
	doi          = {10.1098/rstb.2023.0110},
	issn         = {1471-2970},
	url          = {http://dx.doi.org/10.1098/rstb.2023.0110}
}
@article{Potamitis_2009,
	author       = {Potamitis, Ilyas and Ganchev, Todor and Kontodimas, Dimitris},
	year         = {2009},
	title        = {On Automatic Bioacoustic Detection of Pests: The Cases of &lt;I&gt;Rhynchophorus ferrugineus&lt;/I&gt; and &lt;I&gt;Sitophilus oryzae&lt;/I&gt;},
	month        = {08},
	journal      = {Journal of Economic Entomology},
	publisher    = {Oxford University Press (OUP)},
	volume       = 102,
	number       = 4,
	pages        = {1681-1690},
	doi          = {10.1603/029.102.0436},
	issn         = {0022-0493},
	url          = {http://dx.doi.org/10.1603/029.102.0436}
}
@article{RANKIN2024102511,
	author       = {Shannon Rankin and Taiki Sakai and Frederick I. Archer and Jay Barlow and Danielle Cholewiak and Annamaria I. DeAngelis and Jennifer L.K. McCullough and Erin M. Oleson and Anne E. Simonis and Melissa S. Soldevilla and Jennifer S. Trickey},
	year         = {2024},
	title        = {Open-source machine learning BANTER acoustic classification of beaked whale echolocation pulses},
	journal      = {Ecological Informatics},
	volume       = 80,
	pages        = 102511,
	doi          = {https://doi.org/10.1016/j.ecoinf.2024.102511},
	issn         = {1574-9541},
	url          = {https://www.sciencedirect.com/science/article/pii/S1574954124000530},
	keywords     = {Bioacoustics, Machine learning, Random forest, Species classification, Passive acoustic monitoring, Beaked whale},
	abstract     = {Passive acoustic monitoring is increasingly used for assessing populations of marine mammals; however, analysis of large datasets is limited by our ability to easily classify sounds detected. Classification of beaked whale acoustic events, in particular, requires evaluation of multiple lines of evidence by expert analysts. Here we present a highly automated approach to acoustic detection and classification using supervised machine learning and open source software methods. Data from four large scale surveys of beaked whales (northwestern North Atlantic, southwestern North Atlantic, Hawaii, and eastern North Pacific) were analyzed using PAMGuard (acoustic detection), PAMpal (acoustic analysis) and BANTER (hierarchical random forest classifier). Overall classification accuracy ranged from 88% for the southwestern North Atlantic data to 97% for the northwestern North Atlantic. Results for many species could likely be improved with increased sample sizes, consideration of alternative automated detectors, and addition of relevant environmental features. These methods provide a highly automated approach to acoustic detection and classification using open source methods that can be readily adopted for other species and geographic regions.}
}
@article{Brinkl_v_2023,
	author       = {Brinkløv, Signe M. M. and Macaulay, Jamie and Bergler, Christian and Tougaard, Jakob and Beedholm, Kristian and Elmeros, Morten and Madsen, Peter Teglberg},
	year         = {2023},
	title        = {Open-source workflow approaches to passive acoustic monitoring of bats},
	month        = {05},
	journal      = {Methods in Ecology and Evolution},
	publisher    = {Wiley},
	volume       = 14,
	number       = 7,
	pages        = {1747-1763},
	doi          = {10.1111/2041-210x.14131},
	issn         = {2041-210X},
	url          = {http://dx.doi.org/10.1111/2041-210X.14131}
}
@article{Metcalf2021,
	author       = {Metcalf,  Oliver C. and Barlow,  Jos and Marsden,  Stuart and Gomes de Moura,  Nárgila and Berenguer,  Erika and Ferreira,  Joice and Lees,  Alexander C.},
	year         = {2021},
	title        = {Optimizing tropical forest bird surveys using passive acoustic monitoring and high temporal resolution sampling},
	month        = {07},
	journal      = {Remote Sensing in Ecology and Conservation},
	publisher    = {Wiley},
	volume       = 8,
	number       = 1,
	pages        = {45--56},
	doi          = {10.1002/rse2.227},
	issn         = {2056-3485},
	url          = {http://dx.doi.org/10.1002/rse2.227},
	editor       = {Pettorelli,  Nathalie and Astaras,  Christos}
}
@article{10.1371/journal.pone.0229058,
	author       = {Gillespie, Douglas AND Palmer, Laura AND Macaulay, Jamie AND Sparling, Carol AND Hastie, Gordon},
	year         = {2020},
	title        = {Passive acoustic methods for tracking the 3D movements of small cetaceans around marine structures},
	month        = {05},
	journal      = {PLOS ONE},
	publisher    = {Public Library of Science},
	volume       = 15,
	number       = 5,
	pages        = {1--16},
	doi          = {10.1371/journal.pone.0229058},
	url          = {https://doi.org/10.1371/journal.pone.0229058},
	abstract     = {A wide range of anthropogenic structures exist in the marine environment with the extent of these set to increase as the global offshore renewable energy industry grows. Many of these pose acute risks to marine wildlife; for example, tidal energy generators have the potential to injure or kill seals and small cetaceans through collisions with moving turbine parts. Information on fine scale behaviour of animals close to operational turbines is required to understand the likely impact of these new technologies. There are inherent challenges associated with measuring the underwater movements of marine animals which have, so far, limited data collection. Here, we describe the development and application of a system for monitoring the three-dimensional movements of cetaceans in the immediate vicinity of a subsea structure. The system comprises twelve hydrophones and software for the detection and localisation of vocal marine mammals. We present data demonstrating the systems practical performance during a deployment on an operational tidal turbine between October 2017 and October 2019. Three-dimensional locations of cetaceans were derived from the passive acoustic data using time of arrival differences on each hydrophone. Localisation accuracy was assessed with an artificial sound source at known locations and a refined method of error estimation is presented. Calibration trials show that the system can accurately localise sounds to 2m accuracy within 20m of the turbine but that localisations become highly inaccurate at distances greater than 35m. The system is currently being used to provide data on rates of encounters between cetaceans and the turbine and to provide high resolution tracking data for animals close to the turbine. These data can be used to inform stakeholders and regulators on the likely impact of tidal turbines on cetaceans.}
}
@mastersthesis{Burfin2022,
	author       = {Burfin, Thomas Wojciech},
	year         = {2022},
	title        = {Passive acoustic monitoring and audio subsampling: optimizing autonomous methods for avian biodiversity assessments},
	url          = {http://hdl.handle.net/10451/54427},
	school       = {Universidade de Lisboa}
}
@techreport{https://doi.org/10.25607/obp-876,
	author       = {Browning, Ella and Gibb, Rory and Glover-Kapfer, Paul and Jones, Kate E.},
	year         = {2017},
	title        = {Passive acoustic monitoring in ecology and conservation.},
	publisher    = {WWF-UK},
	doi          = {10.25607/OBP-876},
	url          = {https://repository.oceanbestpractices.org/handle/11329/1370},
	keywords     = {Acoustic sensors, Acoustic monitoring, Conservation, Field ecology, Ecoacoustics, Parameter Discipline::Physical oceanography::Acoustics, Instrument Type Vocabulary::acoustic backscatter sensors},
	language     = {en}
}
@misc{ref47,
	author       = {W Slingerland},
	year         = {2021},
	title        = {Passive Acoustic Monitoring in Ranomafana National Park},
	journal      = {},
	doi          = {},
	url          = {https://studenttheses.uu.nl/handle/20.500.12932/299},
	abstract     = {Vocally active species partition the soundscape to communicate on food availability, mate attraction and predation risk. Tropical forests, with high vegetation density, have many vocally active species, including many frugivores that help maintain the forest through seed dispersal or pollination. The tropical forests of Madagascar have been subjected to extreme anthropogenic disturbances leading to land degradation which threatens many native frugivorous species with extinction. To reforest and restore the tropical forests in Madagascar, more knowledge on the presence, behavior, and interaction of frugivorous species is needed. In this thesis I used a dataset gathered through Passive Acoustic Monitoring (PAM) in Ranomafana National Park (RNP) to characterize the soundscape and vocal activity of frugivorous bird species in RNP. I described daily and hourly variations in the soundscape with Sound Pressure Level (SPL) analysis in Kaleidoscope. I isolated the calls of individual frugivorous bird species using cluster analysis and pattern matching analysis in the programs Kaleidoscope and ARBIMON, respectively, and compared the acoustic niche space of different species using the activity package in R. I found a general increase in sound intensity between sunrise and sunset. However, the sound intensity variation across days was strongly influenced by geophysical sounds, especially rainfall. I successfully identified vocalizations using publicly available recordings of the four species Coracopsis nigra, C. vasa, Hypsipetes madagascariensis, and Philepitta castanea. The vocal activity of four frugivorous bird species showed little acoustic niche overlap in the temporal and spectral dimensions. With these results I showed that PAM can be used to observe frugivorous bird species in tropical forests to increase knowledge on these functionally important species. However, I recommend addressing some remaining technical and execution challenges to improve detection and study of tropical frugivores. First, a method to separate biological sounds from geophysical and anthropogenic sounds will help in observing vocal species. Second, there needs to be further study on how the location of the sensor influences sound quality and background noise of the recordings. Finally, I recommend collaboration with a local ornithologist to improve species identification based on vocalization. I have shown that particularly with these improvements, PAM represents a viable alternative to in-person observation of frugivore bird species in a dense tropical forest.},
	keywords     = {},
	type         = {Master's Thesis}
}
@article{DUFOURQ2022101688,
	author       = {Emmanuel Dufourq and Carly Batist and Ruben Foquet and Ian Durbach},
	year         = {2022},
	title        = {Passive acoustic monitoring of animal populations with transfer learning},
	journal      = {Ecological Informatics},
	volume       = 70,
	pages        = 101688,
	doi          = {https://doi.org/10.1016/j.ecoinf.2022.101688},
	issn         = {1574-9541},
	url          = {https://www.sciencedirect.com/science/article/pii/S1574954122001388},
	keywords     = {Transfer learning, Convolutional neural networks, Deep learning, Vocalisation classification, Bioacoustics},
	abstract     = {Progress in deep learning, more specifically in using convolutional neural networks (CNNs) for the creation of classification models, has been tremendous in recent years. Within bioacoustics research, there has been a large number of recent studies that use CNNs. Designing CNN architectures from scratch is non-trivial and requires knowledge of machine learning. Furthermore, hyper-parameter tuning associated with CNNs is extremely time consuming and requires expensive hardware. In this paper we assess whether it is possible to build good bioacoustic classifiers by adapting and re-using existing CNNs pre-trained on the ImageNet dataset – instead of designing them from scratch, a strategy known as transfer learning that has proved highly successful in other domains. This study is a first attempt to conduct a large-scale investigation on how transfer learning can be used for passive acoustic monitoring (PAM), to simplify the implementation of CNNs and the design decisions when creating them, and to remove time consuming hyper-parameter tuning phases. We compare 12 modern CNN architectures across 4 passive acoustic datasets that target calls of the Hainan gibbon Nomascus hainanus, the critically endangered black-and-white ruffed lemur Varecia variegata, the vulnerable Thyolo alethe Chamaetylas choloensis, and the Pin-tailed whydah Vidua macroura. We focus our work on data scarcity issues by training PAM binary classification models very small datasets, with as few as 25 verified examples. Our findings reveal that transfer learning can result in up to 82% F1 score while keeping CNN implementation details to a minimum, thus rendering this approach accessible, easier to design, and speeding up further vocalisation annotations to create PAM robust models.}
}
@phdthesis{8e0e1070e82311dfa891000ea68e967b,
	author       = {Kyhn, {Line Anker}},
	year         = {2010},
	title        = {Passive acoustic monitoring of toothed whales with implications for mitigation, management and biology},
	publisher    = {National Environmental Research Institute, Aarhus University},
	abstract     = {Toothed whales are vocal animals and their social life as well as successful orientation and feeding depends on emission and reception of sound. Such sounds may e.g. be clicks used for echolocation or whistles used for communication and they can be monitored in time and space by means of passive acoustic monitoring (PAM). PAM is particularly suited to study small inconspicuous for these species. Among the small odontocetes, four produce the same special echolocation click type, the narrow band high frequency (NBHF) click that has evolved through convergent evolution. Clicks of the individual NBHF species are very similar and all these species may thus potentially be monitored by applying the same datalogger systems. However, since some of these species live sympatrically it is essential to identify potential acoustic species differences that may be used for species recognition in PAM.        The focus of chapter II in this thesis was specifically to try to find a method to combine traditional transect survey distance sampling and acoustic monitoring by means of cue counting to be able to estimate densities from datalogger data. The problem is how to derive a detection function, i.e. a function that describes the probability of detecting an acoustic cue at a given distance from the datalogger? In chapter II I describe one such possibility where we tracked harbour porpoises visually around dataloggers by means of a theodolite and following compared the visual and acoustic detections in a mark-recapture design to describe the detection function. From the detection function we then calculated the effective detection radius, which we then used to estimate the density of porpoises in the area. As cue for the acoustic detections we tested different durations with click trains. We obtained the cue production rate from acoustic tags being fitted to wild harbour porpoises. From the visual sighting we also estimated density within 100 m radius of each datalogger. The detection functions were successful in estimating densities of around the same level as we found for the visual observations, and more importantly the detection functions derived per datalogger could level out the sensitivity differences between the dataloggers (chapter II). Knowing that passive acoustic monitoring may be used to derive densities of small odontocetes such as NBHF species, the next step is to obtain accurate click descriptions. A species' sounds must be well defined according to specific sound source parameters to be able to build precise filters for the acoustic dataloggers to sort the correct signals from noise. Such definitions require that the variation at the level of species is known and therefore that each focal species have been recorded using appropriate equipment under natural conditions. This was the focus of chapter III, IV & VI, where I show that NBHF clicks are very similar, however when applying specific criteria to compare the clicks there a species specific differences at a statistical level. I used these differences to successfully differentiate the species in Monte Carlo simulations, which means that it may also be possible to separate sympatric NBHF species with acoustic monitoring. Secondly, I was interested in examining the species differences in an evolutionary light to see if there were differences pertaining to possible habitat specializations of each species as is seen for Microchiropteran bats and this was the focus of chapter IV & VI. It appeared that costal cluttered habitats may be limiting for NBHF species since they produce lower source levels when recorded in cluttered habitats and clutter does not favour production of high source levels. I further argue that the small centroid frequency differences observed between sympatric NBHF species may be caused by character displacement and be the means of acoustic species separation for sympatric species, which appear favourable in an environment where use of the visual sense is greatly reduced. In chapter V we use the fact that two sister species at a disputable position in the dolphin taxonomy both produce NBHF to argue that both Peale's and hourglass dolphins are closely associated with the Cephalorhynchid dolphins, which is in accordance with new molecular phylogenies. In chapter I use the information I have gathered on spectral source properties as well as on source levels and directionality and use this information to challenge the theories for the evolution of the NBHF click type. I conclude that the NBHF signals likely evolved to meet the dual requirements of operating an effective sonar system and at the same time to minimize the risk of killer whale predation from passive listening. The high frequency part of the NBHF click thus likely evolved as a product of the species' small body sizes to obtain directionality high enough to yield efficient biosonar.},
	keywords     = {akustik, ekkolokalisering, marsvin, tandhval, populationst{\ae}thed, habitat specialisering, acoustic monitoring, density estimation, cue counting, acoustics, echolocation, click specializations, endotoxin},
	language     = {English}
}
@inproceedings{Riera2012PatternsOS,
	author       = {Amalis Riera},
	year         = {2012},
	title        = {Patterns of seasonal occurrence of sympatric killer whale lineages in waters off Southern Vancouver Island and Washington state, as determined by passive acoustic monitoring},
	url          = {https://api.semanticscholar.org/CorpusID:89916138}
}
@article{Ruff_2023,
	author       = {Ruff, Zachary J. and Lesmeister, Damon B. and Jenkins, Julianna M.A. and Sullivan, Christopher M.},
	year         = {2023},
	title        = {PNW-Cnet v4: Automated species identification for passive acoustic monitoring},
	month        = {07},
	journal      = {SoftwareX},
	publisher    = {Elsevier BV},
	volume       = {23},
	pages        = 101473,
	doi          = {10.1016/j.softx.2023.101473},
	issn         = {2352-7110},
	url          = {http://dx.doi.org/10.1016/j.softx.2023.101473}
}
@techreport{Frouin-Mouy2016,
	author       = {Frouin-Mouy, H. and Yurk, H. and Mouy, X. and Martin, B.},
	year         = {2016},
	title        = {Prince Rupert - Aurora LNG Acoustic Monitoring Study: Chatham Sound Region},
	number       = {Document 01129, Version 3.0},
	url          = {https://projects.eao.gov.bc.ca/api/document/58923173b637cc02bea163f0/fetch/Appendix_O_Acoustic_Monitoring_Final_screening.pdf},
	institution  = {JASCO Applied Sciences},
	type         = {Technical Report}
}
@bachelorsthesis{AubachRatcliffe2023,
	author       = {Aubach Ratcliffe, Júlia Neus},
	year         = {2023},
	title        = {Processing of bowhead whale vocalisations and handling of acoustic recording equipment},
	month        = 11,
	url          = {https://upcommons.upc.edu/handle/2117/397581},
	school       = {Universitat Politècnica de Catalunya},
	type         = {Bachelor's Thesis}
}
@article{Zhao_2020,
	author       = {Zhao, Ying and Shen, Xiaoli and Li, Sheng and Zhang, Yanyun and Peng, Renhua and Ma, Keping},
	year         = {2020},
	title        = {Progress and outlook for soundscape ecology},
	journal      = {Biodiversity Science},
	publisher    = {Biodiversity Science},
	volume       = {28},
	number       = 7,
	pages        = {806–820},
	doi          = {10.17520/biods.2020114},
	issn         = {1005-0094},
	url          = {http://dx.doi.org/10.17520/biods.2020114}
}
@inproceedings{Chen_2021,
	author       = {Chen, Yankun and Wang, Weiping and Liang, Yinian and Zhou, Defu and Dong, Chao and Li, Jie},
	year         = {2021},
	title        = {Real-time Detection and Classification for Targeted Marine Mammals},
	month        = {07},
	booktitle    = {2021 OES China Ocean Acoustics (COA)},
	publisher    = {IEEE},
	pages        = {1027-1031},
	doi          = {10.1109/coa50123.2021.9519906},
	url          = {http://dx.doi.org/10.1109/COA50123.2021.9519906}
}
@article{Rivas2024.11.04.621915,
	author       = {Rivas, Francisco and Brizio, Cesare and Buzzetti, Filippo M. and Pijanowski, Bryan},
	year         = {2024},
	title        = {Rthoptera: Standardised Insect Bioacoustics in R},
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.11.04.621915},
	url          = {https://www.biorxiv.org/content/early/2024/11/08/2024.11.04.621915},
	elocation-id = {2024.11.04.621915},
	abstract     = {Crickets were the first study subjects at the dawn of bioacoustics, yet, almost 100 years later, freely available and robust analysis tools and protocols are still needed.We introduce Rthoptera, a new open-source R package for the analysis of insect acoustic signals, offering accurate graphics and standardised temporal and spectral measurements through a streamlined workflow. Our package delivers results in a fraction of the time typically required by multi-software methods. Most of the functions have interactive versions (Shiny applications), facilitating their adoption by researchers with any level of technical expertise.New acoustic metrics are introduced: Spectral Excursion, Pattern Complexity Index, Temporal Excursion, Dynamic Excursion, and Broadband Activity Index.Accompanied by an appropriate recording protocol, our tool can become the backbone of a standard analysis protocol for comparative insect bioacoustics.Competing Interest StatementThe authors have declared no competing interest.},
	eprint       = {https://www.biorxiv.org/content/early/2024/11/08/2024.11.04.621915.full.pdf}
}
@article{10.1371/journal.pcbi.1008698,
	author       = {Lin, Tzu-Hao AND Akamatsu, Tomonari AND Tsao, Yu},
	year         = {2021},
	title        = {Sensing ecosystem dynamics via audio source separation: A case study of marine soundscapes off northeastern Taiwan},
	month        = {02},
	journal      = {PLOS Computational Biology},
	publisher    = {Public Library of Science},
	volume       = 17,
	number       = {2},
	pages        = {1--23},
	doi          = {10.1371/journal.pcbi.1008698},
	url          = {https://doi.org/10.1371/journal.pcbi.1008698},
	abstract     = {Remote acquisition of information on ecosystem dynamics is essential for conservation management, especially for the deep ocean. Soundscape offers unique opportunities to study the behavior of soniferous marine animals and their interactions with various noise-generating activities at a fine temporal resolution. However, the retrieval of soundscape information remains challenging owing to limitations in audio analysis techniques that are effective in the face of highly variable interfering sources. This study investigated the application of a seafloor acoustic observatory as a long-term platform for observing marine ecosystem dynamics through audio source separation. A source separation model based on the assumption of source-specific periodicity was used to factorize time-frequency representations of long-duration underwater recordings. With minimal supervision, the model learned to discriminate source-specific spectral features and prove to be effective in the separation of sounds made by cetaceans, soniferous fish, and abiotic sources from the deep-water soundscapes off northeastern Taiwan. Results revealed phenological differences among the sound sources and identified diurnal and seasonal interactions between cetaceans and soniferous fish. The application of clustering to source separation results generated a database featuring the diversity of soundscapes and revealed a compositional shift in clusters of cetacean vocalizations and fish choruses during diurnal and seasonal cycles. The source separation model enables the transformation of single-channel audio into multiple channels encoding the dynamics of biophony, geophony, and anthropophony, which are essential for characterizing the community of soniferous animals, quality of acoustic habitat, and their interactions. Our results demonstrated the application of source separation could facilitate acoustic diversity assessment, which is a crucial task in soundscape-based ecosystem monitoring. Future implementation of soundscape information retrieval in long-term marine observation networks will lead to the use of soundscapes as a new tool for conservation management in an increasingly noisy ocean.}
}
@article{Rasmussen_2024,
	author       = {Rasmussen, Jeppe H. and Stowell, Dan and Briefer, Elodie F.},
	year         = {2024},
	title        = {Sound evidence for biodiversity monitoring},
	month        = {07},
	journal      = {Science},
	publisher    = {American Association for the Advancement of Science (AAAS)},
	volume       = 385,
	number       = 6705,
	pages        = {138-140},
	doi          = {10.1126/science.adh2716},
	issn         = {1095-9203},
	url          = {http://dx.doi.org/10.1126/science.adh2716}
}
@article{Monczak2019,
	author       = {Monczak,  A and Mueller,  C and Miller,  ME and Ji,  Y and Borgianini,  SA and Montie,  EW},
	year         = {2019},
	title        = {Sound patterns of snapping shrimp,  fish,  and dolphins in an estuarine soundscape of the southeastern USA},
	month        = {01},
	journal      = {Marine Ecology Progress Series},
	publisher    = {Inter-Research Science Center},
	volume       = 609,
	pages        = {49--68},
	doi          = {10.3354/meps12813},
	issn         = {1616-1599},
	url          = {http://dx.doi.org/10.3354/meps12813}
}
@misc{bressler2023soundbaydeeplearningframework,
	author       = {Noam Bressler and Michael Faran and Amit Galor and Michael Moshe Michelashvili and Tomer Nachshon and Noa Weiss},
	year         = {2023},
	title        = {Soundbay: Deep Learning Framework for Marine Mammals and Bioacoustic Research},
	url          = {https://arxiv.org/abs/2311.04343},
	eprint       = {2311.04343},
	archiveprefix = {arXiv},
	primaryclass = {cs.SD}
}
@article{10.1121/1.2932958,
	author       = {Baumann, Simone and Hildebrand, John A. and Wiggins, Sean M. and Schnitzler, Hans-Ulrich},
	year         = {2008},
	title        = {Species identification and measurement of activity in odontocete species of Palmyra Atoll by acoustic monitoring},
	month        = {05},
	journal      = {The Journal of the Acoustical Society of America},
	volume       = 123,
	number       = {5_Supplement},
	pages        = {3099--3099},
	doi          = {10.1121/1.2932958},
	issn         = {0001-4966},
	url          = {https://doi.org/10.1121/1.2932958},
	abstract     = {Acoustic monitoring has been used to study odontocete presence at Palmyra Atoll, a remote island in the Northern Line Islands chain. Long-term recordings of high-frequency, broadband acoustic data have become possible with recent technological advances. A High-frequency Autonomous Recording Package (HARP) has been developed which samples at 200 kHz with a duty cycle of 1/4 for up to seven months. This instrument has recorded since October 2006 at Palmyra Atoll. Visual and acoustic surveys were conducted around Palmyra Atoll using a four-element towed hydrophone array sampling real-time at 200 kHz to obtain species-specific acoustic data. These data are used as reference for automatic detection algorithms applied on the long-term recordings. To date, acoustically and visually detected odontocetes include bottlenose dolphins (Tursiops truncatus), spinner dolphins (Stenella longirostris), melon-headed whales (Peponocephala electra) and beaked whales of the genus Mesoplodon. The long-term HARP data reveal acoustic activity primarily at night time and predominantely odontocete clicks. Both the beaked as well as the melon-headed whales are present year round and show a distinct daily acoustic activity cycle.}
}
@patent{Urazghildiiev2019,
	author       = {Urazghildiiev, Ildar},
	year         = {2019},
	title        = {System and methods for processing and the visualization of bioacoustical information},
	month        = {03},
	day          = 5,
	number       = {US10222493B2},
	url          = {https://patents.justia.com/patent/10222493},
	institution  = {Cornell University}
}
@article{Bas-2017,
	author       = {Bas, Yves and Bas, Didier and Julien, Jean-François},
	year         = {2017},
	title        = {Tadarida: A Toolbox for Animal Detection on Acoustic Recordings},
	month        = {02},
	journal      = {Journal of Open Research Software},
	doi          = {10.5334/jors.154},
	abstract     = {Passive Acoustic Monitoring (PAM) recently extended to a very wide range of animals, but no available open software has been sufficiently generic to automatically treat several taxonomic groups. Here we present Tadarida, a software toolbox allowing for the detection and labelling of recorded sound events, and to classify any new acoustic data into known classes. It is made up of three modules handling Detection, Labelling and Classification and running on either Linux or Windows. This development resulted in the first open software (1) allowing generic sound event detection (multi-taxa), (2) providing graphical sound labelling at a single-instance level and (3) covering the whole process from sound detection to classification. This generic and modular design opens numerous reuse opportunities among (bio)acoustics researchers, especially for those managing and/or developing PAM schemes.The whole toolbox is openly developed in C++ (Detection and Labelling) and R (Classification) and stored at https://github.com/YvesBas.},
	keyword      = {en_US}
}
@article{Milanelli2024,
	author       = {Milanelli,  A.M. and Rossi-Santos,  M.R. and Fruet,  P.F. and Assump\c{c}ão,  R. and Cavalcanti,  A.M. and Dalla Rosa,  L.},
	year         = {2024},
	title        = {Temporal patterns in the soundscape of the port area in an urban estuary},
	month        = {02},
	journal      = {Estuarine,  Coastal and Shelf Science},
	publisher    = {Elsevier BV},
	volume       = {297},
	pages        = 108596,
	doi          = {10.1016/j.ecss.2023.108596},
	issn         = {0272-7714},
	url          = {http://dx.doi.org/10.1016/j.ecss.2023.108596}
}
@article{10.1371/journal.pone.0210364,
	author       = {Alpízar, Priscilla AND Rodríguez-Herrera, Bernal AND Jung, Kirsten},
	year         = {2019},
	title        = {The effect of local land use on aerial insectivorous bats (Chiroptera) within the two dominating crop types in the Northern-Caribbean lowlands of Costa Rica},
	month        = {01},
	journal      = {PLOS ONE},
	publisher    = {Public Library of Science},
	volume       = 14,
	number       = 1,
	pages        = {1--15},
	doi          = {10.1371/journal.pone.0210364},
	url          = {https://doi.org/10.1371/journal.pone.0210364},
	abstract     = {Land transformation into agricultural areas and the intensification of management practices represent two of the most devastating threats to biodiversity worldwide. Within this study, we investigated the effect of intensively managed agroecosystems on bat activity and species composition within two focal areas differing in landscape structure. We sampled bats via acoustic monitoring and insects with flight interception traps in banana and pineapple monoculture plantations and two nearby protected forested areas within the area of Sarapiquí, Costa Rica. Our results revealed that general occurrence and feeding activity of bats was higher above plantations compared to forested areas. We also recorded higher species richness at recording sites in plantations. This trend was especially strong within a fragmented landscape, with only four species recorded in forests, but 12 above pineapple plantations. Several bat species, however, occurred only once or twice above plantations, and forest specialist species such as Centronycteris centralis, Myotis riparius and Pteronotus mesoamericanus were only recorded at forest sites. Our results indicated, that mostly mobile open space and edge foraging bat species can use plantations as potential foraging habitat and might even take advantage of temporal insect outbreaks. However, forests are vital refugia for several species, including slower flying forest specialists, and thus a prerequisite to safeguard bat diversity within agricultural dominated landscapes.}
}
@article{Do_Nascimento_2024,
	author       = {Do Nascimento, Leandro A. and Pérez-Granados, Cristian and Alencar, Janderson B. Rodrigues and Beard, Karen H.},
	year         = {2024},
	title        = {Time and habitat structure shape insect acoustic activity in the Amazon},
	month        = {05},
	journal      = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	publisher    = {The Royal Society},
	volume       = 379,
	number       = 1904,
	doi          = {10.1098/rstb.2023.0112},
	issn         = {1471-2970},
	url          = {http://dx.doi.org/10.1098/rstb.2023.0112}
}
@article{Gomes2020-sb,
	author       = {Gomes, Dylan G E and Appel, Giulliana and Barber, Jesse R},
	year         = {2020},
	title        = {Time of night and moonlight structure vertical space use by insectivorous bats in a Neotropical rainforest: an acoustic monitoring study},
	month        = 12,
	journal      = {PeerJ},
	publisher    = {PeerJ},
	volume       = 8,
	number       = {e10591},
	pages        = {e10591},
	abstract     = {BACKGROUND: Previous research has shown diverse vertical space use by various taxa, highlighting the importance of forest vertical structure. Yet, we know little about vertical space use of tropical forests, and we often fail to explore how this three-dimensional space use changes over time. METHODS: Here we use canopy tower systems in French Guiana and passive acoustic monitoring to measure Neotropical bat activity above and below the forest canopy throughout nine nights. We use a Bayesian generalized linear mixed effect model and kernel density estimates to demonstrate patterns in space-use over time. RESULTS: We found that different bats use both canopy and understory space differently and that these patterns change throughout the night. Overall, bats were more active above the canopy (including Cormura brevirostris, Molossus molossus, Peropteryx kappleri and Peropteryx macrotis), but multiple species or acoustic complexes (when species identification was impossible) were more active in the understory (such as Centronycteris maximiliani, Myotis riparius, Pteronotus alitonus and Pteronotus rubiginosus). We also found that most bats showed temporally-changing preferences in hourly activity. Some species were less active (e.g., P. kappleri and P. macrotis), whereas others were more active (Pteronotus gymnonotus, C. brevirostris, and M. molossus) on nights with higher moon illuminance. DISCUSSION: Here we show that Neotropical bats use habitat above the forest canopy and within the forest understory differently throughout the night. While bats generally were more active above the forest canopy, we show that individual groups of bats use space differently over the course of a night, and some prefer the understory. This work highlights the need to consider diel cycles in studies of space use, as animals use different habitats during different periods of the day.},
	keywords     = {Bat activity; Chiroptera; Daily cycle; Diel; Moon; Neotropics; Passive acoustic monitoring; Rainforest; Temporal patterns},
	language     = {en}
}
@article{Rigakis_2021,
	author       = {Rigakis, Iraklis and Potamitis, Ilyas and Tatlas, Nicolaos-Alexandros and Potirakis, Stelios M. and Ntalampiras, Stavros},
	year         = {2021},
	title        = {TreeVibes: Modern Tools for Global Monitoring of Trees for Borers},
	month        = {02},
	journal      = {Smart Cities},
	publisher    = {MDPI AG},
	volume       = 4,
	number       = 1,
	pages        = {271-285},
	doi          = {10.3390/smartcities4010017},
	issn         = {2624-6511},
	url          = {http://dx.doi.org/10.3390/smartcities4010017}
}
@article{https://doi.org/10.1111/2041-210X.13520,
	author       = {Clink, Dena J. and Klinck, Holger},
	year         = {2021},
	title        = {Unsupervised acoustic classification of individual gibbon females and the implications for passive acoustic monitoring},
	journal      = {Methods in Ecology and Evolution},
	volume       = 12,
	number       = {2},
	pages        = {328--341},
	doi          = {https://doi.org/10.1111/2041-210X.13520},
	url          = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13520},
	keywords     = {affinity propagation, Gaussian mixture models, Hylobates, K-medoids, normalized mutual information, passive acoustic monitoring, unsupervised clustering},
	eprint       = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13520},
	abstract     = {Abstract Passive acoustic monitoring (PAM) has the potential to greatly improve our ability to monitor cryptic yet vocal animals. Advances in automated signal detection have increased the scope of PAM, but distinguishing between individuals—which is necessary for density estimation—remains a major challenge. When individual identity is known, supervised classification techniques can be used to distinguish between individuals. Supervised methods require labelled training data, whereas unsupervised techniques do not. If the acoustic signals of individuals are sufficiently different, the number of clusters might represent the number of individuals sampled. The majority of applications of unsupervised techniques in animal vocalizations have focused on quantifying species-specific call repertoires. However, with increased interest in PAM applications, unsupervised methods that can distinguish between individuals are needed. Here we use an existing dataset of Bornean gibbon female calls with known identity from five sites on Malaysian Borneo to test the ability of three different unsupervised clustering algorithms (affinity propagation, K-medoids and Gaussian mixture model-based clustering) to distinguish between individuals. Calls from different gibbon females are readily distinguishable using supervised techniques. For internal validation of unsupervised cluster solutions, we calculated silhouette coefficients. For external validation, we compared clustering results with female identity labels using a standard metric: normalized mutual information. We also calculated classification accuracy by assigning unsupervised cluster solutions to females based on which cluster had the highest number of calls from a particular female. We found that affinity propagation clustering consistently outperformed the other algorithms for all metrics used. In particular, classification accuracy of affinity propagation clustering was more consistent as the number of females increased, and when we randomly sampled females across sites. We conclude that unsupervised techniques may be useful for providing additional information regarding individual identity for PAM applications. We stress that although we use gibbons as a case study, these methods will be applicable for any individually distinct vocal animal.}
}
@article{DazVallejo2023,
	author       = {Díaz-Vallejo,  Mauricio and Chaparro-Herrera,  Sergio and Lopera-Salazar,  Andrea and Castaño-Díaz,  Michael and Correa,  Rodolfo and Parra,  Juan L.},
	year         = {2023},
	title        = {Use of acoustic monitoring to estimate occupancy of the Antioquia Brushfinch ( Atlapetes blancae ),  a critically endangered species,  in San Pedro de los Milagros,  Antioquia},
	journal      = {Journal of Field Ornithology},
	publisher    = {Resilience Alliance,  Inc.},
	volume       = 94,
	number       = {2},
	doi          = {10.5751/jfo-00248-940204},
	issn         = {1557-9263},
	url          = {http://dx.doi.org/10.5751/JFO-00248-940204}
}
@phdthesis{soton438947,
	author       = {Peter Prince},
	year         = {2019},
	title        = {Using software-based acoustic detection and supporting tools to enable large-scale environmental monitoring},
	month        = 10,
	publisher    = {University of Southampton},
	url          = {https://eprints.soton.ac.uk/438947/},
	school       = {University of Southampton},
	abstract     = {Acoustic monitoring tools are often constrained to small-scale, short-term studies due to high energy consumption, limited storage, and high equipment costs. To broaden the scope of monitoring projects, affordability, energy efficiency, and space efficiency must be improved on such tools. This thesis describes efforts to empower researchers charged with monitoring ecosystems, faced with the challenges of limited budgets and cryptic targeted events. To this end AudioMoth was developed: a low-cost, open-source acoustic monitoring device which has been widely adopted by the conservation community, with over 6,600 devices sold as of August 2019.{\ensuremath{<}}br/{\ensuremath{>}}{\ensuremath{<}}br/{\ensuremath{>}}This thesis covers the development and deployment of three acoustic detection algorithms that reduce the power and storage requirements of acoustic monitoring. The algorithms aim to detect bat echolocation, to search for evidence of a endangered cicada species, and to collect evidence of poaching in a protected nature reserve. Each algorithm addresses a detection task of increasing complexity - analysing samples multiple times to prevent missed events, implementing extra analytical steps to account for environmental conditions such as wind, and incorporating a hidden Markov model for sample classification in both the time and frequency domain. For each algorithm this thesis reports on their detection accuracy as well as real-world deployments carried out with partner organisations. The deployments demonstrate how acoustic detection algorithms extend the use of low-cost, open-source hardware and facilitate a new avenue for conservation researchers to perform large-scale monitoring.{\ensuremath{<}}br/{\ensuremath{>}}{\ensuremath{<}}br/{\ensuremath{>}}The research also covers an analysis of the accessibility of acoustic monitoring technology, focusing on AudioMoth and its supporting software. This is done using a 75-respondent questionnaire and a thematic analysis done on a series of interviews. The results of both analyses discovered a number of potential methods for improving acoustic monitoring technology in terms of the various forms of accessibility (financial, usability, etc.). The community responses, along with the popularity of AudioMoth and the success of the deployed detection algorithms demonstrate the benefits of providing accessible acoustic monitoring solutions to conservationists.}
}
@phdthesis{Oliveira2022,
	author       = {Oliveira, Pedro Henrique de},
	year         = {2022},
	title        = {Variações espaciais e temporais dos botos-da-tainha via monitoramento acústico passivo},
	url          = {https://repositorio.ufsc.br/handle/123456789/247333},
	school       = {Universidade Federal de Santa Catarina}
}
@phdthesis{https://doi.org/10.7907/m4mt-2q51,
	author       = {Beery,  Sara Meghan},
	year         = {2023},
	title        = {Where the Wild Things Are: Computer Vision for Global-Scale Biodiversity Monitoring},
	publisher    = {California Institute of Technology},
	doi          = {10.7907/M4MT-2Q51},
	url          = {https://resolver.caltech.edu/CaltechTHESIS:04242022-005200355},
	copyright    = {No commercial reproduction,  distribution,  display or performance rights in this work are provided.},
	keywords     = {Computing and Mathematical Sciences,  Conservation Technology,  Machine Learning,  Computer Vision,  Data Science,  Biodiversity Monitoring},
	language     = {en}
}
@article{Xiao_2022,
	author       = {Xiao, Zhishu and Xiao, Wenhong and Wang, Tianming and Li, Sheng and Lian, Xinming and Song, Dazhao and Deng, Xueqin and Zhou, Qihai},
	year         = {2022},
	title        = {Wildlife monitoring and research using camera-trapping technology across China: The current status and future issues},
	journal      = {Biodiversity Science},
	publisher    = {Biodiversity Science},
	volume       = 30,
	number       = 10,
	pages        = {22451},
	doi          = {10.17520/biods.2022451},
	issn         = {1005-0094},
	url          = {http://dx.doi.org/10.17520/biods.2022451}
}
